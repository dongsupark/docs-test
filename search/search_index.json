{
    "docs": [
        {
            "location": "/",
            "text": "Flatcar Container Linux Documentation\n\u00b6\n\n\nWelcome to Flatcar Container Linux documentation\n\n\nGetting Started\n\u00b6\n\n\nFlatcar Container Linux runs on most cloud providers, virtualization platforms and bare metal servers. Running a local VM on your laptop is a great dev environment. Following the \nQuick Start guide\n is the fastest way to get set up.\n\n\n\n\n\n\n\n\nProvisioning\n\n\nCloud Providers\n\n\n\n\n\n\n\n\n\n\nUsing Container Linux Config\n\n\nAmazon EC2\n\n\n\n\n\n\nUsing Config Transpiler\n\n\nDigitalOcean\n\n\n\n\n\n\nCL Config Dynamic Data\n\n\nGoogle Compute Engine\n\n\n\n\n\n\nCL Config Examples\n\n\nMicrosoft Azure\n\n\n\n\n\n\nCL Config Spec\n\n\nQEMU\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBare Metal\n\n\nUpgrading from CoreOS Container Linux\n\n\n\n\n\n\n\n\n\n\nUsing Matchbox\n\n\nMigrate from CoreOS Container Linux\n\n\n\n\n\n\nBooting with iPXE\n\n\nUpdate from CoreOS Container Linux\n directly.\n\n\n\n\n\n\nBooting with PXE\n\n\n\n\n\n\n\n\nInstalling to Disk\n\n\n\n\n\n\n\n\nBooting from ISO\n\n\n\n\n\n\n\n\nRoot filesystem placement\n\n\n\n\n\n\n\n\n\n\nWorking with Clusters\n\u00b6\n\n\nFollow these guides to connect your machines together as a cluster. Configure machine parameters, create users, inject multiple SSH keys, and more with Container Linux Config.\n\n\n\n\n\n\n\n\nCreating Clusters\n\n\nCustomizing Clusters\n\n\n\n\n\n\n\n\n\n\nCluster architectures\n\n\nUsing networkd to customize networking\n\n\n\n\n\n\nUpdate strategies\n\n\nUsing systemd drop-in units\n\n\n\n\n\n\nClustering machines\n\n\nUsing environment variables in systemd units\n\n\n\n\n\n\nVerify Flatcar Container Linux Images with GPG\n\n\nConfiguring DNS\n\n\n\n\n\n\n\n\nConfiguring date & timezone\n\n\n\n\n\n\n\n\nAdding users\n\n\n\n\n\n\n\n\nKernel modules / sysctl parameters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManaging Clusters\n\n\nScaling Clusters\n\n\n\n\n\n\n\n\n\n\nRegistry authentication\n\n\nAdding disk space\n\n\n\n\n\n\niSCSI configuration\n\n\nMounting storage\n\n\n\n\n\n\nAdding swap\n\n\nPower management\n\n\n\n\n\n\nAmazon EC2 Container Service\n\n\n\n\n\n\n\n\nUsing systemd to manage Docker containers\n\n\n\n\n\n\n\n\nUsing systemd and udev rules\n\n\n\n\n\n\n\n\nSwitching release channels\n\n\n\n\n\n\n\n\nScheduling tasks with systemd\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecuring Clusters\n\n\nDebugging Clusters\n\n\n\n\n\n\n\n\n\n\nCustomizing the SSH daemon\n\n\nInstall debugging tools\n\n\n\n\n\n\nConfiguring SSSD on Flatcar Container Linux\n\n\nWorking with btrfs\n\n\n\n\n\n\nHardening a Flatcar Container Linux machine\n\n\nReading the system log\n\n\n\n\n\n\nTrusted Computing Hardware Requirements\n\n\nCollecting crash logs\n\n\n\n\n\n\nAdding Cert Authorities\n\n\nManual Flatcar Container Linux rollbacks\n\n\n\n\n\n\nUsing SELinux\n\n\n\n\n\n\n\n\nDisabling SMT\n\n\n\n\n\n\n\n\n\n\nContainer Runtimes\n\u00b6\n\n\nFlatcar Container Linux supports all of the popular methods for running containers, and you can choose to interact with the containers at a low-level, or use a higher level orchestration framework. Listed below are your options from the highest level abstraction down to the lowest level, the container runtime.\n\n\nDocker\n\u00b6\n\n\nGetting started with Docker\n\n\nCustomizing Docker\n\n\nReference\n\u00b6\n\n\nAPIs and troubleshooting guides for working with Flatcar Container Linux.\n\n\nDeveloper guides\n\n\nIntegrations\n\n\nMigrating from cloud-config to Container Linux Config",
            "title": "Home"
        },
        {
            "location": "/#flatcar-container-linux-documentation",
            "text": "Welcome to Flatcar Container Linux documentation",
            "title": "Flatcar Container Linux Documentation"
        },
        {
            "location": "/#getting-started",
            "text": "Flatcar Container Linux runs on most cloud providers, virtualization platforms and bare metal servers. Running a local VM on your laptop is a great dev environment. Following the  Quick Start guide  is the fastest way to get set up.     Provisioning  Cloud Providers      Using Container Linux Config  Amazon EC2    Using Config Transpiler  DigitalOcean    CL Config Dynamic Data  Google Compute Engine    CL Config Examples  Microsoft Azure    CL Config Spec  QEMU        Bare Metal  Upgrading from CoreOS Container Linux      Using Matchbox  Migrate from CoreOS Container Linux    Booting with iPXE  Update from CoreOS Container Linux  directly.    Booting with PXE     Installing to Disk     Booting from ISO     Root filesystem placement",
            "title": "Getting Started"
        },
        {
            "location": "/#working-with-clusters",
            "text": "Follow these guides to connect your machines together as a cluster. Configure machine parameters, create users, inject multiple SSH keys, and more with Container Linux Config.     Creating Clusters  Customizing Clusters      Cluster architectures  Using networkd to customize networking    Update strategies  Using systemd drop-in units    Clustering machines  Using environment variables in systemd units    Verify Flatcar Container Linux Images with GPG  Configuring DNS     Configuring date & timezone     Adding users     Kernel modules / sysctl parameters        Managing Clusters  Scaling Clusters      Registry authentication  Adding disk space    iSCSI configuration  Mounting storage    Adding swap  Power management    Amazon EC2 Container Service     Using systemd to manage Docker containers     Using systemd and udev rules     Switching release channels     Scheduling tasks with systemd         Securing Clusters  Debugging Clusters      Customizing the SSH daemon  Install debugging tools    Configuring SSSD on Flatcar Container Linux  Working with btrfs    Hardening a Flatcar Container Linux machine  Reading the system log    Trusted Computing Hardware Requirements  Collecting crash logs    Adding Cert Authorities  Manual Flatcar Container Linux rollbacks    Using SELinux     Disabling SMT",
            "title": "Working with Clusters"
        },
        {
            "location": "/#container-runtimes",
            "text": "Flatcar Container Linux supports all of the popular methods for running containers, and you can choose to interact with the containers at a low-level, or use a higher level orchestration framework. Listed below are your options from the highest level abstraction down to the lowest level, the container runtime.",
            "title": "Container Runtimes"
        },
        {
            "location": "/#docker",
            "text": "Getting started with Docker  Customizing Docker",
            "title": "Docker"
        },
        {
            "location": "/#reference",
            "text": "APIs and troubleshooting guides for working with Flatcar Container Linux.  Developer guides  Integrations  Migrating from cloud-config to Container Linux Config",
            "title": "Reference"
        },
        {
            "location": "/os/quickstart/",
            "text": "Flatcar Container Linux quick start\n\u00b6\n\n\nIf you don't have a Flatcar Container Linux machine running, check out the guides on \nrunning Flatcar Container Linux\n on most cloud providers (\nEC2\n, \nAzure\n, \nGCE\n), virtualization platforms (\nVagrant\n, \nVMware\n, \nQEMU/KVM\n) and bare metal servers (\nPXE\n, \niPXE\n, \nISO\n, \nInstaller\n). With any of these guides you will have machines up and running in a few minutes.\n\n\nIt's highly recommended that you set up a cluster of at least 3 machines \u2014 it's not as much fun on a single machine. If you don't want to break the bank, \nVagrant\n allows you to run an entire cluster on your laptop. For a cluster to be properly bootstrapped, you have to provide ideally an \nIgnition config\n (generated from a \nContainer Linux Config\n), or possibly a cloud-config, via user-data, which is covered in each platform's guide.\n\n\nFlatcar Container Linux gives you three essential tools: service discovery, container management and process management. Let's try each of them out.\n\n\nFirst, on the client start your user agent by typing:\n\n\neval $(ssh-agent)\n\n\n\nThen, add your private key to the agent by typing:\n\n\nssh-add\n\n\n\nConnect to a Flatcar Container Linux machine via SSH as the user \ncore\n. For example, on Amazon, use:\n\n\n$ ssh core@an.ip.compute-1.amazonaws.com\nFlatcar Container Linux (beta)\n\n\n\nIf you're using Vagrant, you'll need to connect a bit differently:\n\n\n$ ssh-add ~/.vagrant.d/insecure_private_key\nIdentity added: /Users/core/.vagrant.d/insecure_private_key (/Users/core/.vagrant.d/insecure_private_key)\n$ vagrant ssh core-01\nFlatcar Container Linux (beta)\n\n\n\nService discovery with etcd\n\u00b6\n\n\nThe first building block of Flatcar Container Linux is service discovery with \netcd\n (\ndocs\n). Data stored in etcd is distributed across all of your machines running Flatcar Container Linux. For example, each of your app containers can announce itself to a proxy container, which would automatically know which machines should receive traffic. Building service discovery into your application allows you to add more machines and scale your services seamlessly.\n\n\nIf you used an example \nContainer Linux Config\n or \ncloud-config\n from a guide linked in the first paragraph, etcd is automatically started on boot.\n\n\nA good starting point for a Container Linux Config would be something like:\n\n\netcd:\n  discovery: https://discovery.etcd.io/<token>\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAA...\n\n\n\nIn order to get the discovery token, visit \nhttps://discovery.etcd.io/new\n and you will receive a URL including your token. Paste the whole thing into your Container Linux Config file.\n\n\netcdctl\n is a command line interface to etcd that is preinstalled on Flatcar Container Linux. To set and retrieve a key from etcd you can use the following examples:\n\n\nSet a key \nmessage\n with value \nHello world\n:\n\n\netcdctl set /message \"Hello world\"\n\n\n\nRead the value of \nmessage\n back:\n\n\netcdctl get /message\n\n\n\nYou can also use simple \ncurl\n. These examples correspond to previous ones:\n\n\nSet the value:\n\n\ncurl -L http://127.0.0.1:2379/v2/keys/message -XPUT -d value=\"Hello world\"\n\n\n\nRead the value:\n\n\ncurl -L http://127.0.0.1:2379/v2/keys/message\n\n\n\nIf you followed a guide to set up more than one Flatcar Container Linux machine, you can SSH into another machine and can retrieve this same value.\n\n\nMore detailed information\n\u00b6\n\n\nView Complete Guide\n\n\nRead etcd API Docs\n\n\nContainer management with Docker\n\u00b6\n\n\nThe second building block, \nDocker\n (\ndocs\n), is where your applications and code run. It is installed on each Flatcar Container Linux machine. You should make each of your services (web server, caching, database) into a container and connect them together by reading and writing to etcd. You can quickly try out a minimal busybox container in two different ways:\n\n\nRun a command in the container and then stop it:\n\n\ndocker run busybox /bin/echo hello world\n\n\n\nOpen a shell prompt inside the container:\n\n\ndocker run -i -t busybox /bin/sh\n\n\n\nMore detailed information\n\u00b6\n\n\nView Complete Guide\n\n\nRead Docker Docs",
            "title": "Quick start"
        },
        {
            "location": "/os/quickstart/#flatcar-container-linux-quick-start",
            "text": "If you don't have a Flatcar Container Linux machine running, check out the guides on  running Flatcar Container Linux  on most cloud providers ( EC2 ,  Azure ,  GCE ), virtualization platforms ( Vagrant ,  VMware ,  QEMU/KVM ) and bare metal servers ( PXE ,  iPXE ,  ISO ,  Installer ). With any of these guides you will have machines up and running in a few minutes.  It's highly recommended that you set up a cluster of at least 3 machines \u2014 it's not as much fun on a single machine. If you don't want to break the bank,  Vagrant  allows you to run an entire cluster on your laptop. For a cluster to be properly bootstrapped, you have to provide ideally an  Ignition config  (generated from a  Container Linux Config ), or possibly a cloud-config, via user-data, which is covered in each platform's guide.  Flatcar Container Linux gives you three essential tools: service discovery, container management and process management. Let's try each of them out.  First, on the client start your user agent by typing:  eval $(ssh-agent)  Then, add your private key to the agent by typing:  ssh-add  Connect to a Flatcar Container Linux machine via SSH as the user  core . For example, on Amazon, use:  $ ssh core@an.ip.compute-1.amazonaws.com\nFlatcar Container Linux (beta)  If you're using Vagrant, you'll need to connect a bit differently:  $ ssh-add ~/.vagrant.d/insecure_private_key\nIdentity added: /Users/core/.vagrant.d/insecure_private_key (/Users/core/.vagrant.d/insecure_private_key)\n$ vagrant ssh core-01\nFlatcar Container Linux (beta)",
            "title": "Flatcar Container Linux quick start"
        },
        {
            "location": "/os/quickstart/#service-discovery-with-etcd",
            "text": "The first building block of Flatcar Container Linux is service discovery with  etcd  ( docs ). Data stored in etcd is distributed across all of your machines running Flatcar Container Linux. For example, each of your app containers can announce itself to a proxy container, which would automatically know which machines should receive traffic. Building service discovery into your application allows you to add more machines and scale your services seamlessly.  If you used an example  Container Linux Config  or  cloud-config  from a guide linked in the first paragraph, etcd is automatically started on boot.  A good starting point for a Container Linux Config would be something like:  etcd:\n  discovery: https://discovery.etcd.io/<token>\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAA...  In order to get the discovery token, visit  https://discovery.etcd.io/new  and you will receive a URL including your token. Paste the whole thing into your Container Linux Config file.  etcdctl  is a command line interface to etcd that is preinstalled on Flatcar Container Linux. To set and retrieve a key from etcd you can use the following examples:  Set a key  message  with value  Hello world :  etcdctl set /message \"Hello world\"  Read the value of  message  back:  etcdctl get /message  You can also use simple  curl . These examples correspond to previous ones:  Set the value:  curl -L http://127.0.0.1:2379/v2/keys/message -XPUT -d value=\"Hello world\"  Read the value:  curl -L http://127.0.0.1:2379/v2/keys/message  If you followed a guide to set up more than one Flatcar Container Linux machine, you can SSH into another machine and can retrieve this same value.",
            "title": "Service discovery with etcd"
        },
        {
            "location": "/os/quickstart/#more-detailed-information",
            "text": "View Complete Guide  Read etcd API Docs",
            "title": "More detailed information"
        },
        {
            "location": "/os/quickstart/#container-management-with-docker",
            "text": "The second building block,  Docker  ( docs ), is where your applications and code run. It is installed on each Flatcar Container Linux machine. You should make each of your services (web server, caching, database) into a container and connect them together by reading and writing to etcd. You can quickly try out a minimal busybox container in two different ways:  Run a command in the container and then stop it:  docker run busybox /bin/echo hello world  Open a shell prompt inside the container:  docker run -i -t busybox /bin/sh",
            "title": "Container management with Docker"
        },
        {
            "location": "/os/quickstart/#more-detailed-information_1",
            "text": "View Complete Guide  Read Docker Docs",
            "title": "More detailed information"
        },
        {
            "location": "/os/provisioning/",
            "text": "Provisioning\n\u00b6\n\n\nFlatcar Container Linux automates machine provisioning with a specialized system for applying initial configuration. This system implements a process of (trans)compilation and validation for machine configs, and an atomic service to apply validated configurations to machines.\n\n\nContainer Linux Config\n\u00b6\n\n\nFlatcar Container Linux admins define these configurations in a format called the \nContainer Linux Config\n, which was originally designed for CoreOS Container Linux, but works perfectly well with Flatcar Container Linux. Container Linux Configs are structured as YAML, and intended to be human-readable. The Container Linux Config has features devoted to configuring Flatcar Container Linux services such as \netcd\n, \nrkt\n, Docker, \nflannel\n, and \nlocksmith\n. \nThe defining feature of the config is that it cannot be sent directly to a Flatcar Container Linux provisioning target\n. Instead, it is first validated and transformed into a machine-readable and wire-efficient form.\n\n\nThe following examples demonstrate the simplicity of the Container Linux Config format.\n\n\nThis extremely simple Container Linux Config will fetch and run the current release of etcd:\n\n\netcd:\n\n\n\nExtend the definition to specify the version of etcd to run. The following example will provision a new Flatcar Container Linux machine to fetch and run the etcd service, version 3.1.6:\n\n\netcd:\n  version: 3.1.6\n\n\n\nUse variable replacement to configure the etcd service with the provisioning target's public and private IPv4 addresses, making it repeatable across a group of machines.\n\n\netcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>\n\n\n\nPUBLIC_IPV4\n and \nPRIVATE_IPV4\n are automatically populated from the environment in which Flatcar Container Linux runs, if this metadata exists. Given the many different environments in which Flatcar Container Linux can run, it's difficult if not impossible to accurately determine these variables in every instance. Be certain to check this value as a troubleshooting measure.\n\n\nFor example, the default metadata for an EC2 environment would be used: \npublic_ipv4\n and \nlocal_ipv4\n. On Azure, \neither\n the virtual IP or public IP could be used for the \nPUBLIC_IPV4\n (\nct\n makes a best guess and uses the virtual IP, but this could change in the future), and the dynamic IP would be used for the \nPRIVATE_IPV4\n. On bare metal, this information cannot be reliably derived in a general manner, so these variables cannot be used.\n\n\nBecause variable expansion is unpredictable and complex, and because it is also common for users to inadvertently write invalid configs, the use of a transformation tool is strongly encouraged. The default tool recommended for this task is the \nConfig Transpiler\n (ct for short). The Config Transpiler will validate and transform a Container Linux Config into the format that Flatcar Container Linux can consume: the Ignition Config.\n\n\nIgnition Config\n\u00b6\n\n\nIgnition, the utility in Flatcar Container Linux responsible for provisioning the machine, fetches and executes the Ignition Config. Flatcar Container Linux directly consumes the Ignition Config configuration format.\n\n\nIgnition Configs are mostly static, distro-agnostic, and meant to be generated by a machine rather than a human. While they can be written directly by users, it is highly discouraged due to the ease with which errors may be introduced. Rather than writing Ignition Configs directly, users are encouraged to use provisioning tools like \nMatchbox\n, which transparently translate Container Linux Configs to Ignition Configs, or to use the Config Transpiler itself.\n\n\n\n\nAs shown in this diagram, \nct\n is manually invoked only when users are manually provisioning machines. If a provisioning tool like Matchbox is used, \nct\n will transparently be incorporated into the deployment pipeline. In which case, the user only needs to prepare a Container Linux Config - Ignition and the Ignition Config are merely an implementation detail.\n\n\nConfig Transpiler\n\u00b6\n\n\nThe Container Linux Config Transpiler abstracts the details of configuring Flatcar Container Linux. It's responsible for transforming a Container Linux Config written by a user into an Ignition Config to be consumed by instances of Flatcar Container Linux.\n\n\nThe Container Linux Config Transpiler command line interface, \nct\n for short, can be downloaded from its \nGitHub Releases page\n.\n\n\nThe following config will configure an etcd cluster using the machine's public and private IP addresses:\n\n\netcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>\n\n\n\nAs suggested earlier, \nct\n requires information about the target environment before it can transform configs which use templating. If this config is passed to \nct\n without any other arguments, \nct\n fails with the following error message:\n\n\n$ ct < example.yml\nerror: platform must be specified to use templating\n\n\n\nThis message states that because the config takes advantage of templating (in this case,  \nPUBLIC_IPV4\n), \nct\n must be invoked with the \n--platform\n argument. This extra information is used by \nct\n to make the platform-specific customizations necessary. Keeping the Container Linux Config and the invocation arguments separate allows the Container Linux Config to remain largely platform independent.\n\n\nCT can be invoked again and given Amazon EC2 as an example:\n\n\n$ ct --platform=ec2 < example.yml\n{\"ignition\":{\"version\":\"2.0.0\",\"config\"...\n\n\n\nThis time, \nct\n successfully runs and produces the following Ignition Config:\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"etcd-member.service\",\n      \"enable\": true,\n      \"dropins\": [{\n        \"name\": \"20-clct-etcd-member.conf\",\n        \"contents\": \"[Unit]\\nRequires=coreos-metadata.service\\nAfter=coreos-metadata.service\\n\\n[Service]\\nEnvironmentFile=/run/metadata/coreos\\nExecStart=\\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\\\\n  --listen-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --listen-client-urls=\\\"http://0.0.0.0:2379\\\" \\\\\\n  --initial-advertise-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --advertise-client-urls=\\\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\\\" \\\\\\n  --discovery=\\\"https://discovery.etcd.io/\\u003ctoken\\u003e\\\"\"\n      }]\n    }]\n  }\n}\n\n\n\nThis Ignition Config enables and configures etcd as specified in the above Container Linux Config. This can be more easily seen if the contents of the etcd drop-in are formatted nicely:\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\" \\\n  --discovery=\"https://discovery.etcd.io/<token>\"\n\n\n\nThe details of these changes are covered in depth in Ignition's \nmetadata documentation\n, but the gist is that \ncoreos-metadata\n is used to fetch the IP addresses from the Amazon APIs and then \nsystemd\n is leveraged to substitute the IP addresses into the invocation of etcd. The result is that even though Ignition only runs once, \ncoreos-metadata\n fetches the IP addresses whenever etcd is run, allowing etcd to use IP addresses that have the potential to change.\n\n\nMigrating from cloud configs\n\u00b6\n\n\nPreviously, the recommended way to provision a Flatcar Container Linux machine was with a cloud-config. These configs would be given to a Flatcar Container Linux machine and a utility called \ncoreos-cloudinit\n would read this file and apply the configuration on every boot.\n\n\nFor a \nnumber of reasons\n, coreos-cloudinit has been deprecated in favor of Container Linux Configs and Ignition. For help migrating from these legacy cloud-configs to Container Linux Configs, refer to the \nmigration guide\n.\n\n\nUsing Container Linux Configs\n\u00b6\n\n\nNow that the basics of Container Linux Configs have been covered, a good next step is to read through the \nexamples\n and start experimenting. The \ntroubleshooting guide\n is a good reference for debugging issues.",
            "title": "Using Container Linux Config"
        },
        {
            "location": "/os/provisioning/#provisioning",
            "text": "Flatcar Container Linux automates machine provisioning with a specialized system for applying initial configuration. This system implements a process of (trans)compilation and validation for machine configs, and an atomic service to apply validated configurations to machines.",
            "title": "Provisioning"
        },
        {
            "location": "/os/provisioning/#container-linux-config",
            "text": "Flatcar Container Linux admins define these configurations in a format called the  Container Linux Config , which was originally designed for CoreOS Container Linux, but works perfectly well with Flatcar Container Linux. Container Linux Configs are structured as YAML, and intended to be human-readable. The Container Linux Config has features devoted to configuring Flatcar Container Linux services such as  etcd ,  rkt , Docker,  flannel , and  locksmith .  The defining feature of the config is that it cannot be sent directly to a Flatcar Container Linux provisioning target . Instead, it is first validated and transformed into a machine-readable and wire-efficient form.  The following examples demonstrate the simplicity of the Container Linux Config format.  This extremely simple Container Linux Config will fetch and run the current release of etcd:  etcd:  Extend the definition to specify the version of etcd to run. The following example will provision a new Flatcar Container Linux machine to fetch and run the etcd service, version 3.1.6:  etcd:\n  version: 3.1.6  Use variable replacement to configure the etcd service with the provisioning target's public and private IPv4 addresses, making it repeatable across a group of machines.  etcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>  PUBLIC_IPV4  and  PRIVATE_IPV4  are automatically populated from the environment in which Flatcar Container Linux runs, if this metadata exists. Given the many different environments in which Flatcar Container Linux can run, it's difficult if not impossible to accurately determine these variables in every instance. Be certain to check this value as a troubleshooting measure.  For example, the default metadata for an EC2 environment would be used:  public_ipv4  and  local_ipv4 . On Azure,  either  the virtual IP or public IP could be used for the  PUBLIC_IPV4  ( ct  makes a best guess and uses the virtual IP, but this could change in the future), and the dynamic IP would be used for the  PRIVATE_IPV4 . On bare metal, this information cannot be reliably derived in a general manner, so these variables cannot be used.  Because variable expansion is unpredictable and complex, and because it is also common for users to inadvertently write invalid configs, the use of a transformation tool is strongly encouraged. The default tool recommended for this task is the  Config Transpiler  (ct for short). The Config Transpiler will validate and transform a Container Linux Config into the format that Flatcar Container Linux can consume: the Ignition Config.",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/provisioning/#ignition-config",
            "text": "Ignition, the utility in Flatcar Container Linux responsible for provisioning the machine, fetches and executes the Ignition Config. Flatcar Container Linux directly consumes the Ignition Config configuration format.  Ignition Configs are mostly static, distro-agnostic, and meant to be generated by a machine rather than a human. While they can be written directly by users, it is highly discouraged due to the ease with which errors may be introduced. Rather than writing Ignition Configs directly, users are encouraged to use provisioning tools like  Matchbox , which transparently translate Container Linux Configs to Ignition Configs, or to use the Config Transpiler itself.   As shown in this diagram,  ct  is manually invoked only when users are manually provisioning machines. If a provisioning tool like Matchbox is used,  ct  will transparently be incorporated into the deployment pipeline. In which case, the user only needs to prepare a Container Linux Config - Ignition and the Ignition Config are merely an implementation detail.",
            "title": "Ignition Config"
        },
        {
            "location": "/os/provisioning/#config-transpiler",
            "text": "The Container Linux Config Transpiler abstracts the details of configuring Flatcar Container Linux. It's responsible for transforming a Container Linux Config written by a user into an Ignition Config to be consumed by instances of Flatcar Container Linux.  The Container Linux Config Transpiler command line interface,  ct  for short, can be downloaded from its  GitHub Releases page .  The following config will configure an etcd cluster using the machine's public and private IP addresses:  etcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>  As suggested earlier,  ct  requires information about the target environment before it can transform configs which use templating. If this config is passed to  ct  without any other arguments,  ct  fails with the following error message:  $ ct < example.yml\nerror: platform must be specified to use templating  This message states that because the config takes advantage of templating (in this case,   PUBLIC_IPV4 ),  ct  must be invoked with the  --platform  argument. This extra information is used by  ct  to make the platform-specific customizations necessary. Keeping the Container Linux Config and the invocation arguments separate allows the Container Linux Config to remain largely platform independent.  CT can be invoked again and given Amazon EC2 as an example:  $ ct --platform=ec2 < example.yml\n{\"ignition\":{\"version\":\"2.0.0\",\"config\"...  This time,  ct  successfully runs and produces the following Ignition Config:  {\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"etcd-member.service\",\n      \"enable\": true,\n      \"dropins\": [{\n        \"name\": \"20-clct-etcd-member.conf\",\n        \"contents\": \"[Unit]\\nRequires=coreos-metadata.service\\nAfter=coreos-metadata.service\\n\\n[Service]\\nEnvironmentFile=/run/metadata/coreos\\nExecStart=\\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\\\\n  --listen-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --listen-client-urls=\\\"http://0.0.0.0:2379\\\" \\\\\\n  --initial-advertise-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --advertise-client-urls=\\\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\\\" \\\\\\n  --discovery=\\\"https://discovery.etcd.io/\\u003ctoken\\u003e\\\"\"\n      }]\n    }]\n  }\n}  This Ignition Config enables and configures etcd as specified in the above Container Linux Config. This can be more easily seen if the contents of the etcd drop-in are formatted nicely:  [Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\" \\\n  --discovery=\"https://discovery.etcd.io/<token>\"  The details of these changes are covered in depth in Ignition's  metadata documentation , but the gist is that  coreos-metadata  is used to fetch the IP addresses from the Amazon APIs and then  systemd  is leveraged to substitute the IP addresses into the invocation of etcd. The result is that even though Ignition only runs once,  coreos-metadata  fetches the IP addresses whenever etcd is run, allowing etcd to use IP addresses that have the potential to change.",
            "title": "Config Transpiler"
        },
        {
            "location": "/os/provisioning/#migrating-from-cloud-configs",
            "text": "Previously, the recommended way to provision a Flatcar Container Linux machine was with a cloud-config. These configs would be given to a Flatcar Container Linux machine and a utility called  coreos-cloudinit  would read this file and apply the configuration on every boot.  For a  number of reasons , coreos-cloudinit has been deprecated in favor of Container Linux Configs and Ignition. For help migrating from these legacy cloud-configs to Container Linux Configs, refer to the  migration guide .",
            "title": "Migrating from cloud configs"
        },
        {
            "location": "/os/provisioning/#using-container-linux-configs",
            "text": "Now that the basics of Container Linux Configs have been covered, a good next step is to read through the  examples  and start experimenting. The  troubleshooting guide  is a good reference for debugging issues.",
            "title": "Using Container Linux Configs"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/",
            "text": "Config transpiler overview\n\u00b6\n\n\nThe \nConfig Transpiler\n, ct, is the utility responsible for transforming a user-provided Container Linux Configuration into an \nIgnition\n configuration. The resulting Ignition config can then be provided to a Container Linux machine when it first boots in order to provision it.\n\n\nThe Container Linux Config is intended to be human-friendly, and is thus in YAML. The syntax is rather forgiving, and things like references and multi-line strings are supported.\n\n\nThe resulting Ignition config is very much not intended to be human-friendly. It is an artifact produced by ct that users should simply pass along to their machines. JSON was chosen over a binary format to make the process more transparent and to allow power users to inspect/modify what ct produces, but it would have worked fine if the result from ct had not been human readable at all.\n\n\nWhy a two-step process?\n\u00b6\n\n\nThere are a couple factors motivating the decision to not incorporate support for Container Linux Configs directly into the boot process of Container Linux (as in, the ability to provide a Container Linux Config directly to a booting machine, instead of an Ignition config).\n\n\n\n\nBy making users run their configs through ct before they attempt to boot a machine, issues with their configs can be caught before any machine attempts to boot. This will save users time, as they can much more quickly find problems with their configs. Were users to provide Container Linux Configs directly to machines at first boot, they would need to find a way to extract the Ignition logs from a machine that may have failed to boot, which can be a slow and tedious process.\n\n\nYAML parsing is a complex process that in the past has been rather error-prone. By only doing JSON parsing in the boot path, we can guarantee that the utilities necessary for a machine to boot are simpler and more reliable. We want to allow users to use YAML however, as it's much more human-friendly than JSON, hence the decision to have a tool separate from the boot path to \"transpile\" YAML configurations to machine-appropriate JSON ones.\n\n\n\n\nTell me more about Ignition\n\u00b6\n\n\nIgnition\n is the utility inside of a Container Linux image that is responsible for setting up a machine. It takes in a configuration, written in JSON, that instructs it to do things like add users, format disks, and install systemd units. The artifacts that ct produces are Ignition configs. All of this should be an implementation detail however, users are encouraged to write Container Linux Configs for ct, and to simply pass along the produced JSON file to their machines.\n\n\nHow similar are Container Linux Configs and Ignition configs?\n\u00b6\n\n\nSome features in Container Linux Configs and Ignition configs are identical.  Both support listing users for creation, systemd unit dropins for installation, and files for writing.\n\n\nAll of the differences stem from the fact that Ignition configs are distribution agnostic. An Ignition config can't just tell Ignition to enable etcd, because Ignition doesn't know what etcd is. The config must tell Ignition what systemd unit to enable, and provide a systemd dropin to configure etcd.\n\n\nct on the other hand \ndoes\n understand the specifics of Container Linux. A user can merely specify an etcd version and some etcd options, and ct knows that there's a unit called \netcd-member\n already on the system it can enable. It knows what options are supported by etcd, so it can sanity check them for the user. It can then generate an appropriate systemd dropin with the user's options, and provide Ignition the level of verbosity it needs, that would be tedious for a human to create.",
            "title": "Using Config Transpiler"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#config-transpiler-overview",
            "text": "The  Config Transpiler , ct, is the utility responsible for transforming a user-provided Container Linux Configuration into an  Ignition  configuration. The resulting Ignition config can then be provided to a Container Linux machine when it first boots in order to provision it.  The Container Linux Config is intended to be human-friendly, and is thus in YAML. The syntax is rather forgiving, and things like references and multi-line strings are supported.  The resulting Ignition config is very much not intended to be human-friendly. It is an artifact produced by ct that users should simply pass along to their machines. JSON was chosen over a binary format to make the process more transparent and to allow power users to inspect/modify what ct produces, but it would have worked fine if the result from ct had not been human readable at all.",
            "title": "Config transpiler overview"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#why-a-two-step-process",
            "text": "There are a couple factors motivating the decision to not incorporate support for Container Linux Configs directly into the boot process of Container Linux (as in, the ability to provide a Container Linux Config directly to a booting machine, instead of an Ignition config).   By making users run their configs through ct before they attempt to boot a machine, issues with their configs can be caught before any machine attempts to boot. This will save users time, as they can much more quickly find problems with their configs. Were users to provide Container Linux Configs directly to machines at first boot, they would need to find a way to extract the Ignition logs from a machine that may have failed to boot, which can be a slow and tedious process.  YAML parsing is a complex process that in the past has been rather error-prone. By only doing JSON parsing in the boot path, we can guarantee that the utilities necessary for a machine to boot are simpler and more reliable. We want to allow users to use YAML however, as it's much more human-friendly than JSON, hence the decision to have a tool separate from the boot path to \"transpile\" YAML configurations to machine-appropriate JSON ones.",
            "title": "Why a two-step process?"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#tell-me-more-about-ignition",
            "text": "Ignition  is the utility inside of a Container Linux image that is responsible for setting up a machine. It takes in a configuration, written in JSON, that instructs it to do things like add users, format disks, and install systemd units. The artifacts that ct produces are Ignition configs. All of this should be an implementation detail however, users are encouraged to write Container Linux Configs for ct, and to simply pass along the produced JSON file to their machines.",
            "title": "Tell me more about Ignition"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#how-similar-are-container-linux-configs-and-ignition-configs",
            "text": "Some features in Container Linux Configs and Ignition configs are identical.  Both support listing users for creation, systemd unit dropins for installation, and files for writing.  All of the differences stem from the fact that Ignition configs are distribution agnostic. An Ignition config can't just tell Ignition to enable etcd, because Ignition doesn't know what etcd is. The config must tell Ignition what systemd unit to enable, and provide a systemd dropin to configure etcd.  ct on the other hand  does  understand the specifics of Container Linux. A user can merely specify an etcd version and some etcd options, and ct knows that there's a unit called  etcd-member  already on the system it can enable. It knows what options are supported by etcd, so it can sanity check them for the user. It can then generate an appropriate systemd dropin with the user's options, and provide Ignition the level of verbosity it needs, that would be tedious for a human to create.",
            "title": "How similar are Container Linux Configs and Ignition configs?"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/",
            "text": "Referencing dynamic data\n\u00b6\n\n\nOverview\n\u00b6\n\n\nSometimes it can be useful to refer to data in a Container Linux Config that isn't known until a machine boots, like its network address. This can be accomplished with \ncoreos-metadata\n. coreos-metadata is a very basic utility that fetches information about the current machine and makes it available for consumption. By making it a dependency of services which requires this information, systemd will ensure that coreos-metadata has successfully completed before starting these services. These services can then simply source the fetched information and let systemd perform the environment variable expansions.\n\n\nAs of version 0.2.0, ct has support for making this easy for users. In specific sections of a config, users can enter in dynamic data between \n{}\n, and ct will handle enabling the coreos-metadata service and using the information it provides.\n\n\nThe available information varies by provider, and is expressed in different variables by coreos-metadata. If this feature is used a \n--provider\n flag must be passed to ct. Currently, the \netcd\n and \nflannel\n sections are the only ones which support this feature.\n\n\nSupported data by provider\n\u00b6\n\n\nThis is the information available in each provider.\n\n\n\n\n\n\n\n\n\n\nHOSTNAME\n\n\nPRIVATE_IPV4\n\n\nPUBLIC_IPV4\n\n\nPRIVATE_IPV6\n\n\nPUBLIC_IPV6\n\n\n\n\n\n\n\n\n\n\nAzure\n\n\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nDigital Ocean\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\nEC2\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nGCE\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nPacket\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\u2713\n\n\n\n\n\n\nOpenStack-Metadata\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nVagrant-Virtualbox\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom metadata providers\n\u00b6\n\n\nct\n also supports custom metadata providers. To use the \ncustom\n platform, modify the coreos-metadata service unit to execute your own custom metadata fetcher. The custom metadata fetcher must write an environment file \n/run/metadata/coreos\n defining a \nCOREOS_CUSTOM_*\n environment variable for every piece of dynamic data used in the specified Container Linux Config. The environment variables are the same as in the Container Linux Config, but prefixed with \nCOREOS_CUSTOM_\n.\n\n\nExample\n\u00b6\n\n\nAssume \nhttps://example.com/metadata-script.sh\n is a script which communicates with a metadata service and then writes the following file to \n/run/metadata/coreos\n:\n\nCOREOS_CUSTOM_HOSTNAME=foobar\nCOREOS_CUSTOM_PRIVATE_IPV4=<The instance's private ipv4 address>\nCOREOS_CUSTOM_PUBLIC_IPV4=<The instance's public ipv4 address>\n\n\nThe following Container Linux Config downloads the metadata fetching script, replaces the ExecStart line in \ncoreos-metadata\n service to use the script instead, and configures etcd using the metadata provided. Use the \n--platform=custom\n flag when transpiling.\n\nstorage:\n  files:\n    - filesystem: \"root\"\n      path: \"/opt/get-metadata.sh\"\n      mode: 0755\n      contents:\n        remote:\n          url: \"https://example.com/metadata-script.sh\"\n\nsystemd:\n  units:\n    - name: \"coreos-metadata.service\"\n      dropins:\n       - name: \"use-script.conf\"\n         contents: |\n           [Service]\n           # Empty ExecStart= prevents the previously defined ExecStart from running\n           ExecStart=\n           ExecStart=/opt/get-metadata.sh\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"\n\n\nBehind the scenes\n\u00b6\n\n\nFor a more in-depth walk through of how this feature works, let's look at the etcd example from the \nexamples document\n.\n\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"\n\n\n\nIf we give this example to ct with the \n--platform=ec2\n tag, it produces the following drop-in:\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nEnvironment=\"ETCD_IMAGE_TAG=v3.0.15\"\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --name=\"${COREOS_EC2_HOSTNAME}\" \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --initial-cluster=\"${COREOS_EC2_HOSTNAME}=http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2379\"\n\n\n\nThis drop-in specifies that etcd should run after the coreos-metadata service, and it uses \n/run/metadata/coreos\n as an \nEnvironmentFile\n. This enables the coreos-metadata service, and puts the information it discovers into environment variables. These environment variables are then expanded by systemd when the service starts, inserting the dynamic data into the command-line flags to etcd.",
            "title": "CL Config Dynamic Data"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#referencing-dynamic-data",
            "text": "",
            "title": "Referencing dynamic data"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#overview",
            "text": "Sometimes it can be useful to refer to data in a Container Linux Config that isn't known until a machine boots, like its network address. This can be accomplished with  coreos-metadata . coreos-metadata is a very basic utility that fetches information about the current machine and makes it available for consumption. By making it a dependency of services which requires this information, systemd will ensure that coreos-metadata has successfully completed before starting these services. These services can then simply source the fetched information and let systemd perform the environment variable expansions.  As of version 0.2.0, ct has support for making this easy for users. In specific sections of a config, users can enter in dynamic data between  {} , and ct will handle enabling the coreos-metadata service and using the information it provides.  The available information varies by provider, and is expressed in different variables by coreos-metadata. If this feature is used a  --provider  flag must be passed to ct. Currently, the  etcd  and  flannel  sections are the only ones which support this feature.",
            "title": "Overview"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#supported-data-by-provider",
            "text": "This is the information available in each provider.      HOSTNAME  PRIVATE_IPV4  PUBLIC_IPV4  PRIVATE_IPV6  PUBLIC_IPV6      Azure   \u2713  \u2713      Digital Ocean  \u2713  \u2713  \u2713  \u2713  \u2713    EC2  \u2713  \u2713  \u2713      GCE  \u2713  \u2713  \u2713      Packet  \u2713  \u2713  \u2713   \u2713    OpenStack-Metadata  \u2713  \u2713  \u2713      Vagrant-Virtualbox  \u2713  \u2713",
            "title": "Supported data by provider"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#custom-metadata-providers",
            "text": "ct  also supports custom metadata providers. To use the  custom  platform, modify the coreos-metadata service unit to execute your own custom metadata fetcher. The custom metadata fetcher must write an environment file  /run/metadata/coreos  defining a  COREOS_CUSTOM_*  environment variable for every piece of dynamic data used in the specified Container Linux Config. The environment variables are the same as in the Container Linux Config, but prefixed with  COREOS_CUSTOM_ .",
            "title": "Custom metadata providers"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#example",
            "text": "Assume  https://example.com/metadata-script.sh  is a script which communicates with a metadata service and then writes the following file to  /run/metadata/coreos : COREOS_CUSTOM_HOSTNAME=foobar\nCOREOS_CUSTOM_PRIVATE_IPV4=<The instance's private ipv4 address>\nCOREOS_CUSTOM_PUBLIC_IPV4=<The instance's public ipv4 address>  The following Container Linux Config downloads the metadata fetching script, replaces the ExecStart line in  coreos-metadata  service to use the script instead, and configures etcd using the metadata provided. Use the  --platform=custom  flag when transpiling. storage:\n  files:\n    - filesystem: \"root\"\n      path: \"/opt/get-metadata.sh\"\n      mode: 0755\n      contents:\n        remote:\n          url: \"https://example.com/metadata-script.sh\"\n\nsystemd:\n  units:\n    - name: \"coreos-metadata.service\"\n      dropins:\n       - name: \"use-script.conf\"\n         contents: |\n           [Service]\n           # Empty ExecStart= prevents the previously defined ExecStart from running\n           ExecStart=\n           ExecStart=/opt/get-metadata.sh\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"",
            "title": "Example"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#behind-the-scenes",
            "text": "For a more in-depth walk through of how this feature works, let's look at the etcd example from the  examples document .  etcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"  If we give this example to ct with the  --platform=ec2  tag, it produces the following drop-in:  [Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nEnvironment=\"ETCD_IMAGE_TAG=v3.0.15\"\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --name=\"${COREOS_EC2_HOSTNAME}\" \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --initial-cluster=\"${COREOS_EC2_HOSTNAME}=http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2379\"  This drop-in specifies that etcd should run after the coreos-metadata service, and it uses  /run/metadata/coreos  as an  EnvironmentFile . This enables the coreos-metadata service, and puts the information it discovers into environment variables. These environment variables are then expanded by systemd when the service starts, inserting the dynamic data into the command-line flags to etcd.",
            "title": "Behind the scenes"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/",
            "text": "Examples\n\u00b6\n\n\nHere you can find a bunch of simple examples for using ct, with some explanations about what they do. The examples here are in no way comprehensive, for a full list of all the options present in ct check out the \nconfiguration specification\n.\n\n\nUsers and groups\n\u00b6\n\n\npasswd:\n  users:\n    - name: core\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n\n\n\nThis example modifies the existing \ncore\n user, giving it a known password hash (this will enable login via password), and setting its ssh key.\n\n\npasswd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n        - key2\n    - name: user2\n      ssh_authorized_keys:\n        - key3\n\n\n\nThis example will create two users, \nuser1\n and \nuser2\n. The first user has a password set and two ssh public keys authorized to log in as the user. The second user doesn't have a password set (so log in via password will be disabled), but have one ssh key.\n\n\npasswd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n      home_dir: /home/user1\n      no_create_home: true\n      groups:\n        - wheel\n        - plugdev\n      shell: /bin/bash\n\n\n\nThis example creates one user, \nuser1\n, with the password hash \n$6$43y3tkl...\n, and sets up one ssh public key for the user. The user is also given the home directory \n/home/user1\n, but it's not created, the user is added to the \nwheel\n and \nplugdev\n groups, and the user's shell is set to \n/bin/bash\n.\n\n\nGenerating a password hash\n\u00b6\n\n\nIf you choose to use a password instead of an SSH key, generating a safe hash is extremely important to the security of your system. Simplified hashes like md5crypt are trivial to crack on modern GPU hardware. Here are a few ways to generate secure hashes:\n\n\n# On Debian/Ubuntu (via the package \"whois\")\nmkpasswd --method=SHA-512 --rounds=4096\n\n# OpenSSL (note: this will only make md5crypt.  While better than plantext it should not be considered fully secure)\nopenssl passwd -1\n\n# Python\npython -c \"import crypt,random,string; print(crypt.crypt(input('clear-text password: '), '\\$6\\$' + ''.join([random.choice(string.ascii_letters + string.digits) for _ in range(16)])))\"\n\n# Perl (change password and salt values)\nperl -e 'print crypt(\"password\",\"\\$6\\$SALT\\$\") . \"\\n\"'\n\n\n\nUsing a higher number of rounds will help create more secure passwords, but given enough time, password hashes can be reversed.  On most RPM based distributions there is a tool called mkpasswd available in the \nexpect\n package, but this does not handle \"rounds\" nor advanced hashing algorithms.\n\n\nStorage and files\n\u00b6\n\n\nFiles\n\u00b6\n\n\nstorage:\n  files:\n    - path: /opt/file1\n      filesystem: root\n      contents:\n        inline: Hello, world!\n      mode: 0644\n      user:\n        id: 500\n      group:\n        id: 501\n\n\n\nThis example creates a file at \n/opt/file\n with the contents \nHello, world!\n, permissions 0644 (so readable and writable by the owner, and only readable by everyone else), and the file is owned by user uid 500 and gid 501.\n\n\nstorage:\n  files:\n    - path: /opt/file2\n      filesystem: root\n      contents:\n        remote:\n          url: http://example.com/file2\n          compression: gzip\n          verification:\n            hash:\n              function: sha512\n              sum: 4ee6a9d20cc0e6c7ee187daffa6822bdef7f4cebe109eff44b235f97e45dc3d7a5bb932efc841192e46618f48a6f4f5bc0d15fd74b1038abf46bf4b4fd409f2e\n      mode: 0644\n\n\n\nThis example fetches a gzip-compressed file from \nhttp://example.com/file2\n, makes sure that it matches the provided sha512 hash, and writes it to \n/opt/file2\n.\n\n\nFilesystems\n\u00b6\n\n\nstorage:\n  filesystems:\n    - name: filesystem1\n      mount:\n        device: /dev/disk/by-partlabel/ROOT\n        format: btrfs\n        wipe_filesystem: true\n        label: ROOT\n\n\n\nThis example formats the root filesystem to be \nbtrfs\n, and names it \nfilesystem1\n (primarily for use in the \nfiles\n section).\n\n\nsystemd units\n\u00b6\n\n\nsystemd:\n  units:\n    - name: etcd-member.service\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=infra0\"\n\n\n\nThis example adds a drop-in for the \netcd-member\n unit, setting the name for etcd to \ninfra0\n with an environment variable. More information on systemd dropins can be found in \nthe docs\n.\n\n\nsystemd:\n  units:\n    - name: hello.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=A hello world unit!\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo \"Hello, World!\"\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nThis example creates a new systemd unit called hello.service, enables it so it will run on boot, and defines the contents to simply echo \n\"Hello, World!\"\n.\n\n\nnetworkd units\n\u00b6\n\n\nnetworkd:\n  units:\n    - name: static.network\n      contents: |\n        [Match]\n        Name=enp2s0\n\n        [Network]\n        Address=192.168.0.15/24\n        Gateway=192.168.0.1\n\n\n\nThis example creates a networkd unit to set the IP address on the \nenp2s0\n interface to the static address \n192.168.0.15/24\n, and sets an appropriate gateway. More information on networkd units in CoreOS can be found in \nthe docs\n.\n\n\netcd\n\u00b6\n\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"\n\n\n\nThis example will create a dropin for the \netcd-member\n systemd unit, configuring it to use the specified version and adding all the specified options. This will also enable the \netcd-member\n unit.\n\n\nThis is referencing dynamic data that isn't known until an instance is booted. For more information on how this works, please take a look at the \nreferencing dynamic data\n document.\n\n\nUpdates and Locksmithd\n\u00b6\n\n\nupdate:\n  group:  \"beta\"\nlocksmith:\n  reboot_strategy: \"etcd-lock\"\n  window_start:    \"Sun 1:00\"\n  window_length:   \"2h\"\n\n\n\nThis example configures the Container Linux instance to be a member of the beta group, configures locksmithd to acquire a lock in etcd before rebooting for an update, and only allows reboots during a 2 hour window starting at 1 AM on Sundays.",
            "title": "CL Config Examples"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#examples",
            "text": "Here you can find a bunch of simple examples for using ct, with some explanations about what they do. The examples here are in no way comprehensive, for a full list of all the options present in ct check out the  configuration specification .",
            "title": "Examples"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#users-and-groups",
            "text": "passwd:\n  users:\n    - name: core\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1  This example modifies the existing  core  user, giving it a known password hash (this will enable login via password), and setting its ssh key.  passwd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n        - key2\n    - name: user2\n      ssh_authorized_keys:\n        - key3  This example will create two users,  user1  and  user2 . The first user has a password set and two ssh public keys authorized to log in as the user. The second user doesn't have a password set (so log in via password will be disabled), but have one ssh key.  passwd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n      home_dir: /home/user1\n      no_create_home: true\n      groups:\n        - wheel\n        - plugdev\n      shell: /bin/bash  This example creates one user,  user1 , with the password hash  $6$43y3tkl... , and sets up one ssh public key for the user. The user is also given the home directory  /home/user1 , but it's not created, the user is added to the  wheel  and  plugdev  groups, and the user's shell is set to  /bin/bash .",
            "title": "Users and groups"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#generating-a-password-hash",
            "text": "If you choose to use a password instead of an SSH key, generating a safe hash is extremely important to the security of your system. Simplified hashes like md5crypt are trivial to crack on modern GPU hardware. Here are a few ways to generate secure hashes:  # On Debian/Ubuntu (via the package \"whois\")\nmkpasswd --method=SHA-512 --rounds=4096\n\n# OpenSSL (note: this will only make md5crypt.  While better than plantext it should not be considered fully secure)\nopenssl passwd -1\n\n# Python\npython -c \"import crypt,random,string; print(crypt.crypt(input('clear-text password: '), '\\$6\\$' + ''.join([random.choice(string.ascii_letters + string.digits) for _ in range(16)])))\"\n\n# Perl (change password and salt values)\nperl -e 'print crypt(\"password\",\"\\$6\\$SALT\\$\") . \"\\n\"'  Using a higher number of rounds will help create more secure passwords, but given enough time, password hashes can be reversed.  On most RPM based distributions there is a tool called mkpasswd available in the  expect  package, but this does not handle \"rounds\" nor advanced hashing algorithms.",
            "title": "Generating a password hash"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#storage-and-files",
            "text": "",
            "title": "Storage and files"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#files",
            "text": "storage:\n  files:\n    - path: /opt/file1\n      filesystem: root\n      contents:\n        inline: Hello, world!\n      mode: 0644\n      user:\n        id: 500\n      group:\n        id: 501  This example creates a file at  /opt/file  with the contents  Hello, world! , permissions 0644 (so readable and writable by the owner, and only readable by everyone else), and the file is owned by user uid 500 and gid 501.  storage:\n  files:\n    - path: /opt/file2\n      filesystem: root\n      contents:\n        remote:\n          url: http://example.com/file2\n          compression: gzip\n          verification:\n            hash:\n              function: sha512\n              sum: 4ee6a9d20cc0e6c7ee187daffa6822bdef7f4cebe109eff44b235f97e45dc3d7a5bb932efc841192e46618f48a6f4f5bc0d15fd74b1038abf46bf4b4fd409f2e\n      mode: 0644  This example fetches a gzip-compressed file from  http://example.com/file2 , makes sure that it matches the provided sha512 hash, and writes it to  /opt/file2 .",
            "title": "Files"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#filesystems",
            "text": "storage:\n  filesystems:\n    - name: filesystem1\n      mount:\n        device: /dev/disk/by-partlabel/ROOT\n        format: btrfs\n        wipe_filesystem: true\n        label: ROOT  This example formats the root filesystem to be  btrfs , and names it  filesystem1  (primarily for use in the  files  section).",
            "title": "Filesystems"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#systemd-units",
            "text": "systemd:\n  units:\n    - name: etcd-member.service\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=infra0\"  This example adds a drop-in for the  etcd-member  unit, setting the name for etcd to  infra0  with an environment variable. More information on systemd dropins can be found in  the docs .  systemd:\n  units:\n    - name: hello.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=A hello world unit!\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo \"Hello, World!\"\n\n        [Install]\n        WantedBy=multi-user.target  This example creates a new systemd unit called hello.service, enables it so it will run on boot, and defines the contents to simply echo  \"Hello, World!\" .",
            "title": "systemd units"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#networkd-units",
            "text": "networkd:\n  units:\n    - name: static.network\n      contents: |\n        [Match]\n        Name=enp2s0\n\n        [Network]\n        Address=192.168.0.15/24\n        Gateway=192.168.0.1  This example creates a networkd unit to set the IP address on the  enp2s0  interface to the static address  192.168.0.15/24 , and sets an appropriate gateway. More information on networkd units in CoreOS can be found in  the docs .",
            "title": "networkd units"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#etcd",
            "text": "etcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"  This example will create a dropin for the  etcd-member  systemd unit, configuring it to use the specified version and adding all the specified options. This will also enable the  etcd-member  unit.  This is referencing dynamic data that isn't known until an instance is booted. For more information on how this works, please take a look at the  referencing dynamic data  document.",
            "title": "etcd"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#updates-and-locksmithd",
            "text": "update:\n  group:  \"beta\"\nlocksmith:\n  reboot_strategy: \"etcd-lock\"\n  window_start:    \"Sun 1:00\"\n  window_length:   \"2h\"  This example configures the Container Linux instance to be a member of the beta group, configures locksmithd to acquire a lock in etcd before rebooting for an update, and only allows reboots during a 2 hour window starting at 1 AM on Sundays.",
            "title": "Updates and Locksmithd"
        },
        {
            "location": "/container-linux-config-transpiler/doc/configuration/",
            "text": "Configuration Specification\n\u00b6\n\n\nA Container Linux Configuration, to be processed by ct, is a YAML document conforming to the following specification:\n\n\nNote: all fields are optional unless otherwise marked\n\n\n\n\nignition\n (object): metadata about the configuration itself.\n\n\nconfig\n (objects): options related to the configuration.\n\n\nappend\n (list of objects): a list of the configs to be appended to the current config.\n\n\nsource\n (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and \ndata\n. Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the config.\n\n\nhash\n (object): the hash of the config\n\n\nfunction\n (string): the function used to hash the config. Supported functions are sha512.\n\n\nsum\n (string): the resulting sum of the hash applied to the contents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreplace\n (object): the config that will replace the current.\n\n\nsource\n (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and \ndata\n. Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the config.\n\n\nhash\n (object): the hash of the config\n\n\nfunction\n (string): the function used to hash the config. Supported functions are sha512.\n\n\nsum\n (string): the resulting sum of the hash applied to the contents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntimeouts\n (object): options relating to http timeouts when fetching files over http or https.\n\n\nhttp_response_headers\n (integer): the time to wait (in seconds) for the server's response headers (but not the body) after making a request. 0 indicates no timeout. Default is 10 seconds.\n\n\nhttp_total\n (integer): the time limit (in seconds) for the operation (connection, request, and response), including retries. 0 indicates no timeout. Default is 0.\n\n\n\n\n\n\nsecurity\n (object): options relating to network security.\n\n\ntls\n (object): options relating to TLS when fetching resources over \nhttps\n.\n\n\ncertificate_authorities\n (object): the list of additional certificate authorities (in addition to the system authorities) to be used for TLS verification when fetching over \nhttps\n.\n\n\nsource\n (string, required): the URL of the certificate (in PEM format). Supported schemes are \nhttp\n, \nhttps\n, \ns3\n, \ntftp\n, and \ndata\n. Note: When using \nhttp\n, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the certificate.\n\n\nhash\n (string): the hash of the certificate, in the form \n<type>-<value>\n where type is sha512.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstorage\n (object): describes the desired state of the system's storage devices.\n\n\ndisks\n (list of objects): the list of disks to be configured and their options.\n\n\ndevice\n (string, required): the absolute path to the device. Devices are typically referenced by the \n/dev/disk/by-*\n symlinks.\n\n\nwipe_table\n (boolean): whether or not the partition tables shall be wiped. When true, the partition tables are erased before any further manipulation. Otherwise, the existing entries are left intact.\n\n\npartitions\n (list of objects): the list of partitions and their configuration for this particular disk.\n\n\nlabel\n (string): the PARTLABEL for the partition.\n\n\nnumber\n (integer): the partition number, which dictates it's position in the partition table (one-indexed). If zero, use the next available partition slot.\n\n\nsize\n (string): the size of the partition with a unit (KiB, MiB, GiB). If zero, the partition will fill the remainder of the disk.\n\n\nstart\n (string): the start of the partition with a unit (KiB, MiB, GiB). If zero, the partition will be positioned at the earliest available part of the disk.\n\n\ntype_guid\n (string): the GPT \npartition type GUID\n. If omitted, the default will be 0FC63DAF-8483-4772-8E79-3D69D8477DE4 (Linux filesystem data). The keywords \nlinux_filesystem_data\n, \nraid_partition\n, \nswap_partition\n, and \nraid_containing_root\n can also be used.\n\n\nguid\n (string): the GPT unique partition GUID.\n\n\n\n\n\n\n\n\n\n\nraid\n (list of objects): the list of RAID arrays to be configured.\n\n\nname\n (string, required): the name to use for the resulting md device.\n\n\nlevel\n (string, required): the redundancy level of the array (e.g. linear, raid1, raid5, etc.).\n\n\ndevices\n (list of strings, required): the list of devices (referenced by their absolute path) in the array.\n\n\nspares\n (integer): the number of spares (if applicable) in the array.\n\n\noptions\n (list of strings): any additional options to be passed to mdadm.\n\n\n\n\n\n\nfilesystems\n (list of objects): the list of filesystems to be configured and/or used in the \"files\" section. Either \"mount\" or \"path\" needs to be specified.\n\n\nname\n (string): the identifier for the filesystem, internal to Ignition. This is only required if the filesystem needs to be referenced in the \"files\" section.\n\n\nmount\n (object): contains the set of mount and formatting options for the filesystem. A non-null entry indicates that the filesystem should be mounted before it is used by Ignition.\n\n\ndevice\n (string, required): the absolute path to the device. Devices are typically referenced by the \n/dev/disk/by-*\n symlinks.\n\n\nformat\n (string, required): the filesystem format (ext4, btrfs, or xfs).\n\n\nwipe_filesystem\n (boolean): whether or not to wipe the device before filesystem creation, see \nIgnition's documentation on filesystems\n for more information.\n\n\nlabel\n (string): the label of the filesystem.\n\n\nuuid\n (string): the uuid of the filesystem.\n\n\noptions\n (list of strings): any additional options to be passed to the format-specific mkfs utility.\n\n\ncreate\n (object, DEPRECATED): contains the set of options to be used when creating the filesystem. A non-null entry indicates that the filesystem shall be created.\n\n\nforce\n (boolean, DEPRECATED): whether or not the create operation shall overwrite an existing filesystem.\n\n\noptions\n (list of strings, DEPRECATED): any additional options to be passed to the format-specific mkfs utility.\n\n\n\n\n\n\n\n\n\n\npath\n (string): the mount-point of the filesystem. A non-null entry indicates that the filesystem has already been mounted by the system at the specified path. This is really only useful for \"/sysroot\".\n\n\n\n\n\n\nfiles\n (list of objects): the list of files, rooted in this particular filesystem, to be written.\n\n\nfilesystem\n (string, required): the internal identifier of the filesystem. This matches the last filesystem with the given identifier.\n\n\npath\n (string, required): the absolute path to the file.\n\n\noverwrite\n (boolean): whether to delete preexisting nodes at the path. Defaults to true.\n\n\nappend\n (boolean): whether to append to the specified file. Creates a new file if nothing exists at the path. Cannot be set if overwrite is set to true.\n\n\ncontents\n (object): options related to the contents of the file.\n\n\ninline\n (string): the contents of the file.\n\n\nlocal\n (string): the path to a local file, relative to the \n--files-dir\n directory. When using local files, the \n--files-dir\n flag must be passed to \nct\n. The file contents are included in the generated config.\n\n\nremote\n (object): options related to the fetching of remote file contents. Remote files are fetched by Ignition when Ignition runs, the contents are not included in the generated config.\n\n\ncompression\n (string): the type of compression used on the contents (null or gzip)\n\n\nurl\n (string): the URL of the file contents. Supported schemes are http, https, tftp, s3, and \ndata\n. Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the file contents.\n\n\nhash\n (object): the hash of the config\n\n\nfunction\n (string): the function used to hash the config. Supported functions are sha512.\n\n\nsum\n (string): the resulting sum of the hash applied to the contents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmode\n (integer): the file's permission mode.\n\n\nuser\n (object): specifies the file's owner.\n\n\nid\n (integer): the user ID of the owner.\n\n\nname\n (string): the user name of the owner.\n\n\n\n\n\n\ngroup\n (object): specifies the group of the owner.\n\n\nid\n (integer): the group ID of the owner.\n\n\nname\n (string): the group name of the owner.\n\n\n\n\n\n\n\n\n\n\ndirectories\n (list of objects): the list of directories to be created.\n\n\nfilesystem\n (string, required): the internal identifier of the filesystem in which to create the directory. This matches the last filesystem with the given identifier.\n\n\npath\n (string, required): the absolute path to the directory.\n\n\noverwrite\n (boolean): whether to delete preexisting nodes at the path.\n\n\nmode\n (integer): the directory's permission mode.\n\n\nuser\n (object): specifies the directory's owner.\n\n\nid\n (integer): the user ID of the owner.\n\n\nname\n (string): the user name of the owner.\n\n\n\n\n\n\ngroup\n (object): specifies the group of the owner.\n\n\nid\n (integer): the group ID of the owner.\n\n\nname\n (string): the group name of the owner.\n\n\n\n\n\n\n\n\n\n\nlinks\n (list of objects): the list of links to be created\n\n\nfilesystem\n (string, required): the internal identifier of the filesystem in which to write the link. This matches the last filesystem with the given identifier.\n\n\npath\n (string, required): the absolute path to the link\n\n\noverwrite\n (boolean): whether to delete preexisting nodes at the path.\n\n\nuser\n (object): specifies the symbolic link's owner.\n\n\nid\n (integer): the user ID of the owner.\n\n\nname\n (string): the user name of the owner.\n\n\n\n\n\n\ngroup\n (object): specifies the group of the owner.\n\n\nid\n (integer): the group ID of the owner.\n\n\nname\n (string): the group name of the owner.\n\n\n\n\n\n\ntarget\n (string, required): the target path of the link\n\n\nhard\n (boolean): a symbolic link is created if this is false, a hard one if this is true.\n\n\n\n\n\n\n\n\n\n\nsystemd\n (object): describes the desired state of the systemd units.\n\n\nunits\n (list of objects): the list of systemd units.\n\n\nname\n (string, required): the name of the unit. This must be suffixed with a valid unit type (e.g. \"thing.service\").\n\n\nenable\n (boolean, DEPRECATED): whether or not the service shall be enabled. When true, the service is enabled. In order for this to have any effect, the unit must have an install section.\n\n\nenabled\n (boolean): whether or not the service shall be enabled. When true, the service is enabled. When false, the service is disabled. When omitted, the service is unmodified. In order for this to have any effect, the unit must have an install section.\n\n\nmask\n (boolean): whether or not the service shall be masked. When true, the service is masked by symlinking it to \n/dev/null\n.\n\n\ncontents\n (string): the contents of the unit.\n\n\ndropins\n (list of objects): the list of drop-ins for the unit.\n\n\nname\n (string, required): the name of the drop-in. This must be suffixed with \".conf\".\n\n\ncontents\n (string): the contents of the drop-in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnetworkd\n (object): describes the desired state of the networkd files.\n\n\nunits\n (list of objects): the list of networkd files.\n\n\nname\n (string, required): the name of the file. This must be suffixed with a valid unit type (e.g. \"00-eth0.network\").\n\n\ncontents\n (string): the contents of the networkd file.\n\n\ndropins\n (list of objects): the list of drop-ins for the unit.\n\n\nname\n (string, required): the name of the drop-in. This must be suffixed with \".conf\".\n\n\ncontents\n (string): the contents of the drop-in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npasswd\n (object): describes the desired additions to the passwd database.\n\n\nusers\n (list of objects): the list of accounts that shall exist.\n\n\nname\n (string, required): the username for the account.\n\n\npassword_hash\n (string): the encrypted password for the account.\n\n\nssh_authorized_keys\n (list of strings): a list of SSH keys to be added to the user's authorized_keys.\n\n\nuid\n (integer): the user ID of the account.\n\n\ngecos\n (string): the GECOS field of the account.\n\n\nhome_dir\n (string): the home directory of the account.\n\n\nno_create_home\n (boolean): whether or not to create the user's home directory. This only has an effect if the account doesn't exist yet.\n\n\nprimary_group\n (string): the name of the primary group of the account.\n\n\ngroups\n (list of strings): the list of supplementary groups of the account.\n\n\nno_user_group\n (boolean): whether or not to create a group with the same name as the user. This only has an effect if the account doesn't exist yet.\n\n\nno_log_init\n (boolean): whether or not to add the user to the lastlog and faillog databases. This only has an effect if the account doesn't exist yet.\n\n\nshell\n (string): the login shell of the new account.\n\n\nsystem\n (bool): whether or not to make the account a system account. This only has an effect if the account doesn't exist yet.\n\n\ncreate\n (object, DEPRECATED): contains the set of options to be used when creating the user. A non-null entry indicates that the user account shall be created.\n\n\nuid\n (integer, DEPRECATED): the user ID of the new account.\n\n\ngecos\n (string, DEPRECATED): the GECOS field of the new account.\n\n\nhome_dir\n (string, DEPRECATED): the home directory of the new account.\n\n\nno_create_home\n (boolean, DEPRECATED): whether or not to create the user's home directory.\n\n\nprimary_group\n (string, DEPRECATED): the name or ID of the primary group of the new account.\n\n\ngroups\n (list of strings, DEPRECATED): the list of supplementary groups of the new account.\n\n\nno_user_group\n (boolean, DEPRECATED): whether or not to create a group with the same name as the user.\n\n\nno_log_init\n (boolean, DEPRECATED): whether or not to add the user to the lastlog and faillog databases.\n\n\nshell\n (string, DEPRECATED): the login shell of the new account.\n\n\n\n\n\n\n\n\n\n\ngroups\n (list of objects): the list of groups to be added.\n\n\nname\n (string, required): the name of the group.\n\n\ngid\n (integer): the group ID of the new group.\n\n\npassword_hash\n (string): the encrypted password of the new group.\n\n\n\n\n\n\n\n\n\n\netcd\n\n\nversion\n (string): the version of etcd to be run\n\n\nother options\n (string): this section accepts any valid etcd options for the version of etcd specified. For a comprehensive list, please consult etcd's documentation. Note all options here should be in snake_case, not spine-case.\n\n\n\n\n\n\nflannel\n\n\nversion\n (string): the version of flannel to be run\n\n\nnetwork_config\n (string): the flannel configuration to be written into etcd before flannel starts.\n\n\nother options\n (string): this section accepts any valid flannel options for the version of flannel specified. For a comprehensive list, please consult flannel's documentation. Note all options here should be in snake_case, not spine-case.\n\n\n\n\n\n\ndocker\n\n\nflags\n (list of strings): additional flags to pass to the docker daemon when it is started\n\n\n\n\n\n\nupdate\n\n\ngroup\n (string): the update group to follow. Most users will want one of: stable, beta, alpha.\n\n\nserver\n (string): the server to fetch updates from.\n\n\n\n\n\n\nlocksmith\n\n\nreboot_strategy\n (string): the reboot strategy for locksmithd to follow. Must be one of: reboot, etcd-lock, off.\n\n\nwindow_start\n (string, required if window-length isn't empty): the start of the window that locksmithd can reboot the machine during\n\n\nwindow_length\n (string, required if window-start isn't empty): the duration of the window that locksmithd can reboot the machine during\n\n\ngroup\n (string): the locksmith etcd group to be part of for reboot control\n\n\netcd_endpoints\n (string): the endpoints of etcd locksmith should use\n\n\netcd_cafile\n (string): the tls CA file to use when communicating with etcd\n\n\netcd_certfile\n (string): the tls cert file to use when communicating with etcd\n\n\netcd_keyfile\n (string): the tls key file to use when communicating with etcd",
            "title": "CL Config Spec"
        },
        {
            "location": "/container-linux-config-transpiler/doc/configuration/#configuration-specification",
            "text": "A Container Linux Configuration, to be processed by ct, is a YAML document conforming to the following specification:  Note: all fields are optional unless otherwise marked   ignition  (object): metadata about the configuration itself.  config  (objects): options related to the configuration.  append  (list of objects): a list of the configs to be appended to the current config.  source  (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and  data . Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the config.  hash  (object): the hash of the config  function  (string): the function used to hash the config. Supported functions are sha512.  sum  (string): the resulting sum of the hash applied to the contents.        replace  (object): the config that will replace the current.  source  (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and  data . Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the config.  hash  (object): the hash of the config  function  (string): the function used to hash the config. Supported functions are sha512.  sum  (string): the resulting sum of the hash applied to the contents.          timeouts  (object): options relating to http timeouts when fetching files over http or https.  http_response_headers  (integer): the time to wait (in seconds) for the server's response headers (but not the body) after making a request. 0 indicates no timeout. Default is 10 seconds.  http_total  (integer): the time limit (in seconds) for the operation (connection, request, and response), including retries. 0 indicates no timeout. Default is 0.    security  (object): options relating to network security.  tls  (object): options relating to TLS when fetching resources over  https .  certificate_authorities  (object): the list of additional certificate authorities (in addition to the system authorities) to be used for TLS verification when fetching over  https .  source  (string, required): the URL of the certificate (in PEM format). Supported schemes are  http ,  https ,  s3 ,  tftp , and  data . Note: When using  http , it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the certificate.  hash  (string): the hash of the certificate, in the form  <type>-<value>  where type is sha512.            storage  (object): describes the desired state of the system's storage devices.  disks  (list of objects): the list of disks to be configured and their options.  device  (string, required): the absolute path to the device. Devices are typically referenced by the  /dev/disk/by-*  symlinks.  wipe_table  (boolean): whether or not the partition tables shall be wiped. When true, the partition tables are erased before any further manipulation. Otherwise, the existing entries are left intact.  partitions  (list of objects): the list of partitions and their configuration for this particular disk.  label  (string): the PARTLABEL for the partition.  number  (integer): the partition number, which dictates it's position in the partition table (one-indexed). If zero, use the next available partition slot.  size  (string): the size of the partition with a unit (KiB, MiB, GiB). If zero, the partition will fill the remainder of the disk.  start  (string): the start of the partition with a unit (KiB, MiB, GiB). If zero, the partition will be positioned at the earliest available part of the disk.  type_guid  (string): the GPT  partition type GUID . If omitted, the default will be 0FC63DAF-8483-4772-8E79-3D69D8477DE4 (Linux filesystem data). The keywords  linux_filesystem_data ,  raid_partition ,  swap_partition , and  raid_containing_root  can also be used.  guid  (string): the GPT unique partition GUID.      raid  (list of objects): the list of RAID arrays to be configured.  name  (string, required): the name to use for the resulting md device.  level  (string, required): the redundancy level of the array (e.g. linear, raid1, raid5, etc.).  devices  (list of strings, required): the list of devices (referenced by their absolute path) in the array.  spares  (integer): the number of spares (if applicable) in the array.  options  (list of strings): any additional options to be passed to mdadm.    filesystems  (list of objects): the list of filesystems to be configured and/or used in the \"files\" section. Either \"mount\" or \"path\" needs to be specified.  name  (string): the identifier for the filesystem, internal to Ignition. This is only required if the filesystem needs to be referenced in the \"files\" section.  mount  (object): contains the set of mount and formatting options for the filesystem. A non-null entry indicates that the filesystem should be mounted before it is used by Ignition.  device  (string, required): the absolute path to the device. Devices are typically referenced by the  /dev/disk/by-*  symlinks.  format  (string, required): the filesystem format (ext4, btrfs, or xfs).  wipe_filesystem  (boolean): whether or not to wipe the device before filesystem creation, see  Ignition's documentation on filesystems  for more information.  label  (string): the label of the filesystem.  uuid  (string): the uuid of the filesystem.  options  (list of strings): any additional options to be passed to the format-specific mkfs utility.  create  (object, DEPRECATED): contains the set of options to be used when creating the filesystem. A non-null entry indicates that the filesystem shall be created.  force  (boolean, DEPRECATED): whether or not the create operation shall overwrite an existing filesystem.  options  (list of strings, DEPRECATED): any additional options to be passed to the format-specific mkfs utility.      path  (string): the mount-point of the filesystem. A non-null entry indicates that the filesystem has already been mounted by the system at the specified path. This is really only useful for \"/sysroot\".    files  (list of objects): the list of files, rooted in this particular filesystem, to be written.  filesystem  (string, required): the internal identifier of the filesystem. This matches the last filesystem with the given identifier.  path  (string, required): the absolute path to the file.  overwrite  (boolean): whether to delete preexisting nodes at the path. Defaults to true.  append  (boolean): whether to append to the specified file. Creates a new file if nothing exists at the path. Cannot be set if overwrite is set to true.  contents  (object): options related to the contents of the file.  inline  (string): the contents of the file.  local  (string): the path to a local file, relative to the  --files-dir  directory. When using local files, the  --files-dir  flag must be passed to  ct . The file contents are included in the generated config.  remote  (object): options related to the fetching of remote file contents. Remote files are fetched by Ignition when Ignition runs, the contents are not included in the generated config.  compression  (string): the type of compression used on the contents (null or gzip)  url  (string): the URL of the file contents. Supported schemes are http, https, tftp, s3, and  data . Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the file contents.  hash  (object): the hash of the config  function  (string): the function used to hash the config. Supported functions are sha512.  sum  (string): the resulting sum of the hash applied to the contents.          mode  (integer): the file's permission mode.  user  (object): specifies the file's owner.  id  (integer): the user ID of the owner.  name  (string): the user name of the owner.    group  (object): specifies the group of the owner.  id  (integer): the group ID of the owner.  name  (string): the group name of the owner.      directories  (list of objects): the list of directories to be created.  filesystem  (string, required): the internal identifier of the filesystem in which to create the directory. This matches the last filesystem with the given identifier.  path  (string, required): the absolute path to the directory.  overwrite  (boolean): whether to delete preexisting nodes at the path.  mode  (integer): the directory's permission mode.  user  (object): specifies the directory's owner.  id  (integer): the user ID of the owner.  name  (string): the user name of the owner.    group  (object): specifies the group of the owner.  id  (integer): the group ID of the owner.  name  (string): the group name of the owner.      links  (list of objects): the list of links to be created  filesystem  (string, required): the internal identifier of the filesystem in which to write the link. This matches the last filesystem with the given identifier.  path  (string, required): the absolute path to the link  overwrite  (boolean): whether to delete preexisting nodes at the path.  user  (object): specifies the symbolic link's owner.  id  (integer): the user ID of the owner.  name  (string): the user name of the owner.    group  (object): specifies the group of the owner.  id  (integer): the group ID of the owner.  name  (string): the group name of the owner.    target  (string, required): the target path of the link  hard  (boolean): a symbolic link is created if this is false, a hard one if this is true.      systemd  (object): describes the desired state of the systemd units.  units  (list of objects): the list of systemd units.  name  (string, required): the name of the unit. This must be suffixed with a valid unit type (e.g. \"thing.service\").  enable  (boolean, DEPRECATED): whether or not the service shall be enabled. When true, the service is enabled. In order for this to have any effect, the unit must have an install section.  enabled  (boolean): whether or not the service shall be enabled. When true, the service is enabled. When false, the service is disabled. When omitted, the service is unmodified. In order for this to have any effect, the unit must have an install section.  mask  (boolean): whether or not the service shall be masked. When true, the service is masked by symlinking it to  /dev/null .  contents  (string): the contents of the unit.  dropins  (list of objects): the list of drop-ins for the unit.  name  (string, required): the name of the drop-in. This must be suffixed with \".conf\".  contents  (string): the contents of the drop-in.        networkd  (object): describes the desired state of the networkd files.  units  (list of objects): the list of networkd files.  name  (string, required): the name of the file. This must be suffixed with a valid unit type (e.g. \"00-eth0.network\").  contents  (string): the contents of the networkd file.  dropins  (list of objects): the list of drop-ins for the unit.  name  (string, required): the name of the drop-in. This must be suffixed with \".conf\".  contents  (string): the contents of the drop-in.        passwd  (object): describes the desired additions to the passwd database.  users  (list of objects): the list of accounts that shall exist.  name  (string, required): the username for the account.  password_hash  (string): the encrypted password for the account.  ssh_authorized_keys  (list of strings): a list of SSH keys to be added to the user's authorized_keys.  uid  (integer): the user ID of the account.  gecos  (string): the GECOS field of the account.  home_dir  (string): the home directory of the account.  no_create_home  (boolean): whether or not to create the user's home directory. This only has an effect if the account doesn't exist yet.  primary_group  (string): the name of the primary group of the account.  groups  (list of strings): the list of supplementary groups of the account.  no_user_group  (boolean): whether or not to create a group with the same name as the user. This only has an effect if the account doesn't exist yet.  no_log_init  (boolean): whether or not to add the user to the lastlog and faillog databases. This only has an effect if the account doesn't exist yet.  shell  (string): the login shell of the new account.  system  (bool): whether or not to make the account a system account. This only has an effect if the account doesn't exist yet.  create  (object, DEPRECATED): contains the set of options to be used when creating the user. A non-null entry indicates that the user account shall be created.  uid  (integer, DEPRECATED): the user ID of the new account.  gecos  (string, DEPRECATED): the GECOS field of the new account.  home_dir  (string, DEPRECATED): the home directory of the new account.  no_create_home  (boolean, DEPRECATED): whether or not to create the user's home directory.  primary_group  (string, DEPRECATED): the name or ID of the primary group of the new account.  groups  (list of strings, DEPRECATED): the list of supplementary groups of the new account.  no_user_group  (boolean, DEPRECATED): whether or not to create a group with the same name as the user.  no_log_init  (boolean, DEPRECATED): whether or not to add the user to the lastlog and faillog databases.  shell  (string, DEPRECATED): the login shell of the new account.      groups  (list of objects): the list of groups to be added.  name  (string, required): the name of the group.  gid  (integer): the group ID of the new group.  password_hash  (string): the encrypted password of the new group.      etcd  version  (string): the version of etcd to be run  other options  (string): this section accepts any valid etcd options for the version of etcd specified. For a comprehensive list, please consult etcd's documentation. Note all options here should be in snake_case, not spine-case.    flannel  version  (string): the version of flannel to be run  network_config  (string): the flannel configuration to be written into etcd before flannel starts.  other options  (string): this section accepts any valid flannel options for the version of flannel specified. For a comprehensive list, please consult flannel's documentation. Note all options here should be in snake_case, not spine-case.    docker  flags  (list of strings): additional flags to pass to the docker daemon when it is started    update  group  (string): the update group to follow. Most users will want one of: stable, beta, alpha.  server  (string): the server to fetch updates from.    locksmith  reboot_strategy  (string): the reboot strategy for locksmithd to follow. Must be one of: reboot, etcd-lock, off.  window_start  (string, required if window-length isn't empty): the start of the window that locksmithd can reboot the machine during  window_length  (string, required if window-start isn't empty): the duration of the window that locksmithd can reboot the machine during  group  (string): the locksmith etcd group to be part of for reboot control  etcd_endpoints  (string): the endpoints of etcd locksmith should use  etcd_cafile  (string): the tls CA file to use when communicating with etcd  etcd_certfile  (string): the tls cert file to use when communicating with etcd  etcd_keyfile  (string): the tls key file to use when communicating with etcd",
            "title": "Configuration Specification"
        },
        {
            "location": "/os/booting-on-ec2/",
            "text": "Running Flatcar Container Linux on EC2\n\u00b6\n\n\nThe current AMIs for all Flatcar Container Linux channels and EC2 regions are listed below and updated frequently. Using CloudFormation is the easiest way to launch a cluster, but it is also possible to follow the manual steps at the end of the article. Questions can be directed to the Flatcar Container Linux \nIRC channel\n or \nuser mailing list\n.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n        View as json feed: \namd64\n\n        {% if site.data.alpha-channel-arm.amis.size > 0 %}\n        \narm64\n\n        {% endif %}\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.alpha-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.alpha-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n        View as json feed: \namd64\n\n        {% if site.data.beta-channel-arm.amis.size > 0 %}\n        \narm64\n\n        {% endif %}\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.beta-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.beta-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.edge-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.edge-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n        View as json feed: \namd64\n\n        {% if site.data.stable-channel-arm.amis.size > 0 %}\n        \narm64\n\n        {% endif %}\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.stable-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.stable-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nCloudFormation will launch a cluster of Flatcar Container Linux machines with a security and autoscaling group.\n\n\nContainer Linux Configs\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n.\n\n\nYou can provide a raw Ignition config to Flatcar Container Linux via the Amazon web console or \nvia the EC2 API\n.\n\n\nAs an example, this Container Linux Config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nInstance storage\n\u00b6\n\n\nEphemeral disks and additional EBS volumes attached to instances can be mounted with a \n.mount\n unit. Amazon's block storage devices are attached differently \ndepending on the instance type\n. Here's the Container Linux Config to format and mount the first ephemeral disk, \nxvdb\n, on most instance types:\n\n\nstorage:\n  filesystems:\n    - mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\n\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target\n\n\n\nFor more information about mounting storage, Amazon's \nown documentation\n is the best source. You can also read about \nmounting storage on Flatcar Container Linux\n.\n\n\nAdding more machines\n\u00b6\n\n\nTo add more instances to the cluster, just launch more with the same Container Linux Config, the appropriate security group and the AMI for that region. New instances will join the cluster regardless of region if the security groups are configured correctly.\n\n\nSSH to your instances\n\u00b6\n\n\nFlatcar Container Linux is set up to be a little more secure than other cloud images. By default, it uses the \ncore\n user instead of \nroot\n and doesn't use a password for authentication. You'll need to add an SSH key(s) via the AWS console or add keys/passwords via your Container Linux Config in order to log in.\n\n\nTo connect to an instance after it's created, run:\n\n\nssh core@<ip address>\n\n\n\nMultiple clusters\n\u00b6\n\n\nIf you would like to create multiple clusters you will need to change the \"Stack Name\". You can find the direct \ntemplate file on S3\n.\n\n\nManual setup\n\u00b6\n\n\n{% for region in site.data.alpha-channel.amis %}\n  {% if region.name == 'us-east-1' %}\n\nTL;DR:\n launch three instances of \n{{region.hvm}}\n (amd64) in \n{{region.name}}\n with a security group that has open port 22, 2379, 2380, 4001, and 7001 and the same \"User Data\" of each host. SSH uses the \ncore\n user and you have \netcd\n and \nDocker\n to play with.\n  {% endif %}\n{% endfor %}\n\n\nCreating the security group\n\u00b6\n\n\nYou need open port 2379, 2380, 7001 and 4001 between servers in the \netcd\n cluster. Step by step instructions below.\n\n\nThis step is only needed once\n\n\nFirst we need to create a security group to allow Flatcar Container Linux instances to communicate with one another.\n\n\n\n\nGo to the \nsecurity group\n page in the EC2 console.\n\n\nClick \"Create Security Group\"\n\n\nName: flatcar-testing\n\n\nDescription: Flatcar Container Linux instances\n\n\nVPC: No VPC\n\n\nClick: \"Yes, Create\"\n\n\n\n\n\n\nIn the details of the security group, click the \nInbound\n tab\n\n\nFirst, create a security group rule for SSH\n\n\nCreate a new rule: \nSSH\n\n\nSource: 0.0.0.0/0\n\n\nClick: \"Add Rule\"\n\n\n\n\n\n\nAdd two security group rules for etcd communication\n\n\nCreate a new rule: \nCustom TCP rule\n\n\nPort range: 2379\n\n\nSource: type \"flatcar-testing\" until your security group auto-completes. Should be something like \"sg-8d4feabc\"\n\n\nClick: \"Add Rule\"\n\n\nRepeat this process for port range 2380, 4001 and 7001 as well\n\n\n\n\n\n\nClick \"Apply Rule Changes\"\n\n\n\n\nLaunching a test cluster\n\u00b6\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n  \n\n  \n\n    \n\n      \nWe will be launching three instances, with a few parameters in the User Data, and selecting our security group.\n\n      \n\n        \n\n        {% for region in site.data.alpha-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the \nquick launch wizard\n to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n        \n\n        \n\n          On the second page of the wizard, launch 3 servers to test our clustering\n          \n\n            \nNumber of instances: 3\n\n            \nClick \"Continue\"\n\n          \n\n        \n\n        \n\n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at \nhttps://discovery.etcd.io/new?size=3\n, configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n        \n\n        \n\n          Use \nct\n to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:3\u0003\n          \n\n            \nPaste configuration into \"User Data\"\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Storage Configuration\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Tags\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Create Key Pair\n          \n\n            \nChoose a key of your choice, it will be added in addition to the one in the gist.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Choose one or more of your existing Security Groups\n          \n\n            \n\"flatcar-testing\" as above.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Launch!\n        \n\n      \n\n    \n\n    \n\n      \nWe will be launching three instances, with a few parameters in the User Data, and selecting our security group.\n\n      \n\n        \n\n        {% for region in site.data.beta-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the \nquick launch wizard\n to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n        \n\n        \n\n          On the second page of the wizard, launch 3 servers to test our clustering\n          \n\n            \nNumber of instances: 3\n\n            \nClick \"Continue\"\n\n          \n\n        \n\n        \n\n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at \nhttps://discovery.etcd.io/new?size=3\n, configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n        \n\n        \n\n          Use \nct\n to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:4\u0003\n          \n\n            \nPaste configuration into \"User Data\"\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Storage Configuration\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Tags\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Create Key Pair\n          \n\n            \nChoose a key of your choice, it will be added in addition to the one in the gist.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Choose one or more of your existing Security Groups\n          \n\n            \n\"flatcar-testing\" as above.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Launch!\n        \n\n      \n\n    \n\n    \n\n      \nWe will be launching three instances, with a few parameters in the User Data, and selecting our security group.\n\n      \n\n        \n\n        {% for region in site.data.stable-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the \nquick launch wizard\n to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n        \n\n        \n\n          On the second page of the wizard, launch 3 servers to test our clustering\n          \n\n            \nNumber of instances: 3\n\n            \nClick \"Continue\"\n\n          \n\n        \n\n        \n\n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at \nhttps://discovery.etcd.io/new?size=3\n, configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n        \n\n        \n\n          Use \nct\n to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:5\u0003\n          \n\n            \nPaste configuration into \"User Data\"\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Storage Configuration\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Tags\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Create Key Pair\n          \n\n            \nChoose a key of your choice, it will be added in addition to the one in the gist.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Choose one or more of your existing Security Groups\n          \n\n            \n\"flatcar-testing\" as above.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Launch!\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nInstallation from a VMDK image\n\u00b6\n\n\nOne of the possible ways of installation is to import the generated VMDK Flatcar image as a snapshot. The image file will be in \nhttps://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2\n.\nMake sure you download the signature (it's available in \nhttps://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2.sig\n) and check it before proceeding.\n\n\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2.sig\n$ gpg --verify flatcar_production_ami_vmdk_image.vmdk.bz2.sig\ngpg: assuming signed data in 'flatcar_production_ami_vmdk_image.vmdk.bz2'\ngpg: Signature made Thu 15 Mar 2018 10:27:57 AM CET\ngpg:                using RSA key A621F1DA96C93C639506832D603443A1D0FC498C\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" [ultimate]\n\n\n\nThen, follow the instructions in \nImporting a Disk as a Snapshot Using VM Import/Export\n. You'll need to upload the uncompressed vmdk file to S3.\n\n\nAfter the snapshot is imported, you can go to \"Snapshots\" in the EC2 dashboard, and generate an AMI image from it.\nTo make it work, use \n/dev/sda2\n as the \"Root device name\" and you probably want to select \"Hardware-assisted virtualization\" as \"Virtualization type\".\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Amazon EC2"
        },
        {
            "location": "/os/booting-on-ec2/#running-flatcar-container-linux-on-ec2",
            "text": "The current AMIs for all Flatcar Container Linux channels and EC2 regions are listed below and updated frequently. Using CloudFormation is the easiest way to launch a cluster, but it is also possible to follow the manual steps at the end of the article. Questions can be directed to the Flatcar Container Linux  IRC channel  or  user mailing list .",
            "title": "Running Flatcar Container Linux on EC2"
        },
        {
            "location": "/os/booting-on-ec2/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n        View as json feed:  amd64 \n        {% if site.data.alpha-channel-arm.amis.size > 0 %}\n         arm64 \n        {% endif %}\n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.alpha-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.alpha-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n        View as json feed:  amd64 \n        {% if site.data.beta-channel-arm.amis.size > 0 %}\n         arm64 \n        {% endif %}\n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.beta-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.beta-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.edge-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.edge-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n        View as json feed:  amd64 \n        {% if site.data.stable-channel-arm.amis.size > 0 %}\n         arm64 \n        {% endif %}\n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.stable-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.stable-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     CloudFormation will launch a cluster of Flatcar Container Linux machines with a security and autoscaling group.",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-on-ec2/#container-linux-configs",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features .  You can provide a raw Ignition config to Flatcar Container Linux via the Amazon web console or  via the EC2 API .  As an example, this Container Linux Config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-on-ec2/#instance-storage",
            "text": "Ephemeral disks and additional EBS volumes attached to instances can be mounted with a  .mount  unit. Amazon's block storage devices are attached differently  depending on the instance type . Here's the Container Linux Config to format and mount the first ephemeral disk,  xvdb , on most instance types:  storage:\n  filesystems:\n    - mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\n\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target  For more information about mounting storage, Amazon's  own documentation  is the best source. You can also read about  mounting storage on Flatcar Container Linux .",
            "title": "Instance storage"
        },
        {
            "location": "/os/booting-on-ec2/#adding-more-machines",
            "text": "To add more instances to the cluster, just launch more with the same Container Linux Config, the appropriate security group and the AMI for that region. New instances will join the cluster regardless of region if the security groups are configured correctly.",
            "title": "Adding more machines"
        },
        {
            "location": "/os/booting-on-ec2/#ssh-to-your-instances",
            "text": "Flatcar Container Linux is set up to be a little more secure than other cloud images. By default, it uses the  core  user instead of  root  and doesn't use a password for authentication. You'll need to add an SSH key(s) via the AWS console or add keys/passwords via your Container Linux Config in order to log in.  To connect to an instance after it's created, run:  ssh core@<ip address>",
            "title": "SSH to your instances"
        },
        {
            "location": "/os/booting-on-ec2/#multiple-clusters",
            "text": "If you would like to create multiple clusters you will need to change the \"Stack Name\". You can find the direct  template file on S3 .",
            "title": "Multiple clusters"
        },
        {
            "location": "/os/booting-on-ec2/#manual-setup",
            "text": "{% for region in site.data.alpha-channel.amis %}\n  {% if region.name == 'us-east-1' %} TL;DR:  launch three instances of  {{region.hvm}}  (amd64) in  {{region.name}}  with a security group that has open port 22, 2379, 2380, 4001, and 7001 and the same \"User Data\" of each host. SSH uses the  core  user and you have  etcd  and  Docker  to play with.\n  {% endif %}\n{% endfor %}",
            "title": "Manual setup"
        },
        {
            "location": "/os/booting-on-ec2/#creating-the-security-group",
            "text": "You need open port 2379, 2380, 7001 and 4001 between servers in the  etcd  cluster. Step by step instructions below.  This step is only needed once  First we need to create a security group to allow Flatcar Container Linux instances to communicate with one another.   Go to the  security group  page in the EC2 console.  Click \"Create Security Group\"  Name: flatcar-testing  Description: Flatcar Container Linux instances  VPC: No VPC  Click: \"Yes, Create\"    In the details of the security group, click the  Inbound  tab  First, create a security group rule for SSH  Create a new rule:  SSH  Source: 0.0.0.0/0  Click: \"Add Rule\"    Add two security group rules for etcd communication  Create a new rule:  Custom TCP rule  Port range: 2379  Source: type \"flatcar-testing\" until your security group auto-completes. Should be something like \"sg-8d4feabc\"  Click: \"Add Rule\"  Repeat this process for port range 2380, 4001 and 7001 as well    Click \"Apply Rule Changes\"",
            "title": "Creating the security group"
        },
        {
            "location": "/os/booting-on-ec2/#launching-a-test-cluster",
            "text": "Stable Channel \n     Beta Channel \n     Alpha Channel \n   \n   \n     \n       We will be launching three instances, with a few parameters in the User Data, and selecting our security group. \n       \n         \n        {% for region in site.data.alpha-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the  quick launch wizard  to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n         \n         \n          On the second page of the wizard, launch 3 servers to test our clustering\n           \n             Number of instances: 3 \n             Click \"Continue\" \n           \n         \n         \n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at  https://discovery.etcd.io/new?size=3 , configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n         \n         \n          Use  ct  to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:3\u0003\n           \n             Paste configuration into \"User Data\" \n             \"Continue\" \n           \n         \n         \n          Storage Configuration\n           \n             \"Continue\" \n           \n         \n         \n          Tags\n           \n             \"Continue\" \n           \n         \n         \n          Create Key Pair\n           \n             Choose a key of your choice, it will be added in addition to the one in the gist. \n             \"Continue\" \n           \n         \n         \n          Choose one or more of your existing Security Groups\n           \n             \"flatcar-testing\" as above. \n             \"Continue\" \n           \n         \n         \n          Launch!\n         \n       \n     \n     \n       We will be launching three instances, with a few parameters in the User Data, and selecting our security group. \n       \n         \n        {% for region in site.data.beta-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the  quick launch wizard  to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n         \n         \n          On the second page of the wizard, launch 3 servers to test our clustering\n           \n             Number of instances: 3 \n             Click \"Continue\" \n           \n         \n         \n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at  https://discovery.etcd.io/new?size=3 , configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n         \n         \n          Use  ct  to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:4\u0003\n           \n             Paste configuration into \"User Data\" \n             \"Continue\" \n           \n         \n         \n          Storage Configuration\n           \n             \"Continue\" \n           \n         \n         \n          Tags\n           \n             \"Continue\" \n           \n         \n         \n          Create Key Pair\n           \n             Choose a key of your choice, it will be added in addition to the one in the gist. \n             \"Continue\" \n           \n         \n         \n          Choose one or more of your existing Security Groups\n           \n             \"flatcar-testing\" as above. \n             \"Continue\" \n           \n         \n         \n          Launch!\n         \n       \n     \n     \n       We will be launching three instances, with a few parameters in the User Data, and selecting our security group. \n       \n         \n        {% for region in site.data.stable-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the  quick launch wizard  to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n         \n         \n          On the second page of the wizard, launch 3 servers to test our clustering\n           \n             Number of instances: 3 \n             Click \"Continue\" \n           \n         \n         \n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at  https://discovery.etcd.io/new?size=3 , configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n         \n         \n          Use  ct  to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:5\u0003\n           \n             Paste configuration into \"User Data\" \n             \"Continue\" \n           \n         \n         \n          Storage Configuration\n           \n             \"Continue\" \n           \n         \n         \n          Tags\n           \n             \"Continue\" \n           \n         \n         \n          Create Key Pair\n           \n             Choose a key of your choice, it will be added in addition to the one in the gist. \n             \"Continue\" \n           \n         \n         \n          Choose one or more of your existing Security Groups\n           \n             \"flatcar-testing\" as above. \n             \"Continue\" \n           \n         \n         \n          Launch!",
            "title": "Launching a test cluster"
        },
        {
            "location": "/os/booting-on-ec2/#installation-from-a-vmdk-image",
            "text": "One of the possible ways of installation is to import the generated VMDK Flatcar image as a snapshot. The image file will be in  https://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2 .\nMake sure you download the signature (it's available in  https://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2.sig ) and check it before proceeding.  $ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2.sig\n$ gpg --verify flatcar_production_ami_vmdk_image.vmdk.bz2.sig\ngpg: assuming signed data in 'flatcar_production_ami_vmdk_image.vmdk.bz2'\ngpg: Signature made Thu 15 Mar 2018 10:27:57 AM CET\ngpg:                using RSA key A621F1DA96C93C639506832D603443A1D0FC498C\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" [ultimate]  Then, follow the instructions in  Importing a Disk as a Snapshot Using VM Import/Export . You'll need to upload the uncompressed vmdk file to S3.  After the snapshot is imported, you can go to \"Snapshots\" in the EC2 dashboard, and generate an AMI image from it.\nTo make it work, use  /dev/sda2  as the \"Root device name\" and you probably want to select \"Hardware-assisted virtualization\" as \"Virtualization type\".",
            "title": "Installation from a VMDK image"
        },
        {
            "location": "/os/booting-on-ec2/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-azure/",
            "text": "Running Flatcar Container Linux on Microsoft Azure\n\u00b6\n\n\nCreating resource group via Microsoft Azure CLI\n\u00b6\n\n\nFollow the \ninstallation and configuration guides\n for the Microsoft Azure CLI to set up your local installation.\n\n\nInstances on Microsoft Azure must be created within a resource group. Create a new resource group with the following command:\n\n\naz group create --name group-1 --location <location>\n\n\n\nNow that you have a resource group, you can choose a channel of Flatcar Container Linux you would like to install.\n\n\nChoosing a Channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be \nupdated automatically\n with different schedules per channel. This feature\ncan be \ndisabled\n, although it is not recommended to do so. The \nrelease notes\n contain\ninformation about specific features and bug fixes.\n\n\nThe following command will create a single instance. For more details, check out \nLaunching via the Microsoft Azure CLI\n.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within\n        the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-stable\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-beta\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks the master branch and is released frequently. The newest versions of system\n        libraries and utilities are available for testing in this channel. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n        and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-edge\n\n      \n\n    \n\n  \n\n\n\n\n\nUploading an Image\n\u00b6\n\n\nOfficial Flatcar Container Linux images are not available on Azure at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.\n\n\nTo do so, run the following command:\n\ndocker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload \\\n  --resource-group <resource group> \\\n  --storage-account-name <storage account name>\n\n\nWhere:\n\n\n\n\n<resource group>\n should be a valid \nResource Group\n name.\n\n\n<storage account name>\n should be a valid \nStorage Account\n name.\n\n\n\n\nDuring execution, the script will ask you to log into your Azure account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to Azure.\n\n\nIf uploading fails with one of the following errors, it usually indicates a problem on Azure's side:\n\n\nPut https://mystorage.blob.core.windows.net/vhds?restype=container: dial tcp: lookup iago-dev.blob.core.windows.net on 80.58.61.250:53: no such host\n\n\n\nstorage: service returned error: StatusCode=403, ErrorCode=AuthenticationFailed, ErrorMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. RequestId:a3ed1ebc-701e-010c-5258-0a2e84000000 Time:2019-05-14T13:26:00.1253383Z, RequestId=a3ed1ebc-701e-010c-5258-0a2e84000000, QueryParameterName=, QueryParameterValue=\n\n\n\nThe command is idempotent and it is therefore safe to re-run it in case of failure.\n\n\nTo see all available options, run:\n\ndocker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -g, --resource-group        Azure resource group.\n  -s, --storage-account-name  Azure storage account name. Must be between 3 and 24 characters and unique within Azure.\n\n Optional arguments:\n  -c, --channel              Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version              Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name           Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n  -l, --location             Azure location to storage image. To list available locations run with '--locations'. Defaults to 'westeurope'.\n  -S, --storage-account-type Type of storage account. Defaults to 'Standard_LRS'.\n\n\nThe Dockerfile for the \nquay.io/kinvolk/azure-flatcar-image-upload\n image is managed \nhere\n.\n\n\nContainer Linux Config\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more\nvia a Container Linux Config. Head over to the \nprovisioning docs\n to learn how to use Container Linux Configs.\nNote that Microsoft Azure doesn't allow an instance's userdata to be modified after the instance had been launched. This\nisn't a problem since Ignition, the tool that consumes the userdata, only runs on the first boot.\n\n\nYou can provide a raw Ignition config (produced from a Container Linux Config) to Flatcar Container Linux via the \nMicrosoft Azure CLI\n.\n\n\nAs an example, the following config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nLaunching Instances via the Microsoft Azure CLI\n\u00b6\n\n\nYou can lunch instance of Flatcar Container Linux by executing following command:\n\n\naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nFor information on using Flatcar Container Linux check out the \nFlatcar Container Linux quickstart guide\n or dive into \nmore specific topics\n.",
            "title": "Azure"
        },
        {
            "location": "/os/booting-on-azure/#running-flatcar-container-linux-on-microsoft-azure",
            "text": "",
            "title": "Running Flatcar Container Linux on Microsoft Azure"
        },
        {
            "location": "/os/booting-on-azure/#creating-resource-group-via-microsoft-azure-cli",
            "text": "Follow the  installation and configuration guides  for the Microsoft Azure CLI to set up your local installation.  Instances on Microsoft Azure must be created within a resource group. Create a new resource group with the following command:  az group create --name group-1 --location <location>  Now that you have a resource group, you can choose a channel of Flatcar Container Linux you would like to install.",
            "title": "Creating resource group via Microsoft Azure CLI"
        },
        {
            "location": "/os/booting-on-azure/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be  updated automatically  with different schedules per channel. This feature\ncan be  disabled , although it is not recommended to do so. The  release notes  contain\ninformation about specific features and bug fixes.  The following command will create a single instance. For more details, check out  Launching via the Microsoft Azure CLI .  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within\n        the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-stable \n       \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-beta \n       \n     \n     \n       \n         The Alpha channel closely tracks the master branch and is released frequently. The newest versions of system\n        libraries and utilities are available for testing in this channel. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha \n       \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n        and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-edge",
            "title": "Choosing a Channel"
        },
        {
            "location": "/os/booting-on-azure/#uploading-an-image",
            "text": "Official Flatcar Container Linux images are not available on Azure at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.  To do so, run the following command: docker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload \\\n  --resource-group <resource group> \\\n  --storage-account-name <storage account name>  Where:   <resource group>  should be a valid  Resource Group  name.  <storage account name>  should be a valid  Storage Account  name.   During execution, the script will ask you to log into your Azure account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to Azure.  If uploading fails with one of the following errors, it usually indicates a problem on Azure's side:  Put https://mystorage.blob.core.windows.net/vhds?restype=container: dial tcp: lookup iago-dev.blob.core.windows.net on 80.58.61.250:53: no such host  storage: service returned error: StatusCode=403, ErrorCode=AuthenticationFailed, ErrorMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. RequestId:a3ed1ebc-701e-010c-5258-0a2e84000000 Time:2019-05-14T13:26:00.1253383Z, RequestId=a3ed1ebc-701e-010c-5258-0a2e84000000, QueryParameterName=, QueryParameterValue=  The command is idempotent and it is therefore safe to re-run it in case of failure.  To see all available options, run: docker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -g, --resource-group        Azure resource group.\n  -s, --storage-account-name  Azure storage account name. Must be between 3 and 24 characters and unique within Azure.\n\n Optional arguments:\n  -c, --channel              Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version              Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name           Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n  -l, --location             Azure location to storage image. To list available locations run with '--locations'. Defaults to 'westeurope'.\n  -S, --storage-account-type Type of storage account. Defaults to 'Standard_LRS'.  The Dockerfile for the  quay.io/kinvolk/azure-flatcar-image-upload  image is managed  here .",
            "title": "Uploading an Image"
        },
        {
            "location": "/os/booting-on-azure/#container-linux-config",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more\nvia a Container Linux Config. Head over to the  provisioning docs  to learn how to use Container Linux Configs.\nNote that Microsoft Azure doesn't allow an instance's userdata to be modified after the instance had been launched. This\nisn't a problem since Ignition, the tool that consumes the userdata, only runs on the first boot.  You can provide a raw Ignition config (produced from a Container Linux Config) to Flatcar Container Linux via the  Microsoft Azure CLI .  As an example, the following config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/booting-on-azure/#launching-instances-via-the-microsoft-azure-cli",
            "text": "You can lunch instance of Flatcar Container Linux by executing following command:  az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha",
            "title": "Launching Instances via the Microsoft Azure CLI"
        },
        {
            "location": "/os/booting-on-azure/#using-flatcar-container-linux",
            "text": "For information on using Flatcar Container Linux check out the  Flatcar Container Linux quickstart guide  or dive into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-digitalocean/",
            "text": "Running Flatcar Container Linux on DigitalOcean\n\u00b6\n\n\nOn Digital Ocean, users can upload Flatcar Container Linux as a \ncustom image\n. Digital Ocean offers a \nquick start guide\n that walks you through the process.\n\n\nThe \nimport URL\n should be \nhttps://<channel>.release.flatcar-linux.net/amd64-usr/<version>/flatcar_production_digitalocean_image.bin.bz2\n. See the \nrelease page\n for version and channel history.\n\n\nThe following command will create a single droplet. For more details, check out \nLaunching via the API\n.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Container Linux {{site.alpha-channel}}.\n\n        \nLaunch Flatcar Container Linux Droplet\n\n        \nLaunch via DigitalOcean API by specifying \n$REGION\n, \n$SIZE\n and \n$SSH_KEY_ID\n:\n\n        \ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-alpha\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Container Linux {{site.beta-channel}}.\n\n        \nLaunch Flatcar Container Linux Droplet\n\n        \nLaunch via DigitalOcean API by specifying \n$REGION\n, \n$SIZE\n and \n$SSH_KEY_ID\n:\n\n        \ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-beta\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'\n\n      \n\n    \n\n    \n\n      \n\n        \n\n        \nThe Stable channel should be used by production clusters. Versions of Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Container Linux {{site.stable-channel}}.\n\n        \nLaunch Flatcar Container Linux Droplet\n\n        \nLaunch via DigitalOcean API by specifying \n$REGION\n, \n$SIZE\n and \n$SSH_KEY_ID\n:\n\n        \ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-stable\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'\n\n      \n\n      \n\n    \n\n  \n\n\n\n\n\nContainer Linux Configs\n\u00b6\n\n\nContainer Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n. Note that DigitalOcean doesn't allow an instance's userdata to be modified after the instance has been launched. This isn't a problem since Ignition only runs on the first boot.\n\n\nYou can provide a raw Ignition config to Container Linux via the DigitalOcean web console or \nvia the DigitalOcean API\n.\n\n\nAs an example, this config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nAdding more machines\n\u00b6\n\n\nTo add more instances to the cluster, just launch more with the same Container Linux Config. New instances will join the cluster regardless of region.\n\n\nSSH to your droplets\n\u00b6\n\n\nContainer Linux is set up to be a little more secure than other DigitalOcean images. By default, it uses the core user instead of root and doesn't use a password for authentication. You'll need to add an SSH key(s) via the web console or add keys/passwords via your Ignition config in order to log in.\n\n\nTo connect to a droplet after it's created, run:\n\n\nssh core@<ip address>\n\n\n\nLaunching droplets\n\u00b6\n\n\nVia the API\n\u00b6\n\n\nFor starters, generate a \nPersonal Access Token\n and save it in an environment variable:\n\n\nread TOKEN\n# Enter your Personal Access Token\n\n\n\nUpload your SSH key via \nDigitalOcean's API\n or the web console. Retrieve the SSH key ID via the \n\"list all keys\"\n method:\n\n\ncurl --request GET \"https://api.digitalocean.com/v2/account/keys\" \\\n     --header \"Authorization: Bearer $TOKEN\"\n\n\n\nSave the key ID from the previous command in an environment variable:\n\n\nread SSH_KEY_ID\n# Enter your SSH key ID\n\n\n\nCreate a 512MB droplet with private networking in NYC3 from the Container Linux Stable image:\n\n\ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\n      \"region\":\"nyc3\",\n      \"image\":\"coreos-stable\",\n      \"size\":\"512mb\",\n      \"name\":\"core-1\",\n      \"private_networking\":true,\n      \"ssh_keys\":['$SSH_KEY_ID'],\n      \"user_data\": \"'\"$(cat config.ign | sed 's/\"/\\\\\"/g')\"'\"\n}'\n\n\n\n\nFor more details, check out \nDigitalOcean's API documentation\n.\n\n\nVia the web console\n\u00b6\n\n\n\n\nOpen the \n\"new droplet\"\n page in the web console.\n\n\nGive the machine a hostname, select the size, and choose a region.\n\n\n  \n\n    \n\n    \nChoosing a size and hostname\n\n  \n\n\n\n\nEnable User Data and add your Ignition config in the text box.\n\n\n  \n\n    \n\n    \nDroplet settings for networking and Ignition\n\n  \n\n\n\n\nChoose your \npreferred channel\n of Container Linux.\n\n\n  \n\n    \n\n    \nChoosing a Container Linux channel\n\n  \n\n\n\n\nSelect your SSH keys.\n\n\n\n\nNote that DigitalOcean is not able to inject a root password into Flatcar Container Linux images like it does with other images. You'll need to add your keys via the web console or add keys or passwords via your Container Linux Config in order to log in.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Digital Ocean"
        },
        {
            "location": "/os/booting-on-digitalocean/#running-flatcar-container-linux-on-digitalocean",
            "text": "On Digital Ocean, users can upload Flatcar Container Linux as a  custom image . Digital Ocean offers a  quick start guide  that walks you through the process.  The  import URL  should be  https://<channel>.release.flatcar-linux.net/amd64-usr/<version>/flatcar_production_digitalocean_image.bin.bz2 . See the  release page  for version and channel history.  The following command will create a single droplet. For more details, check out  Launching via the API .  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n   \n   \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Container Linux {{site.alpha-channel}}. \n         Launch Flatcar Container Linux Droplet \n         Launch via DigitalOcean API by specifying  $REGION ,  $SIZE  and  $SSH_KEY_ID : \n         curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-alpha\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}' \n       \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Container Linux {{site.beta-channel}}. \n         Launch Flatcar Container Linux Droplet \n         Launch via DigitalOcean API by specifying  $REGION ,  $SIZE  and  $SSH_KEY_ID : \n         curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-beta\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}' \n       \n     \n     \n       \n         \n         The Stable channel should be used by production clusters. Versions of Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Container Linux {{site.stable-channel}}. \n         Launch Flatcar Container Linux Droplet \n         Launch via DigitalOcean API by specifying  $REGION ,  $SIZE  and  $SSH_KEY_ID : \n         curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-stable\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'",
            "title": "Running Flatcar Container Linux on DigitalOcean"
        },
        {
            "location": "/os/booting-on-digitalocean/#container-linux-configs",
            "text": "Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features . Note that DigitalOcean doesn't allow an instance's userdata to be modified after the instance has been launched. This isn't a problem since Ignition only runs on the first boot.  You can provide a raw Ignition config to Container Linux via the DigitalOcean web console or  via the DigitalOcean API .  As an example, this config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-on-digitalocean/#adding-more-machines",
            "text": "To add more instances to the cluster, just launch more with the same Container Linux Config. New instances will join the cluster regardless of region.",
            "title": "Adding more machines"
        },
        {
            "location": "/os/booting-on-digitalocean/#ssh-to-your-droplets",
            "text": "Container Linux is set up to be a little more secure than other DigitalOcean images. By default, it uses the core user instead of root and doesn't use a password for authentication. You'll need to add an SSH key(s) via the web console or add keys/passwords via your Ignition config in order to log in.  To connect to a droplet after it's created, run:  ssh core@<ip address>",
            "title": "SSH to your droplets"
        },
        {
            "location": "/os/booting-on-digitalocean/#launching-droplets",
            "text": "",
            "title": "Launching droplets"
        },
        {
            "location": "/os/booting-on-digitalocean/#via-the-api",
            "text": "For starters, generate a  Personal Access Token  and save it in an environment variable:  read TOKEN\n# Enter your Personal Access Token  Upload your SSH key via  DigitalOcean's API  or the web console. Retrieve the SSH key ID via the  \"list all keys\"  method:  curl --request GET \"https://api.digitalocean.com/v2/account/keys\" \\\n     --header \"Authorization: Bearer $TOKEN\"  Save the key ID from the previous command in an environment variable:  read SSH_KEY_ID\n# Enter your SSH key ID  Create a 512MB droplet with private networking in NYC3 from the Container Linux Stable image:  curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\n      \"region\":\"nyc3\",\n      \"image\":\"coreos-stable\",\n      \"size\":\"512mb\",\n      \"name\":\"core-1\",\n      \"private_networking\":true,\n      \"ssh_keys\":['$SSH_KEY_ID'],\n      \"user_data\": \"'\"$(cat config.ign | sed 's/\"/\\\\\"/g')\"'\"\n}'  For more details, check out  DigitalOcean's API documentation .",
            "title": "Via the API"
        },
        {
            "location": "/os/booting-on-digitalocean/#via-the-web-console",
            "text": "Open the  \"new droplet\"  page in the web console.  Give the machine a hostname, select the size, and choose a region. \n   \n     \n     Choosing a size and hostname \n     Enable User Data and add your Ignition config in the text box. \n   \n     \n     Droplet settings for networking and Ignition \n     Choose your  preferred channel  of Container Linux. \n   \n     \n     Choosing a Container Linux channel \n     Select your SSH keys.   Note that DigitalOcean is not able to inject a root password into Flatcar Container Linux images like it does with other images. You'll need to add your keys via the web console or add keys or passwords via your Container Linux Config in order to log in.",
            "title": "Via the web console"
        },
        {
            "location": "/os/booting-on-digitalocean/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-google-compute-engine/",
            "text": "Running Flatcar Container Linux on Google Compute Engine\n\u00b6\n\n\nBefore proceeding, you will need a GCE account (\nGCE free trial \n) and \ninstall gcloud\n on your machine. In each command below, be sure to insert your project name in place of \n<project-id>\n.\n\n\nAfter installation, log into your account with \ngcloud auth login\n and enter your project ID when prompted.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nCreate 3 instances from the image above using our Ignition from \nexample.ign\n:\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-stable --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-beta --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-alpha --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n      and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-edge --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n  \n\n\n\n\n\nUploading an Image\n\u00b6\n\n\nOfficial Flatcar Container Linux images are not available on Google Cloud at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.\n\n\nTo do so, run the following command:\n\ndocker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload \\\n  --bucket-name <bucket name> \\\n  --project-id <project id>\n\n\nWhere:\n\n\n\n\n<bucket name>\n should be a valid \nbucket\n name.\n\n\n<project id>\n should be your project ID.\n\n\n\n\nDuring execution, the script will ask you to log into your Google account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to the Google Cloud.\n\n\nTo see all available options, run:\n\ndocker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -b, --bucket-name Name of GCP bucket for storing images.\n  -p, --project-id  ID of the project for creating bucket.\n\n Optional arguments:\n  -c, --channel     Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version     Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name  Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n\n Optional flags:\n   -f, --force-reupload If used, image will be uploaded even if it already exist in the bucket.\n   -F, --force-recreate If user, if compute image already exist, it will be removed and recreated.\n\n\nThe Dockerfile for the \nquay.io/kinvolk/google-cloud-flatcar-image-upload\n image is managed \nhere\n.\n\n\nUpgrade from CoreOS Container Linux\n\u00b6\n\n\nYou can also \nupgrade from an existing CoreOS Container Linux system\n.\n\n\nContainer Linux Config\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n.\n\n\nYou can provide a raw Ignition config to Flatcar Container Linux via the Google Cloud console's metadata field \nuser-data\n or via a flag using \ngcloud\n.\n\n\nAs an example, this config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nAdditional storage\n\u00b6\n\n\nAdditional disks attached to instances can be mounted with a \n.mount\n unit. Each disk can be accessed via \n/dev/disk/by-id/google-<disk-name>\n. Here's the Container Linux Config to format and mount a disk called \ndatabase-backup\n:\n\n\nstorage:\n  filesystems:\n    - mount:\n        device: /dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        format: ext4\n\nsystemd:\n  units:\n    - name: media-backup.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        Where=/media/backup\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target\n\n\n\nFor more information about mounting storage, Google's \nown documentation\n is the best source. You can also read about \nmounting storage on Flatcar Container Linux\n.\n\n\nAdding more machines\n\u00b6\n\n\nTo add more instances to the cluster, just launch more with the same Ignition config inside of the project.\n\n\nSSH and users\n\u00b6\n\n\nUsers are added to Container Linux on GCE by the user provided configuration (i.e. Ignition, cloudinit) and by either the GCE account manager or \nGCE OS Login\n. OS Login is used if it is enabled for the instance, otherwise the GCE account manager is used.\n\n\nUsing the GCE account manager\n\u00b6\n\n\nYou can log in your Flatcar Container Linux instances using:\n\n\ngcloud compute ssh --zone us-central1-a core@<instance-name>\n\n\n\nUsers other than \ncore\n, which are set up by the GCE account manager, may not be a member of required groups. If you have issues, try running commands such as \njournalctl\n with sudo.\n\n\nUsing OS Login\n\u00b6\n\n\nYou can log in using your Google account on instances with OS Login enabled. OS Login needs to be \nenabled in the GCE console\n and on the instance. It is enabled by default on instances provisioned with Container Linux 1898.0.0 or later. Once enabled, you can log into your Container Linux instances using:\n\n\ngcloud compute ssh --zone us-central1-a <instance-name>\n\n\n\nThis will use your GCE user to log in.\n\n\nDisabling OS Login on newly provisioned nodes\n\u00b6\n\n\nYou can disable the OS Login functionality by masking the \noem-gce-enable-oslogin.service\n unit:\n\n\nsystemd:\n  units:\n    - name: oem-gce-enable-oslogin.service\n      mask: true\n\n\n\nWhen disabling OS Login functionality on the instance, it is also recommended to disable it in the GCE console.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Google Compute Engine"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#running-flatcar-container-linux-on-google-compute-engine",
            "text": "Before proceeding, you will need a GCE account ( GCE free trial  ) and  install gcloud  on your machine. In each command below, be sure to insert your project name in place of  <project-id> .  After installation, log into your account with  gcloud auth login  and enter your project ID when prompted.",
            "title": "Running Flatcar Container Linux on Google Compute Engine"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  Create 3 instances from the image above using our Ignition from  example.ign :  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-stable --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-beta --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign \n     \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-alpha --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n      and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-edge --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#uploading-an-image",
            "text": "Official Flatcar Container Linux images are not available on Google Cloud at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.  To do so, run the following command: docker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload \\\n  --bucket-name <bucket name> \\\n  --project-id <project id>  Where:   <bucket name>  should be a valid  bucket  name.  <project id>  should be your project ID.   During execution, the script will ask you to log into your Google account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to the Google Cloud.  To see all available options, run: docker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -b, --bucket-name Name of GCP bucket for storing images.\n  -p, --project-id  ID of the project for creating bucket.\n\n Optional arguments:\n  -c, --channel     Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version     Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name  Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n\n Optional flags:\n   -f, --force-reupload If used, image will be uploaded even if it already exist in the bucket.\n   -F, --force-recreate If user, if compute image already exist, it will be removed and recreated.  The Dockerfile for the  quay.io/kinvolk/google-cloud-flatcar-image-upload  image is managed  here .",
            "title": "Uploading an Image"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#upgrade-from-coreos-container-linux",
            "text": "You can also  upgrade from an existing CoreOS Container Linux system .",
            "title": "Upgrade from CoreOS Container Linux"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#container-linux-config",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features .  You can provide a raw Ignition config to Flatcar Container Linux via the Google Cloud console's metadata field  user-data  or via a flag using  gcloud .  As an example, this config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#additional-storage",
            "text": "Additional disks attached to instances can be mounted with a  .mount  unit. Each disk can be accessed via  /dev/disk/by-id/google-<disk-name> . Here's the Container Linux Config to format and mount a disk called  database-backup :  storage:\n  filesystems:\n    - mount:\n        device: /dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        format: ext4\n\nsystemd:\n  units:\n    - name: media-backup.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        Where=/media/backup\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target  For more information about mounting storage, Google's  own documentation  is the best source. You can also read about  mounting storage on Flatcar Container Linux .",
            "title": "Additional storage"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#adding-more-machines",
            "text": "To add more instances to the cluster, just launch more with the same Ignition config inside of the project.",
            "title": "Adding more machines"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#ssh-and-users",
            "text": "Users are added to Container Linux on GCE by the user provided configuration (i.e. Ignition, cloudinit) and by either the GCE account manager or  GCE OS Login . OS Login is used if it is enabled for the instance, otherwise the GCE account manager is used.",
            "title": "SSH and users"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#using-the-gce-account-manager",
            "text": "You can log in your Flatcar Container Linux instances using:  gcloud compute ssh --zone us-central1-a core@<instance-name>  Users other than  core , which are set up by the GCE account manager, may not be a member of required groups. If you have issues, try running commands such as  journalctl  with sudo.",
            "title": "Using the GCE account manager"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#using-os-login",
            "text": "You can log in using your Google account on instances with OS Login enabled. OS Login needs to be  enabled in the GCE console  and on the instance. It is enabled by default on instances provisioned with Container Linux 1898.0.0 or later. Once enabled, you can log into your Container Linux instances using:  gcloud compute ssh --zone us-central1-a <instance-name>  This will use your GCE user to log in.",
            "title": "Using OS Login"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#disabling-os-login-on-newly-provisioned-nodes",
            "text": "You can disable the OS Login functionality by masking the  oem-gce-enable-oslogin.service  unit:  systemd:\n  units:\n    - name: oem-gce-enable-oslogin.service\n      mask: true  When disabling OS Login functionality on the instance, it is also recommended to disable it in the GCE console.",
            "title": "Disabling OS Login on newly provisioned nodes"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-qemu/",
            "text": "Running Flatcar Container Linux on QEMU\n\u00b6\n\n\nThese instructions will bring up a single Flatcar Container Linux instance under QEMU, the small Swiss Army knife of virtual machine and CPU emulators. If you need to do more such as \nconfiguring networks\n differently refer to the \nQEMU Wiki\n and \nUser Documentation\n.\n\n\nYou can direct questions to the \nIRC channel\n or \nmailing list\n.\n\n\nInstall QEMU\n\u00b6\n\n\nIn addition to Linux it can be run on Windows and OS X but works best on Linux. It should be available on just about any distro.\n\n\nDebian or Ubuntu\n\u00b6\n\n\nDocumentation for \nDebian\n has more details but to get started all you need is:\n\n\nsudo apt-get install qemu-system-x86 qemu-utils\n\n\n\nFedora or RedHat\n\u00b6\n\n\nThe Fedora wiki has a \nquick howto\n but the basic install is easy:\n\n\nsudo yum install qemu-system-x86 qemu-img\n\n\n\nArch\n\u00b6\n\n\nThis is all you need to get started:\n\n\nsudo pacman -S qemu\n\n\n\nMore details can be found on \nArch's QEMU wiki page\n.\n\n\nGentoo\n\u00b6\n\n\nAs to be expected, Gentoo can be a little more complicated but all the required kernel options and USE flags are covered in the \nGentoo Wiki\n. Usually this should be sufficient:\n\n\necho app-emulation/qemu qemu_softmmu_targets_x86_64 virtfs xattr >> /etc/portage/package.use\nemerge -av app-emulation/qemu\n\n\n\nStartup Flatcar Container Linux\n\u00b6\n\n\nOnce QEMU is installed you can download and start the latest Flatcar Container Linux image.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n       \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n  \n\n\n\n\n\nStarting is as simple as:\n\n\n./flatcar_production_qemu.sh -nographic\n\n\n\nSSH keys\n\u00b6\n\n\nIn order to log in to the virtual machine you will need to use ssh keys. If you don't already have a ssh key pair you can generate one simply by running the command \nssh-keygen\n. The wrapper script will automatically look for public keys in ssh-agent if available and at the default locations \n~/.ssh/id_dsa.pub\n or \n~/.ssh/id_rsa.pub\n. If you need to provide an alternate location use the -a option:\n\n\n./flatcar_production_qemu.sh -a ~/.ssh/authorized_keys -- -nographic\n\n\n\nNote: Options such as \n-a\n for the wrapper script must be specified before any options for QEMU. To make the separation between the two explicit you can use \n--\n but that isn't required. See \n./flatcar_production_qemu.sh -h\n for details.\n\n\nOnce the virtual machine has started you can log in via SSH:\n\n\nssh -l core -p 2222 localhost\n\n\n\nSSH config\n\u00b6\n\n\nTo simplify this and avoid potential host key errors in the future add the following to \n~/.ssh/config\n:\n\n\nHost flatcar\nHostName localhost\nPort 2222\nUser core\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/null\n\n\n\nNow you can log in to the virtual machine with:\n\n\nssh flatcar\n\n\n\nContainer Linux Configs\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n. An Ignition config can be passed to the virtual machine using the QEMU Firmware Configuration Device. The wrapper script provides a method for doing so:\n\n\n./flatcar_production_qemu.sh -i config.ign -- -nographic\n\n\n\nThis will pass the contents of \nconfig.ign\n through to Ignition, which runs in the virtual machine.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "QEMU"
        },
        {
            "location": "/os/booting-with-qemu/#running-flatcar-container-linux-on-qemu",
            "text": "These instructions will bring up a single Flatcar Container Linux instance under QEMU, the small Swiss Army knife of virtual machine and CPU emulators. If you need to do more such as  configuring networks  differently refer to the  QEMU Wiki  and  User Documentation .  You can direct questions to the  IRC channel  or  mailing list .",
            "title": "Running Flatcar Container Linux on QEMU"
        },
        {
            "location": "/os/booting-with-qemu/#install-qemu",
            "text": "In addition to Linux it can be run on Windows and OS X but works best on Linux. It should be available on just about any distro.",
            "title": "Install QEMU"
        },
        {
            "location": "/os/booting-with-qemu/#debian-or-ubuntu",
            "text": "Documentation for  Debian  has more details but to get started all you need is:  sudo apt-get install qemu-system-x86 qemu-utils",
            "title": "Debian or Ubuntu"
        },
        {
            "location": "/os/booting-with-qemu/#fedora-or-redhat",
            "text": "The Fedora wiki has a  quick howto  but the basic install is easy:  sudo yum install qemu-system-x86 qemu-img",
            "title": "Fedora or RedHat"
        },
        {
            "location": "/os/booting-with-qemu/#arch",
            "text": "This is all you need to get started:  sudo pacman -S qemu  More details can be found on  Arch's QEMU wiki page .",
            "title": "Arch"
        },
        {
            "location": "/os/booting-with-qemu/#gentoo",
            "text": "As to be expected, Gentoo can be a little more complicated but all the required kernel options and USE flags are covered in the  Gentoo Wiki . Usually this should be sufficient:  echo app-emulation/qemu qemu_softmmu_targets_x86_64 virtfs xattr >> /etc/portage/package.use\nemerge -av app-emulation/qemu",
            "title": "Gentoo"
        },
        {
            "location": "/os/booting-with-qemu/#startup-flatcar-container-linux",
            "text": "Once QEMU is installed you can download and start the latest Flatcar Container Linux image.",
            "title": "Startup Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-qemu/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n        \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     Starting is as simple as:  ./flatcar_production_qemu.sh -nographic",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-with-qemu/#ssh-keys",
            "text": "In order to log in to the virtual machine you will need to use ssh keys. If you don't already have a ssh key pair you can generate one simply by running the command  ssh-keygen . The wrapper script will automatically look for public keys in ssh-agent if available and at the default locations  ~/.ssh/id_dsa.pub  or  ~/.ssh/id_rsa.pub . If you need to provide an alternate location use the -a option:  ./flatcar_production_qemu.sh -a ~/.ssh/authorized_keys -- -nographic  Note: Options such as  -a  for the wrapper script must be specified before any options for QEMU. To make the separation between the two explicit you can use  --  but that isn't required. See  ./flatcar_production_qemu.sh -h  for details.  Once the virtual machine has started you can log in via SSH:  ssh -l core -p 2222 localhost",
            "title": "SSH keys"
        },
        {
            "location": "/os/booting-with-qemu/#ssh-config",
            "text": "To simplify this and avoid potential host key errors in the future add the following to  ~/.ssh/config :  Host flatcar\nHostName localhost\nPort 2222\nUser core\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/null  Now you can log in to the virtual machine with:  ssh flatcar",
            "title": "SSH config"
        },
        {
            "location": "/os/booting-with-qemu/#container-linux-configs",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features . An Ignition config can be passed to the virtual machine using the QEMU Firmware Configuration Device. The wrapper script provides a method for doing so:  ./flatcar_production_qemu.sh -i config.ign -- -nographic  This will pass the contents of  config.ign  through to Ignition, which runs in the virtual machine.",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-with-qemu/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-ipxe/",
            "text": "Booting Flatcar Container Linux via iPXE\n\u00b6\n\n\nThese instructions will walk you through booting Flatcar Container Linux via iPXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be \ninstalled to disk\n.\n\n\nA minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.\n\n\nConfiguring iPXE\n\u00b6\n\n\niPXE can be used on any platform that can boot an ISO image.\nThis includes many cloud providers and physical hardware.\n\n\nTo illustrate iPXE in action we will use qemu-kvm in this guide.\n\n\nSetting up iPXE boot script\n\u00b6\n\n\nWhen configuring the Flatcar Container Linux iPXE boot script there are a few kernel options that may be useful but all are optional.\n\n\n\n\nrootfstype=tmpfs\n: Use tmpfs for the writable root filesystem. This is the default behavior.\n\n\nrootfstype=btrfs\n: Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.\n\n\nroot\n: Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g: \nroot=/dev/sda1\n, \nroot=LABEL=ROOT\n or \nroot=UUID=2c618316-d17a-4688-b43b-aa19d97ea821\n.\n\n\nsshkey\n: Add the given SSH public key to the \ncore\n user's authorized_keys file. Replace the example key below with your own (it is usually in \n~/.ssh/id_rsa.pub\n)\n\n\nconsole\n: Enable kernel output and a login prompt on a given tty. The default, \ntty0\n, generally maps to VGA. Can be used multiple times, e.g. \nconsole=tty0 console=ttyS0\n\n\nflatcar.autologin\n: Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the \nconsole\n option, e.g. \nconsole=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0\n. Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals (\ntty1\n, \ntty2\n, etc), not the VGA console itself (\ntty0\n).\n\n\nflatcar.first_boot=1\n: Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the \nconfig transpiler documentation\n for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.\n\n\nignition.config.url\n: Download the Ignition config from the specified URL. \nhttp\n, \nhttps\n, \ns3\n, and \ntftp\n schemes are supported.\n\n\nip\n: Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See \nIgnition documentation\n for the complete syntax.\n\n\n\n\nChoose a Channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nSetting up the Boot Script\n\u00b6\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://alpha.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://beta.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://edge.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://stable.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n  \n\n\n\n\n\nAn easy place to host this boot script is on \nhttp://pastie.org\n. Be sure to reference the \"raw\" version of script, which is accessed by clicking on the clipboard in the top right.\n\n\nBooting iPXE\n\u00b6\n\n\nFirst, download and boot the iPXE image.\nWe will use \nqemu-kvm\n in this guide but use whatever process you normally use for booting an ISO on your platform.\n\n\nwget http://boot.ipxe.org/ipxe.iso\nqemu-kvm -m 1024 ipxe.iso --curses\n\n\n\nNext press Ctrl+B to get to the iPXE prompt and type in the following commands:\n\n\niPXE> dhcp\niPXE> chain http://${YOUR_BOOT_URL}\n\n\n\nImmediately iPXE should download your boot script URL and start grabbing the images from the Flatcar Container Linux storage site:\n\n\n${YOUR_BOOT_URL}... ok\nhttp://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz... 98%\n\n\n\nAfter a few moments of downloading Flatcar Container Linux should boot normally.\n\n\nUpdate process\n\u00b6\n\n\nSince Flatcar Container Linux's upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.\n\n\nInstallation\n\u00b6\n\n\nFlatcar Container Linux can be completely installed on disk or run from RAM but store user data on disk. Read more in our \nInstalling Flatcar Container Linux guide\n.\n\n\nAdding a custom OEM\n\u00b6\n\n\nSimilar to the \nOEM partition\n in Flatcar Container Linux disk images, iPXE images can be customized with an \nIgnition config\n bundled in the initramfs. You can view the \ninstructions on the PXE docs\n.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Booting with iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#booting-flatcar-container-linux-via-ipxe",
            "text": "These instructions will walk you through booting Flatcar Container Linux via iPXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be  installed to disk .  A minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.",
            "title": "Booting Flatcar Container Linux via iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#configuring-ipxe",
            "text": "iPXE can be used on any platform that can boot an ISO image.\nThis includes many cloud providers and physical hardware.  To illustrate iPXE in action we will use qemu-kvm in this guide.",
            "title": "Configuring iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#setting-up-ipxe-boot-script",
            "text": "When configuring the Flatcar Container Linux iPXE boot script there are a few kernel options that may be useful but all are optional.   rootfstype=tmpfs : Use tmpfs for the writable root filesystem. This is the default behavior.  rootfstype=btrfs : Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.  root : Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g:  root=/dev/sda1 ,  root=LABEL=ROOT  or  root=UUID=2c618316-d17a-4688-b43b-aa19d97ea821 .  sshkey : Add the given SSH public key to the  core  user's authorized_keys file. Replace the example key below with your own (it is usually in  ~/.ssh/id_rsa.pub )  console : Enable kernel output and a login prompt on a given tty. The default,  tty0 , generally maps to VGA. Can be used multiple times, e.g.  console=tty0 console=ttyS0  flatcar.autologin : Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the  console  option, e.g.  console=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0 . Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals ( tty1 ,  tty2 , etc), not the VGA console itself ( tty0 ).  flatcar.first_boot=1 : Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the  config transpiler documentation  for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.  ignition.config.url : Download the Ignition config from the specified URL.  http ,  https ,  s3 , and  tftp  schemes are supported.  ip : Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See  Ignition documentation  for the complete syntax.",
            "title": "Setting up iPXE boot script"
        },
        {
            "location": "/os/booting-with-ipxe/#choose-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.",
            "title": "Choose a Channel"
        },
        {
            "location": "/os/booting-with-ipxe/#setting-up-the-boot-script",
            "text": "Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://alpha.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://beta.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://edge.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://stable.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     An easy place to host this boot script is on  http://pastie.org . Be sure to reference the \"raw\" version of script, which is accessed by clicking on the clipboard in the top right.",
            "title": "Setting up the Boot Script"
        },
        {
            "location": "/os/booting-with-ipxe/#booting-ipxe",
            "text": "First, download and boot the iPXE image.\nWe will use  qemu-kvm  in this guide but use whatever process you normally use for booting an ISO on your platform.  wget http://boot.ipxe.org/ipxe.iso\nqemu-kvm -m 1024 ipxe.iso --curses  Next press Ctrl+B to get to the iPXE prompt and type in the following commands:  iPXE> dhcp\niPXE> chain http://${YOUR_BOOT_URL}  Immediately iPXE should download your boot script URL and start grabbing the images from the Flatcar Container Linux storage site:  ${YOUR_BOOT_URL}... ok\nhttp://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz... 98%  After a few moments of downloading Flatcar Container Linux should boot normally.",
            "title": "Booting iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#update-process",
            "text": "Since Flatcar Container Linux's upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.",
            "title": "Update process"
        },
        {
            "location": "/os/booting-with-ipxe/#installation",
            "text": "Flatcar Container Linux can be completely installed on disk or run from RAM but store user data on disk. Read more in our  Installing Flatcar Container Linux guide .",
            "title": "Installation"
        },
        {
            "location": "/os/booting-with-ipxe/#adding-a-custom-oem",
            "text": "Similar to the  OEM partition  in Flatcar Container Linux disk images, iPXE images can be customized with an  Ignition config  bundled in the initramfs. You can view the  instructions on the PXE docs .",
            "title": "Adding a custom OEM"
        },
        {
            "location": "/os/booting-with-ipxe/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-pxe/",
            "text": "Booting Flatcar Container Linux via PXE\n\u00b6\n\n\nThese instructions will walk you through booting Flatcar Container Linux via PXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be \ninstalled to disk\n.\n\n\nA minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.\n\n\nConfiguring pxelinux\n\u00b6\n\n\nThis guide assumes you already have a working PXE server using \npxelinux\n. If you need suggestions on how to set a server up, check out guides for \nDebian\n, \nFedora\n or \nUbuntu\n.\n\n\nSetting up pxelinux.cfg\n\u00b6\n\n\nWhen configuring the Flatcar Container Linux pxelinux.cfg there are a few kernel options that may be useful but all are optional.\n\n\n\n\nrootfstype=tmpfs\n: Use tmpfs for the writable root filesystem. This is the default behavior.\n\n\nrootfstype=btrfs\n: Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.\n\n\nroot\n: Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g: \nroot=/dev/sda1\n, \nroot=LABEL=ROOT\n or \nroot=UUID=2c618316-d17a-4688-b43b-aa19d97ea821\n.\n\n\nsshkey\n: Add the given SSH public key to the \ncore\n user's authorized_keys file. Replace the example key below with your own (it is usually in \n~/.ssh/id_rsa.pub\n)\n\n\nconsole\n: Enable kernel output and a login prompt on a given tty. The default, \ntty0\n, generally maps to VGA. Can be used multiple times, e.g. \nconsole=tty0 console=ttyS0\n\n\nflatcar.autologin\n: Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the \nconsole\n option, e.g. \nconsole=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0\n. Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals (\ntty1\n, \ntty2\n, etc), not the VGA console itself (\ntty0\n).\n\n\nflatcar.first_boot=1\n: Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the \nconfig transpiler documentation\n for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.\n\n\nignition.config.url\n: Download the Ignition config from the specified URL. \nhttp\n, \nhttps\n, \ns3\n, and \ntftp\n schemes are supported.\n\n\nip\n: Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See \nIgnition documentation\n for the complete syntax.\n\n\n\n\nThis is an example pxelinux.cfg file that assumes Flatcar Container Linux is the only option. You should be able to copy this verbatim into \n/var/lib/tftpboot/pxelinux.cfg/default\n after providing an Ignition config URL:\n\n\ndefault flatcar\nprompt 1\ntimeout 15\n\ndisplay boot.msg\n\nlabel flatcar\n  menu default\n  kernel flatcar_production_pxe.vmlinuz\n  initrd flatcar_production_pxe_image.cpio.gz\n  append flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\n\n\n\nHere's a common config example which should be located at the URL from above:\n\n\nsystemd:\n  units:\n    - name: etcd2.service\n      enable: true\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq...\n\n\n\nChoose a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nPXE booted machines cannot currently update themselves when new versions are released to a channel. To update to the latest version of Flatcar Container Linux download/verify these files again and reboot.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n  \n\n\n\n\n\nBooting the box\n\u00b6\n\n\nAfter setting up the PXE server as outlined above you can start the target machine in PXE boot mode. The machine should grab the image from the server and boot into Flatcar Container Linux. If something goes wrong you can direct questions to the \nIRC channel\n or \nmailing list\n.\n\n\nThis is localhost.unknown_domain (Linux x86_64 3.10.10+) 19:53:36\nSSH host key: 24:2e:f1:3f:5f:9c:63:e5:8c:17:47:32:f4:09:5d:78 (RSA)\nSSH host key: ed:84:4d:05:e3:7d:e3:d0:b9:58:90:58:3b:99:3a:4c (DSA)\nens0: 10.0.2.15 fe80::5054:ff:fe12:3456\nlocalhost login:\n\n\n\nLogging in\n\u00b6\n\n\nThe IP address for the machine should be printed out to the terminal for convenience. If it doesn't show up immediately, press enter a few times and it should show up. Now you can simply SSH in using public key authentication:\n\n\nssh core@10.0.2.15\n\n\n\nUpdate Process\n\u00b6\n\n\nSince our upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.\n\n\nInstallation\n\u00b6\n\n\nOnce booted it is possible to \ninstall Flatcar Container Linux on a local disk\n or to just use local storage for the writable root filesystem while continuing to boot Flatcar Container Linux itself via PXE.\n\n\nIf you plan on using Docker we recommend using a local ext4 filesystem with overlayfs, however, btrfs is also available to use if needed.\n\n\nFor example, to setup an ext4 root filesystem on \n/dev/sda\n:\n\n\nstorage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: ext4\n      wipe_filesystem: true\n      label: ROOT\n\n\n\nAnd add \nroot=/dev/sda1\n or \nroot=LABEL=ROOT\n to the kernel options as documented above.\n\n\nSimilarly, to setup a btrfs root filesystem on \n/dev/sda\n:\n\n\nstorage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: btrfs\n      wipe_filesystem: true\n      label: ROOT\n\n\n\nAdding a Custom OEM\n\u00b6\n\n\nSimilar to the \nOEM partition\n in Flatcar Container Linux disk images, PXE images can be customized with an \nIgnition config\n bundled in the initramfs. Simply create a \n./usr/share/oem/\n directory, add a \nconfig.ign\n file containing the Ignition config, and add the directory tree as an additional initramfs:\n\n\nmkdir -p usr/share/oem\ncp example.ign ./usr/share/oem/config.ign\nfind usr | cpio -o -H newc -O oem.cpio\ngzip oem.cpio\n\n\n\nConfirm the archive looks correct and has your config inside of it:\n\n\ngzip --stdout --decompress oem.cpio.gz | cpio -it\n./\nusr\nusr/share\nusr/share/oem\nusr/share/oem/config.ign\n\n\n\nAdd the \noem.cpio.gz\n file to your PXE boot directory, then \nappend it\n to the \ninitrd\n line in your \npxelinux.cfg\n:\n\n\n...\ninitrd flatcar_production_pxe_image.cpio.gz,oem.cpio.gz\nkernel flatcar_production_pxe.vmlinuz flatcar.first_boot=1\n...\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Booting with PXE"
        },
        {
            "location": "/os/booting-with-pxe/#booting-flatcar-container-linux-via-pxe",
            "text": "These instructions will walk you through booting Flatcar Container Linux via PXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be  installed to disk .  A minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.",
            "title": "Booting Flatcar Container Linux via PXE"
        },
        {
            "location": "/os/booting-with-pxe/#configuring-pxelinux",
            "text": "This guide assumes you already have a working PXE server using  pxelinux . If you need suggestions on how to set a server up, check out guides for  Debian ,  Fedora  or  Ubuntu .",
            "title": "Configuring pxelinux"
        },
        {
            "location": "/os/booting-with-pxe/#setting-up-pxelinuxcfg",
            "text": "When configuring the Flatcar Container Linux pxelinux.cfg there are a few kernel options that may be useful but all are optional.   rootfstype=tmpfs : Use tmpfs for the writable root filesystem. This is the default behavior.  rootfstype=btrfs : Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.  root : Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g:  root=/dev/sda1 ,  root=LABEL=ROOT  or  root=UUID=2c618316-d17a-4688-b43b-aa19d97ea821 .  sshkey : Add the given SSH public key to the  core  user's authorized_keys file. Replace the example key below with your own (it is usually in  ~/.ssh/id_rsa.pub )  console : Enable kernel output and a login prompt on a given tty. The default,  tty0 , generally maps to VGA. Can be used multiple times, e.g.  console=tty0 console=ttyS0  flatcar.autologin : Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the  console  option, e.g.  console=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0 . Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals ( tty1 ,  tty2 , etc), not the VGA console itself ( tty0 ).  flatcar.first_boot=1 : Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the  config transpiler documentation  for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.  ignition.config.url : Download the Ignition config from the specified URL.  http ,  https ,  s3 , and  tftp  schemes are supported.  ip : Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See  Ignition documentation  for the complete syntax.   This is an example pxelinux.cfg file that assumes Flatcar Container Linux is the only option. You should be able to copy this verbatim into  /var/lib/tftpboot/pxelinux.cfg/default  after providing an Ignition config URL:  default flatcar\nprompt 1\ntimeout 15\n\ndisplay boot.msg\n\nlabel flatcar\n  menu default\n  kernel flatcar_production_pxe.vmlinuz\n  initrd flatcar_production_pxe_image.cpio.gz\n  append flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign  Here's a common config example which should be located at the URL from above:  systemd:\n  units:\n    - name: etcd2.service\n      enable: true\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq...",
            "title": "Setting up pxelinux.cfg"
        },
        {
            "location": "/os/booting-with-pxe/#choose-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  PXE booted machines cannot currently update themselves when new versions are released to a channel. To update to the latest version of Flatcar Container Linux download/verify these files again and reboot.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n       \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n       \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n       \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig",
            "title": "Choose a channel"
        },
        {
            "location": "/os/booting-with-pxe/#booting-the-box",
            "text": "After setting up the PXE server as outlined above you can start the target machine in PXE boot mode. The machine should grab the image from the server and boot into Flatcar Container Linux. If something goes wrong you can direct questions to the  IRC channel  or  mailing list .  This is localhost.unknown_domain (Linux x86_64 3.10.10+) 19:53:36\nSSH host key: 24:2e:f1:3f:5f:9c:63:e5:8c:17:47:32:f4:09:5d:78 (RSA)\nSSH host key: ed:84:4d:05:e3:7d:e3:d0:b9:58:90:58:3b:99:3a:4c (DSA)\nens0: 10.0.2.15 fe80::5054:ff:fe12:3456\nlocalhost login:",
            "title": "Booting the box"
        },
        {
            "location": "/os/booting-with-pxe/#logging-in",
            "text": "The IP address for the machine should be printed out to the terminal for convenience. If it doesn't show up immediately, press enter a few times and it should show up. Now you can simply SSH in using public key authentication:  ssh core@10.0.2.15",
            "title": "Logging in"
        },
        {
            "location": "/os/booting-with-pxe/#update-process",
            "text": "Since our upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.",
            "title": "Update Process"
        },
        {
            "location": "/os/booting-with-pxe/#installation",
            "text": "Once booted it is possible to  install Flatcar Container Linux on a local disk  or to just use local storage for the writable root filesystem while continuing to boot Flatcar Container Linux itself via PXE.  If you plan on using Docker we recommend using a local ext4 filesystem with overlayfs, however, btrfs is also available to use if needed.  For example, to setup an ext4 root filesystem on  /dev/sda :  storage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: ext4\n      wipe_filesystem: true\n      label: ROOT  And add  root=/dev/sda1  or  root=LABEL=ROOT  to the kernel options as documented above.  Similarly, to setup a btrfs root filesystem on  /dev/sda :  storage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: btrfs\n      wipe_filesystem: true\n      label: ROOT",
            "title": "Installation"
        },
        {
            "location": "/os/booting-with-pxe/#adding-a-custom-oem",
            "text": "Similar to the  OEM partition  in Flatcar Container Linux disk images, PXE images can be customized with an  Ignition config  bundled in the initramfs. Simply create a  ./usr/share/oem/  directory, add a  config.ign  file containing the Ignition config, and add the directory tree as an additional initramfs:  mkdir -p usr/share/oem\ncp example.ign ./usr/share/oem/config.ign\nfind usr | cpio -o -H newc -O oem.cpio\ngzip oem.cpio  Confirm the archive looks correct and has your config inside of it:  gzip --stdout --decompress oem.cpio.gz | cpio -it\n./\nusr\nusr/share\nusr/share/oem\nusr/share/oem/config.ign  Add the  oem.cpio.gz  file to your PXE boot directory, then  append it  to the  initrd  line in your  pxelinux.cfg :  ...\ninitrd flatcar_production_pxe_image.cpio.gz,oem.cpio.gz\nkernel flatcar_production_pxe.vmlinuz flatcar.first_boot=1\n...",
            "title": "Adding a Custom OEM"
        },
        {
            "location": "/os/booting-with-pxe/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/installing-to-disk/",
            "text": "Installing Flatcar Container Linux to disk\n\u00b6\n\n\nInstall script\n\u00b6\n\n\nThere is a simple installer that will destroy everything on the given target disk and install Flatcar Container Linux. Essentially it downloads an image, verifies it with gpg, and then copies it bit for bit to disk. An installation requires at least 8 GB of usable space on the device.\n\n\nThe script is self-contained and located \non GitHub here\n and can be run from any Linux distribution. You cannot normally install Flatcar Container Linux to the same device that is currently booted. However, the \nFlatcar Container Linux ISO\n or any Linux liveCD will allow Flatcar Container Linux to install to a non-active device.\n\n\nIf you boot Flatcar Container Linux via PXE, the install script is already installed. By default the install script will attempt to install the same version and channel that was PXE-booted:\n\n\nflatcar-install -d /dev/sda -i ignition.json\n\n\n\nignition.json\n should include user information (especially an SSH key) generated from a \nContainer Linux Config\n, or you will not be able to log into your Flatcar Container Linux instance.\n\n\nIf you are installing on VMware, pass \n-o vmware_raw\n to install the VMware-specific image:\n\n\nflatcar-install -d /dev/sda -i ignition.json -o vmware_raw\n\n\n\nChoose a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be \nupdated automatically\n with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \nIf you want to ensure you are installing the latest alpha version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C alpha\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \nIf you want to ensure you are installing the latest beta version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C beta\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \nIf you want to ensure you are installing the latest edge version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C edge\n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \nIf you want to ensure you are installing the latest stable version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C stable\n\n    \n\n  \n\n\n\n\n\nFor reference here are the rest of the \nflatcar-install\n options:\n\n\n-d DEVICE   Install Flatcar Container Linux to the given device.\n-V VERSION  Version to install (e.g. current)\n-B BOARD    Flatcar Container Linux board to use\n-C CHANNEL  Release channel to use (e.g. beta)\n-o OEM      OEM type to install (e.g. ami)\n-c CLOUD    Insert a cloud-init config to be executed on boot.\n-i IGNITION Insert an Ignition config to be executed on boot.\n-b BASEURL  URL to the image mirror (overrides BOARD)\n-k KEYFILE  Override default GPG key for verifying image signature\n-f IMAGE    Install unverified local image file to disk instead of fetching\n-n          Copy generated network units to the root partition.\n-v          Super verbose, for debugging.\n\n\n\nContainer Linux Configs\n\u00b6\n\n\nBy default there isn't a password or any other way to log into a fresh Flatcar Container Linux system. The easiest way to configure accounts, add systemd units, and more is via Container Linux Configs. Jump over to the \ndocs to learn about the supported features\n.\n\n\nAfter using the \nContainer Linux Config Transpiler\n to produce an Ignition config, the installation script will process your \nignition.json\n file specified with the \n-i\n flag and use it when the installation is booted.\n\n\nA Container Linux Config that specifies an SSH key for the \ncore\n user but doesn't use any other parameters looks like:\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\n\n\n\nNote: The \n{PRIVATE_IPV4}\n and \n{PUBLIC_IPV4}\n substitution variables referenced in other documents are not supported on libvirt.\n\n\nTo start the installation script with a reference to our Ignition config, run:\n\n\nflatcar-install -d /dev/sda -C stable -i ~/ignition.json\n\n\n\nAdvanced Container Linux Config example\n\u00b6\n\n\nThis example will configure Flatcar Container Linux components: etcd and flannel. You have to substitute \n<PEER_ADDRESS>\n to your host's IP or DNS address.\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\netcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  advertise_client_urls: http://<PEER_ADDRESS>:2379,http://<PEER_ADDRESS>:4001\n  initial_advertise_peer_urls: http://<PEER_ADDRESS>:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://<PEER_ADDRESS>:2380\nsystemd:\n  units:\n    - name: flanneld.service\n      enable: true\n      dropins:\n      - name: 50-network-config.conf\n        contents: |\n          [Service]\n          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{\"Network\":\"10.1.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}'\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Installing to Disk"
        },
        {
            "location": "/os/installing-to-disk/#installing-flatcar-container-linux-to-disk",
            "text": "",
            "title": "Installing Flatcar Container Linux to disk"
        },
        {
            "location": "/os/installing-to-disk/#install-script",
            "text": "There is a simple installer that will destroy everything on the given target disk and install Flatcar Container Linux. Essentially it downloads an image, verifies it with gpg, and then copies it bit for bit to disk. An installation requires at least 8 GB of usable space on the device.  The script is self-contained and located  on GitHub here  and can be run from any Linux distribution. You cannot normally install Flatcar Container Linux to the same device that is currently booted. However, the  Flatcar Container Linux ISO  or any Linux liveCD will allow Flatcar Container Linux to install to a non-active device.  If you boot Flatcar Container Linux via PXE, the install script is already installed. By default the install script will attempt to install the same version and channel that was PXE-booted:  flatcar-install -d /dev/sda -i ignition.json  ignition.json  should include user information (especially an SSH key) generated from a  Container Linux Config , or you will not be able to log into your Flatcar Container Linux instance.  If you are installing on VMware, pass  -o vmware_raw  to install the VMware-specific image:  flatcar-install -d /dev/sda -i ignition.json -o vmware_raw",
            "title": "Install script"
        },
        {
            "location": "/os/installing-to-disk/#choose-a-channel",
            "text": "Flatcar Container Linux is designed to be  updated automatically  with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       If you want to ensure you are installing the latest alpha version, use the  -C  option: \n       flatcar-install -d /dev/sda -C alpha \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       If you want to ensure you are installing the latest beta version, use the  -C  option: \n       flatcar-install -d /dev/sda -C beta \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       If you want to ensure you are installing the latest edge version, use the  -C  option: \n       flatcar-install -d /dev/sda -C edge \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       If you want to ensure you are installing the latest stable version, use the  -C  option: \n       flatcar-install -d /dev/sda -C stable \n     \n     For reference here are the rest of the  flatcar-install  options:  -d DEVICE   Install Flatcar Container Linux to the given device.\n-V VERSION  Version to install (e.g. current)\n-B BOARD    Flatcar Container Linux board to use\n-C CHANNEL  Release channel to use (e.g. beta)\n-o OEM      OEM type to install (e.g. ami)\n-c CLOUD    Insert a cloud-init config to be executed on boot.\n-i IGNITION Insert an Ignition config to be executed on boot.\n-b BASEURL  URL to the image mirror (overrides BOARD)\n-k KEYFILE  Override default GPG key for verifying image signature\n-f IMAGE    Install unverified local image file to disk instead of fetching\n-n          Copy generated network units to the root partition.\n-v          Super verbose, for debugging.",
            "title": "Choose a channel"
        },
        {
            "location": "/os/installing-to-disk/#container-linux-configs",
            "text": "By default there isn't a password or any other way to log into a fresh Flatcar Container Linux system. The easiest way to configure accounts, add systemd units, and more is via Container Linux Configs. Jump over to the  docs to learn about the supported features .  After using the  Container Linux Config Transpiler  to produce an Ignition config, the installation script will process your  ignition.json  file specified with the  -i  flag and use it when the installation is booted.  A Container Linux Config that specifies an SSH key for the  core  user but doesn't use any other parameters looks like:  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......  Note: The  {PRIVATE_IPV4}  and  {PUBLIC_IPV4}  substitution variables referenced in other documents are not supported on libvirt.  To start the installation script with a reference to our Ignition config, run:  flatcar-install -d /dev/sda -C stable -i ~/ignition.json",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/installing-to-disk/#advanced-container-linux-config-example",
            "text": "This example will configure Flatcar Container Linux components: etcd and flannel. You have to substitute  <PEER_ADDRESS>  to your host's IP or DNS address.  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\netcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  advertise_client_urls: http://<PEER_ADDRESS>:2379,http://<PEER_ADDRESS>:4001\n  initial_advertise_peer_urls: http://<PEER_ADDRESS>:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://<PEER_ADDRESS>:2380\nsystemd:\n  units:\n    - name: flanneld.service\n      enable: true\n      dropins:\n      - name: 50-network-config.conf\n        contents: |\n          [Service]\n          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{\"Network\":\"10.1.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}'",
            "title": "Advanced Container Linux Config example"
        },
        {
            "location": "/os/installing-to-disk/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-iso/",
            "text": "Booting Flatcar Container Linux from an ISO\n\u00b6\n\n\nThe latest Flatcar Container Linux ISOs can be downloaded from the image storage site:\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \n\n      \nDownload Alpha ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \n\n      \nDownload Beta ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \n\n      \nDownload Edge ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \n\n      \nDownload Stable ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n  \n\n\n\n\n\nKnown limitations\n\u00b6\n\n\n\n\nUEFI boot is not currently supported. Boot the system in BIOS compatibility mode.\n\n\nThere is no straightforward way to provide an \nIgnition config\n.\n\n\nA minimum of 2 GB of RAM is required to boot Flatcar Container Linux via ISO.\n\n\n\n\nInstall to disk\n\u00b6\n\n\nThe most common use-case for this ISO is to install Flatcar Container Linux to disk. You can \nfind those instructions here\n.\n\n\nNo authentication on console\n\u00b6\n\n\nThe ISO is configured to start a shell on the console without prompting for a password. This is convenient for installation and troubleshooting, but use caution.",
            "title": "Booting from ISO"
        },
        {
            "location": "/os/booting-with-iso/#booting-flatcar-container-linux-from-an-iso",
            "text": "The latest Flatcar Container Linux ISOs can be downloaded from the image storage site:  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       \n       Download Alpha ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site. \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       \n       Download Beta ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site. \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       \n       Download Edge ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site. \n     \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       \n       Download Stable ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site.",
            "title": "Booting Flatcar Container Linux from an ISO"
        },
        {
            "location": "/os/booting-with-iso/#known-limitations",
            "text": "UEFI boot is not currently supported. Boot the system in BIOS compatibility mode.  There is no straightforward way to provide an  Ignition config .  A minimum of 2 GB of RAM is required to boot Flatcar Container Linux via ISO.",
            "title": "Known limitations"
        },
        {
            "location": "/os/booting-with-iso/#install-to-disk",
            "text": "The most common use-case for this ISO is to install Flatcar Container Linux to disk. You can  find those instructions here .",
            "title": "Install to disk"
        },
        {
            "location": "/os/booting-with-iso/#no-authentication-on-console",
            "text": "The ISO is configured to start a shell on the console without prompting for a password. This is convenient for installation and troubleshooting, but use caution.",
            "title": "No authentication on console"
        },
        {
            "location": "/os/root-filesystem-placement/",
            "text": "Configuring Root Filesystem Placement\n\u00b6\n\n\nFlatcar Container Linux supports composite disk devices such as RAID arrays. If the root filesystem is placed on a composite device, special care must be taken to ensure Flatcar Container Linux can find and mount the filesystem early in the boot process. GPT partition entries have a \npartition type GUID\n that specifies what type of partition it is (e.g. Linux filesystem); Flatcar Container Linux uses special type GUIDs to indicate that a partition is a component of a composite device containing the root filesystem.\n\n\nRoot on RAID\n\u00b6\n\n\nRAID enables multiple disks to be combined into a single logical disk to increase reliability and performance. To create a software RAID array when provisioning a Flatcar Container Linux system, use the \nstorage.raid\n section of a \nContainer Linux Config\n. RAID components containing the root filesystem must have the type GUID \nbe9067b9-ea49-4f15-b4f6-f36f8c9e1818\n. All other RAID arrays must not have that GUID; the Linux RAID partition GUID \na19d880f-05fc-4d3b-a006-743f0f84911e\n is recommended instead. See the \nIgnition documentation\n for more information on setting up RAID for data volumes.\n\n\nOverview\n\u00b6\n\n\nTo place the root filesystem on a RAID array:\n\n\n\n\nCreate the component partitions used in the RAID array with the type GUID \nbe9067b9-ea49-4f15-b4f6-f36f8c9e1818\n.\n\n\nCreate a RAID array from the component partitions.\n\n\nCreate a filesystem labeled \nROOT\n on the RAID array.\n\n\nRemove the \nROOT\n label from the original root filesystem.\n\n\n\n\nExample Container Linux Config\n\u00b6\n\n\nThis Container Linux Config creates partitions on \n/dev/vdb\n and \n/dev/vdc\n that fill each disk, creates a RAID array named \nroot_array\n from those partitions, and finally creates the root filesystem on the array. To prevent inadvertent booting from the \noriginal root filesystem\n, \n/dev/vda9\n is reformatted with a blank ext4 filesystem labeled \nunused\n.\n\n\nWarning: This will erase both \n/dev/vdb\n and \n/dev/vdc\n.\n\n\nstorage:\n  disks:\n    - device: /dev/vdb\n      wipe_table: true\n      partitions:\n       - label: root1\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n    - device: /dev/vdc\n      wipe_table: true\n      partitions:\n       - label: root2\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n  raid:\n    - name: \"root_array\"\n      level: \"raid1\"\n      devices:\n        - \"/dev/vdb1\"\n        - \"/dev/vdc1\"\n  filesystems:\n    - name: \"ROOT\"\n      mount:\n        device: \"/dev/md/root_array\"\n        format: \"ext4\"\n        label: \"ROOT\"\n    - name: \"unused\"\n      mount:\n        device: \"/dev/vda9\"\n        format: \"ext4\"\n        wipe_filesystem: true\n        label: \"unused\"\n\n\nLimitations\n\u00b6\n\n\n\n\nOther system partitions, such as \nUSR-A\n, \nUSR-B\n, \nOEM\n, and \nEFI-SYSTEM\n, cannot be placed on a software RAID array.\n\n\nRAID components containing the root filesystem must be partitions on a GPT-partitioned device, not whole-disk devices or partitions on an MBR-partitioned disk.\n\n\n/etc/mdadm.conf\n cannot be used to configure a RAID array containing the root filesystem.\n\n\nSince Ignition cannot modify the type GUID of existing partitions, the default \nROOT\n partition cannot be reused as a component of a RAID array. A future version of Ignition will support resizing the \nROOT\n partition and changing its type GUID, allowing it to be used as part of a RAID array.",
            "title": "Root filesystem placement"
        },
        {
            "location": "/os/root-filesystem-placement/#configuring-root-filesystem-placement",
            "text": "Flatcar Container Linux supports composite disk devices such as RAID arrays. If the root filesystem is placed on a composite device, special care must be taken to ensure Flatcar Container Linux can find and mount the filesystem early in the boot process. GPT partition entries have a  partition type GUID  that specifies what type of partition it is (e.g. Linux filesystem); Flatcar Container Linux uses special type GUIDs to indicate that a partition is a component of a composite device containing the root filesystem.",
            "title": "Configuring Root Filesystem Placement"
        },
        {
            "location": "/os/root-filesystem-placement/#root-on-raid",
            "text": "RAID enables multiple disks to be combined into a single logical disk to increase reliability and performance. To create a software RAID array when provisioning a Flatcar Container Linux system, use the  storage.raid  section of a  Container Linux Config . RAID components containing the root filesystem must have the type GUID  be9067b9-ea49-4f15-b4f6-f36f8c9e1818 . All other RAID arrays must not have that GUID; the Linux RAID partition GUID  a19d880f-05fc-4d3b-a006-743f0f84911e  is recommended instead. See the  Ignition documentation  for more information on setting up RAID for data volumes.",
            "title": "Root on RAID"
        },
        {
            "location": "/os/root-filesystem-placement/#overview",
            "text": "To place the root filesystem on a RAID array:   Create the component partitions used in the RAID array with the type GUID  be9067b9-ea49-4f15-b4f6-f36f8c9e1818 .  Create a RAID array from the component partitions.  Create a filesystem labeled  ROOT  on the RAID array.  Remove the  ROOT  label from the original root filesystem.",
            "title": "Overview"
        },
        {
            "location": "/os/root-filesystem-placement/#example-container-linux-config",
            "text": "This Container Linux Config creates partitions on  /dev/vdb  and  /dev/vdc  that fill each disk, creates a RAID array named  root_array  from those partitions, and finally creates the root filesystem on the array. To prevent inadvertent booting from the  original root filesystem ,  /dev/vda9  is reformatted with a blank ext4 filesystem labeled  unused .  Warning: This will erase both  /dev/vdb  and  /dev/vdc .  storage:\n  disks:\n    - device: /dev/vdb\n      wipe_table: true\n      partitions:\n       - label: root1\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n    - device: /dev/vdc\n      wipe_table: true\n      partitions:\n       - label: root2\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n  raid:\n    - name: \"root_array\"\n      level: \"raid1\"\n      devices:\n        - \"/dev/vdb1\"\n        - \"/dev/vdc1\"\n  filesystems:\n    - name: \"ROOT\"\n      mount:\n        device: \"/dev/md/root_array\"\n        format: \"ext4\"\n        label: \"ROOT\"\n    - name: \"unused\"\n      mount:\n        device: \"/dev/vda9\"\n        format: \"ext4\"\n        wipe_filesystem: true\n        label: \"unused\"",
            "title": "Example Container Linux Config"
        },
        {
            "location": "/os/root-filesystem-placement/#limitations",
            "text": "Other system partitions, such as  USR-A ,  USR-B ,  OEM , and  EFI-SYSTEM , cannot be placed on a software RAID array.  RAID components containing the root filesystem must be partitions on a GPT-partitioned device, not whole-disk devices or partitions on an MBR-partitioned disk.  /etc/mdadm.conf  cannot be used to configure a RAID array containing the root filesystem.  Since Ignition cannot modify the type GUID of existing partitions, the default  ROOT  partition cannot be reused as a component of a RAID array. A future version of Ignition will support resizing the  ROOT  partition and changing its type GUID, allowing it to be used as part of a RAID array.",
            "title": "Limitations"
        },
        {
            "location": "/os/migrate-from-container-linux/",
            "text": "Migrating from CoreOS Container Linux\n\u00b6\n\n\nWhile Flatcar is compatible with CoreOS Container Linux there are some naming differences you need to be aware of.\n\n\nNOTE:\n See \nUpdating from CoreOS Container Linux\n\nfor additional information on updating an existing cluster.\n\n\nInstallation\n\u00b6\n\n\nInstead of \ncoreos-installer\n you need to use \nflatcar-installer\n.\n\n\nKernel command line parameters\n\u00b6\n\n\nInstead of providing the \ncoreos.first_boot=1\n argument via the boot loader you need to provide \nflatcar.first_boot=1\n.\nThis forces provisioning via Ignition even if the machine (image) was booted already before.\n\n\nInstead of providing the \ncoreos.config.url=SOMEURL\n argument via the boot loader you need to provide \nignition.config.url=SOMEURL\n\nto tell Ignition to download the configuration.\nThe change to a more generic name was done upstream by the Ignition project. Version 0.33 still supports both names and we\nalso do this via the analogous \nflatcar.config.url\n option but we encourage the generic name because future versions of Ignition\nwill only support \nignition.config.url\n.\n\n\nInstead of providing the \ncoreos.oem.id=NAME\n argument via the boot loader you need to provide \nflatcar.oem.id=NAME\n.\n(A change to the more generic name \nignition.platform.id\n was done upstream by the Afterburn project but is not part of Container Linux yet.)\n\n\nRecover from or prevent errors with missing OEM settings (e.g., \ncoreos-metadata-sshkeys@core.service\n):\n While future releases will handle both \ncoreos.oem\n and \nflatcar.oem\n names, all current releases still require \nflatcar.oem.\u2026\n.\nChange the variables in the file \n/usr/share/oem/grub.cfg\n when you update from CoreOS Container Linux:\n\n\n# GRUB settings\nset oem_id=\"myoemvalue\"\nset linux_append=\"$linux flatcar.oem.id=myoemvalue\"\n\n\n\nIgnition configuration with QEMU\n\u00b6\n\n\nInstead of using \nopt/com.coreos/config\n in the \n-fw_cfg\n name-value argument pair for QEMU/KVM or libvirt you need to use \nopt/org.flatcar-linux/config\n.\nThe value in the argument pair specifies the Ignition file to use.\n\n\nIgnition configuration with VMware\n\u00b6\n\n\nInstead of \ncoreos.config.data\n and \ncoreos.config.data.encoding\n for the VMware \nguestinfo.VARIABLE\n command line options you need\nto use \nignition.config.data\n and \nignition.config.data.encoding\n.\nAs for the kernel parameter this change was done upstream by the Ignition project.",
            "title": "Migrating from CoreOS Container Linux"
        },
        {
            "location": "/os/migrate-from-container-linux/#migrating-from-coreos-container-linux",
            "text": "While Flatcar is compatible with CoreOS Container Linux there are some naming differences you need to be aware of.  NOTE:  See  Updating from CoreOS Container Linux \nfor additional information on updating an existing cluster.",
            "title": "Migrating from CoreOS Container Linux"
        },
        {
            "location": "/os/migrate-from-container-linux/#installation",
            "text": "Instead of  coreos-installer  you need to use  flatcar-installer .",
            "title": "Installation"
        },
        {
            "location": "/os/migrate-from-container-linux/#kernel-command-line-parameters",
            "text": "Instead of providing the  coreos.first_boot=1  argument via the boot loader you need to provide  flatcar.first_boot=1 .\nThis forces provisioning via Ignition even if the machine (image) was booted already before.  Instead of providing the  coreos.config.url=SOMEURL  argument via the boot loader you need to provide  ignition.config.url=SOMEURL \nto tell Ignition to download the configuration.\nThe change to a more generic name was done upstream by the Ignition project. Version 0.33 still supports both names and we\nalso do this via the analogous  flatcar.config.url  option but we encourage the generic name because future versions of Ignition\nwill only support  ignition.config.url .  Instead of providing the  coreos.oem.id=NAME  argument via the boot loader you need to provide  flatcar.oem.id=NAME .\n(A change to the more generic name  ignition.platform.id  was done upstream by the Afterburn project but is not part of Container Linux yet.)  Recover from or prevent errors with missing OEM settings (e.g.,  coreos-metadata-sshkeys@core.service ):  While future releases will handle both  coreos.oem  and  flatcar.oem  names, all current releases still require  flatcar.oem.\u2026 .\nChange the variables in the file  /usr/share/oem/grub.cfg  when you update from CoreOS Container Linux:  # GRUB settings\nset oem_id=\"myoemvalue\"\nset linux_append=\"$linux flatcar.oem.id=myoemvalue\"",
            "title": "Kernel command line parameters"
        },
        {
            "location": "/os/migrate-from-container-linux/#ignition-configuration-with-qemu",
            "text": "Instead of using  opt/com.coreos/config  in the  -fw_cfg  name-value argument pair for QEMU/KVM or libvirt you need to use  opt/org.flatcar-linux/config .\nThe value in the argument pair specifies the Ignition file to use.",
            "title": "Ignition configuration with QEMU"
        },
        {
            "location": "/os/migrate-from-container-linux/#ignition-configuration-with-vmware",
            "text": "Instead of  coreos.config.data  and  coreos.config.data.encoding  for the VMware  guestinfo.VARIABLE  command line options you need\nto use  ignition.config.data  and  ignition.config.data.encoding .\nAs for the kernel parameter this change was done upstream by the Ignition project.",
            "title": "Ignition configuration with VMware"
        },
        {
            "location": "/os/update-from-container-linux/",
            "text": "Updating from CoreOS Container Linux\n\u00b6\n\n\nIf you already have CoreOS Container Linux clusters and can't or don't want to freshly install Flatcar Container Linux, you can update to Flatcar Container Linux directly from CoreOS Container Linux by performing the following steps.\n\n\nNOTE:\n General differences when \nmigrating from CoreOS Container Linux\n also apply.\n\n\nAt \nthe end of the section\n you can find the \nupdate-to-flatcar.sh\n script that does all steps for you.\n\n\nGetting the public update key\n\u00b6\n\n\nFirst, you need to get Flatcar's public update key:\n\n\n$ curl -L -o /tmp/key https://raw.githubusercontent.com/flatcar-linux/coreos-overlay/flatcar-master/coreos-base/coreos-au-key/files/official-v2.pub.pem\n\n\n\nSince the \n/usr\n partition is read-only, to allow the updater to use the new key, you need to bind-mount it:\n\n\n$ sudo mount --bind /tmp/key /usr/share/update_engine/update-payload-key.pub.pem\n\n\n\nModifying the configuration files\n\u00b6\n\n\nNow, you need to point update_engine to Flatcar's update server by setting the \nSERVER\n configuration option in \n/etc/coreos/update.conf\n:\n\n\nSERVER=https://public.update.flatcar-linux.net/v1/update/\n\n\n\nTo make sure you get an update even if you're running the same CoreOS Container Linux version as the latest Flatcar Container Linux, you need to force an update by clearing the current version number from the \nrelease\n file.\nThis file also lives in the \n/usr\n partition so you need to do a bind-mount again:\n\n\n$ cp /usr/share/coreos/release /tmp\n$ sudo mount --bind /tmp/release /usr/share/coreos/release\n\n\n\nThen, you need to edit \n/usr/share/coreos/release\n and replace the value of \nCOREOS_RELEASE_VERSION\n with \n0.0.0\n:\n\n\nCOREOS_RELEASE_VERSION=0.0.0\n\n\n\nNOTE:\n In bare metal installations, the path where \nuser_data\n is expected changes from \n/var/lib/coreos-install/user_data\n to \n/var/lib/flatcar-install/user_data\n. Make sure you place your \nuser_data\n in the new path.\n\n\nRestart service and reboot\n\u00b6\n\n\nAfter that, restart the update service so it rescans the edited configuration and initiates an update.\nThe system will reboot into Flatcar Container Linux:\n\n\n$ sudo systemctl restart update-engine\n$ update_engine_client -update\n\n\n\nAll steps in one script\n\u00b6\n\n\nThe \nupdate-to-flatcar.sh\n script does all required steps mentioned above for you:\n\n\n# To be run on the node via SSH\ncore@host ~ $ wget https://docs.flatcar-linux.org/update-to-flatcar.sh\ncore@host ~ $ less update-to-flatcar.sh # Double check the content of the script\ncore@host ~ $ chmod +x update-to-flatcar.sh\ncore@host ~ $ ./update-to-flatcar.sh\n[\u2026]\nDone, please reboot now\ncore@host ~ $ sudo systemctl reboot\n\n\n\nBefore you reboot, check that you migrated the variable names as written in \nMigrating from CoreOS Container Linux\n.\n\n\nGoing back to CoreOS Container Linux\n\u00b6\n\n\nYou can also go the other way.\n\n\nManual rollback\n\u00b6\n\n\nIf you just updated to Flatcar (and haven't done any additional updates), CoreOS Container Linux will still be on your disk, you just need to roll back to the other partition.\n\n\nTo do that, just use this command composition:\n\n\n$ sudo cgpt prioritize \"$(sudo cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\")\"\n\n\n\nNow you can reboot and you'll be back to CoreOS Container Linux.\nRemember to undo your changes in your \n/etc/coreos/update.conf\n after rolling back if you want to keep getting CoreOS Container Linux updates.\n\n\nFor more information about manual rollbacks, check \nPerforming a manual rollback\n.\n\n\nForce an update to CoreOS Container Linux\n\u00b6\n\n\nThis procedure is similar to updating from CoreOS Container Linux to Flatcar Container Linux.\nYou need to get CoreOS Container Linux's public key, point update_engine to CoreOS Container Linux's update server, and force an update.\n\n\nGet CoreOS Container Linux's public key:\n\n\n$ curl -L -o /tmp/key https://raw.githubusercontent.com/coreos/coreos-overlay/master/coreos-base/coreos-au-key/files/official-v2.pub.pem\n\n\n\nBind-mount it:\n\n\n$ sudo mount --bind /tmp/key /usr/share/update_engine/update-payload-key.pub.pem\n\n\n\nCreate an \n/etc/flatcar\n directory and copy the current update configuration:\n\n\n$ sudo mkdir -p /etc/flatcar\n$ sudo cp /etc/coreos/update.conf /etc/flatcar/\n\n\n\nChange the \nSERVER\n field in \n/etc/flatcar/update.conf\n:\n\n\nSERVER=https://public.update.core-os.net/v1/update/\n\n\n\nBind-mount the release file:\n\n\n$ cp /usr/share/flatcar/release /tmp\n$ sudo mount --bind /tmp/release /usr/share/flatcar/release\n\n\n\nEdit \nFLATCAR_RELEASE_VERSION\n to force an update:\n\n\nFLATCAR_RELEASE_VERSION=0.0.0\n\n\n\nAfter that, restart the update service so it rescans the edited configuration and initiates an update.\nThe system will reboot into CoreOS Container Linux:\n\n\n$ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Updating from CoreOS Container Linux"
        },
        {
            "location": "/os/update-from-container-linux/#updating-from-coreos-container-linux",
            "text": "If you already have CoreOS Container Linux clusters and can't or don't want to freshly install Flatcar Container Linux, you can update to Flatcar Container Linux directly from CoreOS Container Linux by performing the following steps.  NOTE:  General differences when  migrating from CoreOS Container Linux  also apply.  At  the end of the section  you can find the  update-to-flatcar.sh  script that does all steps for you.",
            "title": "Updating from CoreOS Container Linux"
        },
        {
            "location": "/os/update-from-container-linux/#getting-the-public-update-key",
            "text": "First, you need to get Flatcar's public update key:  $ curl -L -o /tmp/key https://raw.githubusercontent.com/flatcar-linux/coreos-overlay/flatcar-master/coreos-base/coreos-au-key/files/official-v2.pub.pem  Since the  /usr  partition is read-only, to allow the updater to use the new key, you need to bind-mount it:  $ sudo mount --bind /tmp/key /usr/share/update_engine/update-payload-key.pub.pem",
            "title": "Getting the public update key"
        },
        {
            "location": "/os/update-from-container-linux/#modifying-the-configuration-files",
            "text": "Now, you need to point update_engine to Flatcar's update server by setting the  SERVER  configuration option in  /etc/coreos/update.conf :  SERVER=https://public.update.flatcar-linux.net/v1/update/  To make sure you get an update even if you're running the same CoreOS Container Linux version as the latest Flatcar Container Linux, you need to force an update by clearing the current version number from the  release  file.\nThis file also lives in the  /usr  partition so you need to do a bind-mount again:  $ cp /usr/share/coreos/release /tmp\n$ sudo mount --bind /tmp/release /usr/share/coreos/release  Then, you need to edit  /usr/share/coreos/release  and replace the value of  COREOS_RELEASE_VERSION  with  0.0.0 :  COREOS_RELEASE_VERSION=0.0.0  NOTE:  In bare metal installations, the path where  user_data  is expected changes from  /var/lib/coreos-install/user_data  to  /var/lib/flatcar-install/user_data . Make sure you place your  user_data  in the new path.",
            "title": "Modifying the configuration files"
        },
        {
            "location": "/os/update-from-container-linux/#restart-service-and-reboot",
            "text": "After that, restart the update service so it rescans the edited configuration and initiates an update.\nThe system will reboot into Flatcar Container Linux:  $ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Restart service and reboot"
        },
        {
            "location": "/os/update-from-container-linux/#all-steps-in-one-script",
            "text": "The  update-to-flatcar.sh  script does all required steps mentioned above for you:  # To be run on the node via SSH\ncore@host ~ $ wget https://docs.flatcar-linux.org/update-to-flatcar.sh\ncore@host ~ $ less update-to-flatcar.sh # Double check the content of the script\ncore@host ~ $ chmod +x update-to-flatcar.sh\ncore@host ~ $ ./update-to-flatcar.sh\n[\u2026]\nDone, please reboot now\ncore@host ~ $ sudo systemctl reboot  Before you reboot, check that you migrated the variable names as written in  Migrating from CoreOS Container Linux .",
            "title": "All steps in one script"
        },
        {
            "location": "/os/update-from-container-linux/#going-back-to-coreos-container-linux",
            "text": "You can also go the other way.",
            "title": "Going back to CoreOS Container Linux"
        },
        {
            "location": "/os/update-from-container-linux/#manual-rollback",
            "text": "If you just updated to Flatcar (and haven't done any additional updates), CoreOS Container Linux will still be on your disk, you just need to roll back to the other partition.  To do that, just use this command composition:  $ sudo cgpt prioritize \"$(sudo cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\")\"  Now you can reboot and you'll be back to CoreOS Container Linux.\nRemember to undo your changes in your  /etc/coreos/update.conf  after rolling back if you want to keep getting CoreOS Container Linux updates.  For more information about manual rollbacks, check  Performing a manual rollback .",
            "title": "Manual rollback"
        },
        {
            "location": "/os/update-from-container-linux/#force-an-update-to-coreos-container-linux",
            "text": "This procedure is similar to updating from CoreOS Container Linux to Flatcar Container Linux.\nYou need to get CoreOS Container Linux's public key, point update_engine to CoreOS Container Linux's update server, and force an update.  Get CoreOS Container Linux's public key:  $ curl -L -o /tmp/key https://raw.githubusercontent.com/coreos/coreos-overlay/master/coreos-base/coreos-au-key/files/official-v2.pub.pem  Bind-mount it:  $ sudo mount --bind /tmp/key /usr/share/update_engine/update-payload-key.pub.pem  Create an  /etc/flatcar  directory and copy the current update configuration:  $ sudo mkdir -p /etc/flatcar\n$ sudo cp /etc/coreos/update.conf /etc/flatcar/  Change the  SERVER  field in  /etc/flatcar/update.conf :  SERVER=https://public.update.core-os.net/v1/update/  Bind-mount the release file:  $ cp /usr/share/flatcar/release /tmp\n$ sudo mount --bind /tmp/release /usr/share/flatcar/release  Edit  FLATCAR_RELEASE_VERSION  to force an update:  FLATCAR_RELEASE_VERSION=0.0.0  After that, restart the update service so it rescans the edited configuration and initiates an update.\nThe system will reboot into CoreOS Container Linux:  $ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Force an update to CoreOS Container Linux"
        },
        {
            "location": "/os/cluster-architectures/",
            "text": "Flatcar Container Linux cluster architectures\n\u00b6\n\n\nOverview\n\u00b6\n\n\nDepending on the size and expected use of your Flatcar Container Linux cluster, you will have different architectural requirements. A few of the common cluster architectures, as well as their strengths and weaknesses, are described below.\n\n\nMost of these scenarios dedicate a few machines, bare metal or virtual, to running central cluster services. These may include etcd and the distributed controllers for applications like Kubernetes, Mesos, and OpenStack. Isolating these services onto a few known machines helps to ensure they are distributed across cabinets or availability zones. It also helps in setting up static networking to allow for easy bootstrapping. This architecture helps to resolve concerns about relying on a discovery service.\n\n\nDocker dev environment on laptop\n\u00b6\n\n\n\n\nLaptop development environment with Flatcar Container Linux VM\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nLow\n\n\nLaptop development\n\n\nMinutes\n\n\nNo\n\n\n\n\n\n\n\n\nIf you're developing locally but plan to run containers in production, it's best practice to mirror that environment locally. Run Docker commands on your laptop that control a Flatcar Container Linux VM in VMware Fusion or Virtual box to mirror your container production environment locally.\n\n\nConfiguring your laptop\n\u00b6\n\n\nStart a single Flatcar Container Linux VM with the Docker remote socket enabled in the Container Linux Config (CL Config). Here's what the CL Config looks like:\n\n\nsystemd:\n  units:\n    - name: docker-tcp.socket\n      enable: yes\n      mask: false\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\n    - name: enable-docker-tcp.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable the Docker Socket for the API\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/systemctl enable docker-tcp.socket\n\n\n\nThis file is used to provision your local Flatcar Container Linux machine on its first boot. This sets up and enables the Docker API, which is how you can use Docker on your laptop. The Docker CLI manages containers running within the VM, \nnot\n on your personal operating system.\n\n\nUsing the CL Config Transpiler, or \nct\n, (\ndownload\n) convert the above yaml into an \nIgnition\n. Alternatively, copy the contents of the Igntion tab in the above example. Once you have the Ignition configuration file, pass it to your provider (\ncomplete list of supported Ignition platforms\n).\n\n\nOnce the local VM is running, tell your Docker binary on your personal operating system to use the remote port by exporting an environment variable and start running Docker commands. Run these commands in a terminal \non your local operating system (MacOS or Linux), not in the Flatcar Container Linux virtual machine\n:\n\n\n$ export DOCKER_HOST=tcp://localhost:2375\n$ docker ps\n\n\n\nThis avoids discrepancies between your development and production environments.\n\n\nRelated local installation tools\n\u00b6\n\n\nThere are several different options for testing Flatcar Container Linux locally:\n\n\n\n\nFlatcar Container Linux on QEMU\n is a feature rich way of running Flatcar Container Linux locally, provisioned by Ignition configs like the one shown above.\n\n\nMinikube\n is used for local Kubernetes development. This does not use Flatcar Container Linux but is very fast to setup and is the easiest way to test-drive use Kubernetes.\n\n\n\n\nSmall cluster\n\u00b6\n\n\n\n\nSmall Flatcar Container Linux cluster running etcd on all machines\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nLow\n\n\nSmall clusters, trying out Flatcar Container Linux\n\n\nMinutes\n\n\nYes\n\n\n\n\n\n\n\n\nFor small clusters, between 3-9 machines, running etcd on all of the machines allows for high availability without paying for extra machines that just run etcd.\n\n\nGetting started is easy \u2014 a single CL Config can be used to provision all machines in your environment.\n\n\nOnce you have a small cluster up and running, you can install a Kubernetes on the cluster. You can do this easily using \nTyphoon\n.\n\n\nConfiguring the machines\n\u00b6\n\n\nFor more information on getting started with this architecture, see the Flatcar Container Linux documentation on \nsupported platforms\n. These include \nAmazon EC2\n, \nOpenstack\n, \nAzure\n, \nGoogle Compute Platform\n, \nbare metal iPXE\n, \nDigital Ocean\n, and many more community supported platforms.\n\n\nBoot the desired number of machines with the same CL Config and discovery token. The CL Config specifies which services will be started on each machine.\n\n\nEasy development/testing cluster\n\u00b6\n\n\n\n\nFlatcar Container Linux cluster optimized for development and testing\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nLow\n\n\nDevelopment/Testing\n\n\nMinutes\n\n\nNo\n\n\n\n\n\n\n\n\nWhen getting started with Flatcar Container Linux, it's common to frequently boot, reboot, and destroy machines while tweaking your configuration. To avoid the need to generate new discovery URLs and bootstrap etcd, start a single etcd node, and build your cluster around it.\n\n\nYou can now boot as many machines as you'd like as test workers that read from the etcd node. All the features of Locksmith and etcdctl will continue to work properly but will connect to the etcd node instead of using a local etcd instance. Since etcd isn't running on all of the machines you'll gain a little bit of extra CPU and RAM to play with.\n\n\nYou can easily provision the remaining (non-etcd) nodes with Kubernetes using \nTyphoon\n to start running containerized app with your cluster.\n\n\nOnce this environment is set up, it's ready to be tested. Destroy a machine, and watch Kubernetes reschedule the units, max out the CPU, and rebuild your setup automatically.\n\n\nConfiguration for etcd role\n\u00b6\n\n\nSince we're only using a single etcd node, there is no need to include a discovery token. There isn't any high availability for etcd in this configuration, but that's assumed to be OK for development and testing. Boot this machine first so you can configure the rest with its IP address, which is specified with the networkd unit.\n\n\nThe networkd unit is typically used for bare metal installations that require static networking. See your provider's documentation for specific examples.\n\n\nHere's the CL Config for the etcd machine:\n\n\netcd:\n  version: 3.1.5\n  name: \"etcdserver\"\n  initial_cluster: \"etcdserver=http://10.0.0.101:2380\"\n  initial_advertise_peer_urls: \"http://10.0.0.101:2380\"\n  advertise_client_urls: \"http://10.0.0.101:2379\"\n  listen_client_urls: \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n  listen_peer_urls: \"http://0.0.0.0:2380\"\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=etcdserver\"\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n\n\n\nConfiguration for worker role\n\u00b6\n\n\nThis architecture allows you to boot any number of workers, from a single unit to a large cluster designed for load testing. The notable configuration difference for this role is specifying that applications like Kubernetes should use our etcd proxy instead of starting etcd server locally.\n\n\nProduction cluster with central services\n\u00b6\n\n\n\n\nFlatcar Container Linux cluster separated into central services and workers.\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nHigh\n\n\nLarge bare-metal installations\n\n\nHours\n\n\nYes\n\n\n\n\n\n\n\n\nFor large clusters, it's recommended to set aside 3-5 machines to run central services. Once those are set up, you can boot as many workers as you wish. Each of the workers will use your distributed etcd cluster on the central machines via local etcd proxies. This is explained in greater depth below.\n\n\nConfiguration for central services role\n\u00b6\n\n\nOur central services machines will run services like etcd and Kubernetes controllers that support the rest of the cluster. etcd is configured with static networking and a peers list.\n\n\nFlatcar Container Linux Support\n customers can also specify a \nCoreUpdate\n group ID which allows you to subscribe these machines to a different update channel, controlling updates separately from the worker machines.\n\n\nHere's an example CL Config for one of the central service machines. Be sure to generate a new discovery token with the initial size of your cluster:\n\n\netcd:\n  version: 3.0.15\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi-region and multi-cloud deployments must use $public_ipv4\n  advertise_client_urls: http://10.0.0.101:2379\n  initial_advertise_peer_urls: http://10.0.0.101:2380\n  listen_client_urls: http://0.0.0.0:2379\n  listen_peer_urls: http://10.0.0.101:2380\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Cluster architectures"
        },
        {
            "location": "/os/cluster-architectures/#flatcar-container-linux-cluster-architectures",
            "text": "",
            "title": "Flatcar Container Linux cluster architectures"
        },
        {
            "location": "/os/cluster-architectures/#overview",
            "text": "Depending on the size and expected use of your Flatcar Container Linux cluster, you will have different architectural requirements. A few of the common cluster architectures, as well as their strengths and weaknesses, are described below.  Most of these scenarios dedicate a few machines, bare metal or virtual, to running central cluster services. These may include etcd and the distributed controllers for applications like Kubernetes, Mesos, and OpenStack. Isolating these services onto a few known machines helps to ensure they are distributed across cabinets or availability zones. It also helps in setting up static networking to allow for easy bootstrapping. This architecture helps to resolve concerns about relying on a discovery service.",
            "title": "Overview"
        },
        {
            "location": "/os/cluster-architectures/#docker-dev-environment-on-laptop",
            "text": "Laptop development environment with Flatcar Container Linux VM     Cost  Great For  Set Up Time  Production      Low  Laptop development  Minutes  No     If you're developing locally but plan to run containers in production, it's best practice to mirror that environment locally. Run Docker commands on your laptop that control a Flatcar Container Linux VM in VMware Fusion or Virtual box to mirror your container production environment locally.",
            "title": "Docker dev environment on laptop"
        },
        {
            "location": "/os/cluster-architectures/#configuring-your-laptop",
            "text": "Start a single Flatcar Container Linux VM with the Docker remote socket enabled in the Container Linux Config (CL Config). Here's what the CL Config looks like:  systemd:\n  units:\n    - name: docker-tcp.socket\n      enable: yes\n      mask: false\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\n    - name: enable-docker-tcp.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable the Docker Socket for the API\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/systemctl enable docker-tcp.socket  This file is used to provision your local Flatcar Container Linux machine on its first boot. This sets up and enables the Docker API, which is how you can use Docker on your laptop. The Docker CLI manages containers running within the VM,  not  on your personal operating system.  Using the CL Config Transpiler, or  ct , ( download ) convert the above yaml into an  Ignition . Alternatively, copy the contents of the Igntion tab in the above example. Once you have the Ignition configuration file, pass it to your provider ( complete list of supported Ignition platforms ).  Once the local VM is running, tell your Docker binary on your personal operating system to use the remote port by exporting an environment variable and start running Docker commands. Run these commands in a terminal  on your local operating system (MacOS or Linux), not in the Flatcar Container Linux virtual machine :  $ export DOCKER_HOST=tcp://localhost:2375\n$ docker ps  This avoids discrepancies between your development and production environments.",
            "title": "Configuring your laptop"
        },
        {
            "location": "/os/cluster-architectures/#related-local-installation-tools",
            "text": "There are several different options for testing Flatcar Container Linux locally:   Flatcar Container Linux on QEMU  is a feature rich way of running Flatcar Container Linux locally, provisioned by Ignition configs like the one shown above.  Minikube  is used for local Kubernetes development. This does not use Flatcar Container Linux but is very fast to setup and is the easiest way to test-drive use Kubernetes.",
            "title": "Related local installation tools"
        },
        {
            "location": "/os/cluster-architectures/#small-cluster",
            "text": "Small Flatcar Container Linux cluster running etcd on all machines     Cost  Great For  Set Up Time  Production      Low  Small clusters, trying out Flatcar Container Linux  Minutes  Yes     For small clusters, between 3-9 machines, running etcd on all of the machines allows for high availability without paying for extra machines that just run etcd.  Getting started is easy \u2014 a single CL Config can be used to provision all machines in your environment.  Once you have a small cluster up and running, you can install a Kubernetes on the cluster. You can do this easily using  Typhoon .",
            "title": "Small cluster"
        },
        {
            "location": "/os/cluster-architectures/#configuring-the-machines",
            "text": "For more information on getting started with this architecture, see the Flatcar Container Linux documentation on  supported platforms . These include  Amazon EC2 ,  Openstack ,  Azure ,  Google Compute Platform ,  bare metal iPXE ,  Digital Ocean , and many more community supported platforms.  Boot the desired number of machines with the same CL Config and discovery token. The CL Config specifies which services will be started on each machine.",
            "title": "Configuring the machines"
        },
        {
            "location": "/os/cluster-architectures/#easy-developmenttesting-cluster",
            "text": "Flatcar Container Linux cluster optimized for development and testing     Cost  Great For  Set Up Time  Production      Low  Development/Testing  Minutes  No     When getting started with Flatcar Container Linux, it's common to frequently boot, reboot, and destroy machines while tweaking your configuration. To avoid the need to generate new discovery URLs and bootstrap etcd, start a single etcd node, and build your cluster around it.  You can now boot as many machines as you'd like as test workers that read from the etcd node. All the features of Locksmith and etcdctl will continue to work properly but will connect to the etcd node instead of using a local etcd instance. Since etcd isn't running on all of the machines you'll gain a little bit of extra CPU and RAM to play with.  You can easily provision the remaining (non-etcd) nodes with Kubernetes using  Typhoon  to start running containerized app with your cluster.  Once this environment is set up, it's ready to be tested. Destroy a machine, and watch Kubernetes reschedule the units, max out the CPU, and rebuild your setup automatically.",
            "title": "Easy development/testing cluster"
        },
        {
            "location": "/os/cluster-architectures/#configuration-for-etcd-role",
            "text": "Since we're only using a single etcd node, there is no need to include a discovery token. There isn't any high availability for etcd in this configuration, but that's assumed to be OK for development and testing. Boot this machine first so you can configure the rest with its IP address, which is specified with the networkd unit.  The networkd unit is typically used for bare metal installations that require static networking. See your provider's documentation for specific examples.  Here's the CL Config for the etcd machine:  etcd:\n  version: 3.1.5\n  name: \"etcdserver\"\n  initial_cluster: \"etcdserver=http://10.0.0.101:2380\"\n  initial_advertise_peer_urls: \"http://10.0.0.101:2380\"\n  advertise_client_urls: \"http://10.0.0.101:2379\"\n  listen_client_urls: \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n  listen_peer_urls: \"http://0.0.0.0:2380\"\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=etcdserver\"\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Configuration for etcd role"
        },
        {
            "location": "/os/cluster-architectures/#configuration-for-worker-role",
            "text": "This architecture allows you to boot any number of workers, from a single unit to a large cluster designed for load testing. The notable configuration difference for this role is specifying that applications like Kubernetes should use our etcd proxy instead of starting etcd server locally.",
            "title": "Configuration for worker role"
        },
        {
            "location": "/os/cluster-architectures/#production-cluster-with-central-services",
            "text": "Flatcar Container Linux cluster separated into central services and workers.     Cost  Great For  Set Up Time  Production      High  Large bare-metal installations  Hours  Yes     For large clusters, it's recommended to set aside 3-5 machines to run central services. Once those are set up, you can boot as many workers as you wish. Each of the workers will use your distributed etcd cluster on the central machines via local etcd proxies. This is explained in greater depth below.",
            "title": "Production cluster with central services"
        },
        {
            "location": "/os/cluster-architectures/#configuration-for-central-services-role",
            "text": "Our central services machines will run services like etcd and Kubernetes controllers that support the rest of the cluster. etcd is configured with static networking and a peers list.  Flatcar Container Linux Support  customers can also specify a  CoreUpdate  group ID which allows you to subscribe these machines to a different update channel, controlling updates separately from the worker machines.  Here's an example CL Config for one of the central service machines. Be sure to generate a new discovery token with the initial size of your cluster:  etcd:\n  version: 3.0.15\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi-region and multi-cloud deployments must use $public_ipv4\n  advertise_client_urls: http://10.0.0.101:2379\n  initial_advertise_peer_urls: http://10.0.0.101:2380\n  listen_client_urls: http://0.0.0.0:2379\n  listen_peer_urls: http://10.0.0.101:2380\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Configuration for central services role"
        },
        {
            "location": "/os/update-strategies/",
            "text": "Reboot strategies on updates\n\u00b6\n\n\nThe overarching goal of Flatcar Container Linux is to secure the Internet's backend infrastructure. We believe that automatically updating the operating system is one of the best tools to achieve this goal.\n\n\nWe realize that each Flatcar Container Linux cluster has a unique tolerance for risk and the operational needs of your applications are complex. In order to meet everyone's needs, there are three update strategies that we have developed based on feedback during our alpha period.\n\n\nIt's important to note that updates are always downloaded to the passive partition when they become available. A reboot is the last step of the update, where the active and passive partitions are swapped (\nrollback instructions\n). These strategies control how that reboot occurs:\n\n\n\n\n\n\n\n\nStrategy\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\netcd-lock\n\n\nReboot after first taking a distributed lock in etcd\n\n\n\n\n\n\nreboot\n\n\nReboot immediately after an update is applied\n\n\n\n\n\n\noff\n\n\nDo not reboot after updates are applied\n\n\n\n\n\n\n\n\nReboot strategy options\n\u00b6\n\n\nThe reboot strategy can be set with a Container Linux Config:\n\n\nlocksmith:\n  reboot_strategy: \"etcd-lock\"\n\n\n\netcd-lock\n\u00b6\n\n\nThe \netcd-lock\n strategy mandates that each machine acquire and hold a reboot lock before it is allowed to reboot. The main goal behind this strategy is to allow for an update to be applied to a cluster quickly, without losing the quorum membership in etcd or rapidly reducing capacity for the services running on the cluster. The reboot lock is held until the machine releases it after a successful update.\n\n\nThe number of machines allowed to reboot simultaneously is configurable via a command line utility:\n\n\n$ locksmithctl set-max 4\nOld: 1\nNew: 4\n\n\n\nThis setting is stored in etcd so it won't have to be configured for subsequent machines.\n\n\nTo view the number of available slots and find out which machines in the cluster are holding locks, run:\n\n\n$ locksmithctl status\nAvailable: 0\nMax: 1\n\nMACHINE ID\n69d27b356a94476da859461d3a3bc6fd\n\n\n\nIf needed, you can manually clear a lock by providing the machine ID:\n\n\nlocksmithctl unlock 69d27b356a94476da859461d3a3bc6fd\n\n\n\nReboot immediately\n\u00b6\n\n\nThe \nreboot\n strategy works exactly like it sounds: the machine is rebooted as soon as the update has been installed to the passive partition. If the applications running on your cluster are highly resilient, this strategy was made for you.\n\n\nOff\n\u00b6\n\n\nThe \noff\n strategy is also straightforward. The update will be installed onto the passive partition and await a reboot command to complete the update. We don't recommend this strategy unless you reboot frequently as part of your normal operations workflow.\n\n\nUpdating PXE/iPXE machines\n\u00b6\n\n\nPXE/iPXE machines download a new copy of Flatcar Container Linux every time they are started thus are dependent on the version of Flatcar Container Linux they are served. If you don't automatically load new Flatcar Container Linux images into your PXE/iPXE server, your machines will never have new features or security updates.\n\n\nAn easy solution to this problem is to use iPXE and reference images \ndirectly from the Flatcar Container Linux storage site\n. The \nalpha\n URL is automatically pointed to the new version of Flatcar Container Linux as it is released.\n\n\nDisable Automatic Updates Daemon\n\u00b6\n\n\nIn case when you don't want to install updates onto the passive partition and avoid update process on failure reboot, you can disable \nupdate-engine\n service manually with \nsudo systemctl stop update-engine\n command (it will be enabled automatically next reboot).\n\n\nIf you wish to disable automatic updates permanently, use can configure this with a Container Linux Config. This example will stop \nupdate-engine\n, which executes the updates, and \nlocksmithd\n, which coordinates reboots across the cluster:\n\n\nsystemd:\n  units:\n    - name: update-engine.service\n      mask: true\n    - name: locksmithd.service\n      mask: true\n\n\n\nUpdating behind a proxy\n\u00b6\n\n\nPublic Internet access is required to contact CoreUpdate and download new versions of Flatcar Container Linux. If direct access is not available the \nupdate-engine\n service may be configured to use a HTTP or SOCKS proxy using curl-compatible environment variables, such as \nHTTPS_PROXY\n or \nALL_PROXY\n.\nSee \ncurl's documentation\n for details.\n\n\nsystemd:\n  units:\n    - name: update-engine.service\n      dropins:\n        - name: 50-proxy.conf\n          contents: |\n            [Service]\n            Environment=ALL_PROXY=http://proxy.example.com:3128\n\n\n\nProxy environment variables can also be set \nsystem-wide\n.\n\n\nManually triggering an update\n\u00b6\n\n\nEach machine should check in about 10 minutes after boot and roughly every hour after that. If you'd like to see it sooner, you can force an update check, which will skip any rate-limiting settings that are configured in CoreUpdate.\n\n\n$ update_engine_client -check_for_update\n[0123/220706:INFO:update_engine_client.cc(245)] Initiating update check and install.\n\n\n\nAuto-updates with a maintenance window\n\u00b6\n\n\nLocksmith supports maintenance windows in addition to the reboot strategies mentioned earlier. Maintenance windows define a window of time during which a reboot can occur. These operate in addition to reboot strategies, so if the machine has a maintenance window and requires a reboot lock, the machine will only reboot when it has the lock during that window.\n\n\nWindows are defined by a start time and a length. In this example, the window is defined to be every Thursday between 04:00 and 05:00:\n\n\nlocksmith:\n  reboot_strategy: reboot\n  window_start: Thu 04:00\n  window_length: 1h\n\n\n\nThis will configure a Flatcar Container Linux machine to follow the \nreboot\n strategy, and thus when an update is ready it will simply reboot instead of attempting to grab a lock in etcd. This machine however has also been configured to only reboot between 04:00 and 05:00 on Thursdays, so if an update occurs outside of this window the machine will then wait until it is inside of this window to reboot.\n\n\nFor more information about the supported syntax, refer to the \nLocksmith documentation\n.",
            "title": "Update strategies"
        },
        {
            "location": "/os/update-strategies/#reboot-strategies-on-updates",
            "text": "The overarching goal of Flatcar Container Linux is to secure the Internet's backend infrastructure. We believe that automatically updating the operating system is one of the best tools to achieve this goal.  We realize that each Flatcar Container Linux cluster has a unique tolerance for risk and the operational needs of your applications are complex. In order to meet everyone's needs, there are three update strategies that we have developed based on feedback during our alpha period.  It's important to note that updates are always downloaded to the passive partition when they become available. A reboot is the last step of the update, where the active and passive partitions are swapped ( rollback instructions ). These strategies control how that reboot occurs:     Strategy  Description      etcd-lock  Reboot after first taking a distributed lock in etcd    reboot  Reboot immediately after an update is applied    off  Do not reboot after updates are applied",
            "title": "Reboot strategies on updates"
        },
        {
            "location": "/os/update-strategies/#reboot-strategy-options",
            "text": "The reboot strategy can be set with a Container Linux Config:  locksmith:\n  reboot_strategy: \"etcd-lock\"",
            "title": "Reboot strategy options"
        },
        {
            "location": "/os/update-strategies/#etcd-lock",
            "text": "The  etcd-lock  strategy mandates that each machine acquire and hold a reboot lock before it is allowed to reboot. The main goal behind this strategy is to allow for an update to be applied to a cluster quickly, without losing the quorum membership in etcd or rapidly reducing capacity for the services running on the cluster. The reboot lock is held until the machine releases it after a successful update.  The number of machines allowed to reboot simultaneously is configurable via a command line utility:  $ locksmithctl set-max 4\nOld: 1\nNew: 4  This setting is stored in etcd so it won't have to be configured for subsequent machines.  To view the number of available slots and find out which machines in the cluster are holding locks, run:  $ locksmithctl status\nAvailable: 0\nMax: 1\n\nMACHINE ID\n69d27b356a94476da859461d3a3bc6fd  If needed, you can manually clear a lock by providing the machine ID:  locksmithctl unlock 69d27b356a94476da859461d3a3bc6fd",
            "title": "etcd-lock"
        },
        {
            "location": "/os/update-strategies/#reboot-immediately",
            "text": "The  reboot  strategy works exactly like it sounds: the machine is rebooted as soon as the update has been installed to the passive partition. If the applications running on your cluster are highly resilient, this strategy was made for you.",
            "title": "Reboot immediately"
        },
        {
            "location": "/os/update-strategies/#off",
            "text": "The  off  strategy is also straightforward. The update will be installed onto the passive partition and await a reboot command to complete the update. We don't recommend this strategy unless you reboot frequently as part of your normal operations workflow.",
            "title": "Off"
        },
        {
            "location": "/os/update-strategies/#updating-pxeipxe-machines",
            "text": "PXE/iPXE machines download a new copy of Flatcar Container Linux every time they are started thus are dependent on the version of Flatcar Container Linux they are served. If you don't automatically load new Flatcar Container Linux images into your PXE/iPXE server, your machines will never have new features or security updates.  An easy solution to this problem is to use iPXE and reference images  directly from the Flatcar Container Linux storage site . The  alpha  URL is automatically pointed to the new version of Flatcar Container Linux as it is released.",
            "title": "Updating PXE/iPXE machines"
        },
        {
            "location": "/os/update-strategies/#disable-automatic-updates-daemon",
            "text": "In case when you don't want to install updates onto the passive partition and avoid update process on failure reboot, you can disable  update-engine  service manually with  sudo systemctl stop update-engine  command (it will be enabled automatically next reboot).  If you wish to disable automatic updates permanently, use can configure this with a Container Linux Config. This example will stop  update-engine , which executes the updates, and  locksmithd , which coordinates reboots across the cluster:  systemd:\n  units:\n    - name: update-engine.service\n      mask: true\n    - name: locksmithd.service\n      mask: true",
            "title": "Disable Automatic Updates Daemon"
        },
        {
            "location": "/os/update-strategies/#updating-behind-a-proxy",
            "text": "Public Internet access is required to contact CoreUpdate and download new versions of Flatcar Container Linux. If direct access is not available the  update-engine  service may be configured to use a HTTP or SOCKS proxy using curl-compatible environment variables, such as  HTTPS_PROXY  or  ALL_PROXY .\nSee  curl's documentation  for details.  systemd:\n  units:\n    - name: update-engine.service\n      dropins:\n        - name: 50-proxy.conf\n          contents: |\n            [Service]\n            Environment=ALL_PROXY=http://proxy.example.com:3128  Proxy environment variables can also be set  system-wide .",
            "title": "Updating behind a proxy"
        },
        {
            "location": "/os/update-strategies/#manually-triggering-an-update",
            "text": "Each machine should check in about 10 minutes after boot and roughly every hour after that. If you'd like to see it sooner, you can force an update check, which will skip any rate-limiting settings that are configured in CoreUpdate.  $ update_engine_client -check_for_update\n[0123/220706:INFO:update_engine_client.cc(245)] Initiating update check and install.",
            "title": "Manually triggering an update"
        },
        {
            "location": "/os/update-strategies/#auto-updates-with-a-maintenance-window",
            "text": "Locksmith supports maintenance windows in addition to the reboot strategies mentioned earlier. Maintenance windows define a window of time during which a reboot can occur. These operate in addition to reboot strategies, so if the machine has a maintenance window and requires a reboot lock, the machine will only reboot when it has the lock during that window.  Windows are defined by a start time and a length. In this example, the window is defined to be every Thursday between 04:00 and 05:00:  locksmith:\n  reboot_strategy: reboot\n  window_start: Thu 04:00\n  window_length: 1h  This will configure a Flatcar Container Linux machine to follow the  reboot  strategy, and thus when an update is ready it will simply reboot instead of attempting to grab a lock in etcd. This machine however has also been configured to only reboot between 04:00 and 05:00 on Thursdays, so if an update occurs outside of this window the machine will then wait until it is inside of this window to reboot.  For more information about the supported syntax, refer to the  Locksmith documentation .",
            "title": "Auto-updates with a maintenance window"
        },
        {
            "location": "/os/cluster-discovery/",
            "text": "Flatcar Container Linux cluster discovery\n\u00b6\n\n\nOverview\n\u00b6\n\n\nFlatcar Container Linux uses etcd, a service running on each machine, to handle coordination between software running on the cluster. For a group of Flatcar Container Linux machines to form a cluster, their etcd instances need to be connected.\n\n\nA discovery service, \nhttps://discovery.etcd.io\n, is provided as a free service to help connect etcd instances together by storing a list of peer addresses, metadata and the initial size of the cluster under a unique address, known as the discovery URL. You can generate them very easily:\n\n\n$ curl -w \"\\n\" 'https://discovery.etcd.io/new?size=3'\nhttps://discovery.etcd.io/6a28e078895c5ec737174db2419bb2f3\n\n\n\nThe discovery URL can be provided to each Flatcar Container Linux machine via \nContainer Linux Configs\n. The rest of this guide will explain what's happening behind the scenes, but if you're trying to get clustered as quickly as possible, all you need to do is provide a \nfresh, unique\n discovery token in your config.\n\n\nBoot each one of the machines with identical Container Linux Config and they should be automatically clustered:\n\n\netcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls: http://{PRIVATE_IPV4}:2379,http://{PRIVATE_IPV4}:4001\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://{PRIVATE_IPV4}:2380\n\n\n\nSpecific documentation are provided for each platform's guide. Not all providers support the \n{PRIVATE_IPV4}\n variable substitution.\n\n\nNew clusters\n\u00b6\n\n\nStarting a Flatcar Container Linux cluster requires one of the new machines to become the first leader of the cluster. The initial leader is stored as metadata with the discovery URL in order to inform the other members of the new cluster. Let's walk through a timeline a new three-machine Flatcar Container Linux cluster discovering each other:\n\n\n\n\nAll three machines are booted via a cloud-provider with the same config in the user-data.\n\n\nMachine 1 starts up first. It requests information about the cluster from the discovery token and submits its \n-initial-advertise-peer-urls\n address \n10.10.10.1\n.\n\n\nNo state is recorded into the discovery URL metadata, so machine 1 becomes the leader and records the state as \nstarted\n.\n\n\nMachine 2 boots and submits its \n-initial-advertise-peer-urls\n address \n10.10.10.2\n. It also reads back the list of existing peers (only \n10.10.10.1\n) and attempts to connect to the address listed.\n\n\nMachine 2 connects to Machine 1 and is now part of the cluster as a follower.\n\n\nMachine 3 boots and submits its \n-initial-advertise-peer-urls\n address \n10.10.10.3\n. It reads back the list of peers (\n10.10.10.1\n and \n10.10.10.2\n) and selects one of the addresses to try first. When it connects to a machine in the cluster, the machine is given a full list of the existing other members of the cluster.\n\n\nThe cluster is now bootstrapped with an initial leader and two followers.\n\n\n\n\nThere are a few interesting things happening during this process.\n\n\nFirst, each machine is configured with the same discovery URL and etcd figured out what to do. This allows you to load the same Container Linux Config into an auto-scaling group and it will work whether it is the first or 30\nth\n machine in the group.\n\n\nSecond, machine 3 only needed to use one of the addresses stored in the discovery URL to connect to the cluster. Since etcd uses the Raft consensus algorithm, existing machines in the cluster already maintain a list of healthy members in order for the algorithm to function properly. This list is given to the new machine and it starts normal operations with each of the other cluster members.\n\n\nThird, if you specified \n?size=3\n upon discovery URL creation, any other machines that join the cluster in the future will automatically start as etcd proxies.\n\n\nCommon problems with cluster discovery\n\u00b6\n\n\nExisting clusters\n\u00b6\n\n\nDo not use the public discovery service to reconfigure a running etcd cluster.\n The public discovery service is a convenience for bootstrapping new clusters, especially on cloud providers with dynamic IP assignment, but is not designed for the later case when the cluster is running and member IPs are known.\n\n\nTo promote proxy members or join new members into an existing etcd cluster, configure static discovery and add members. The \netcd cluster reconfiguration guide\n details the steps for performing this reconfiguration on Flatcar Container Linux systems that were originally deployed with public discovery. The more general \netcd cluster reconfiguration document\n explains the operations for removing and adding cluster members in a cluster already configured with static discovery.\n\n\nStale tokens\n\u00b6\n\n\nA common problem with cluster discovery is attempting to boot a new cluster with a stale discovery URL. As explained above, the initial leader election is recorded into the URL, which indicates that the new etcd instance should be joining an existing cluster.\n\n\nIf you provide a stale discovery URL, the new machines will attempt to connect to each of the old peer addresses, which will fail since they don't exist, and the bootstrapping process will fail.\n\n\nIf you're thinking, why can't the new machines just form a new cluster if they're all down. There's a really great reason for this \u2014 if an etcd peer was in a network partition, it would look exactly like the \"full-down\" situation and starting a new cluster would form a split-brain. Since etcd will never be able to determine whether a token has been reused or not, it must assume the worst and abort the cluster discovery.\n\n\nIf you're running into problems with your discovery URL, there are a few sources of information that can help you see what's going on. First, you can open the URL in a browser to see what information etcd is using to bootstrap itself:\n\n\n{\n  action: \"get\",\n  node: {\n    key: \"/_etcd/registry/506f6c1bc729377252232a0121247119\",\n    dir: true,\n    nodes: [\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/0d79b4791be9688332cc05367366551e\",\n        value: \"http://10.183.202.105:7001\",\n        expiration: \"2014-08-17T16:21:37.426001686Z\",\n        ttl: 576008,\n        modifiedIndex: 72783864,\n        createdIndex: 72783864\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/c72c63ffce6680737ea2b670456aaacd\",\n        value: \"http://10.65.177.56:7001\",\n        expiration: \"2014-08-17T12:05:57.717243529Z\",\n        ttl: 560669,\n        modifiedIndex: 72626400,\n        createdIndex: 72626400\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/f7a93d1f0cd4d318c9ad0b624afb9cf9\",\n        value: \"http://10.29.193.50:7001\",\n        expiration: \"2014-08-17T17:18:25.045563473Z\",\n        ttl: 579416,\n        modifiedIndex: 72821950,\n        createdIndex: 72821950\n      }\n    ],\n    modifiedIndex: 69367741,\n    createdIndex: 69367741\n  }\n}\n\n\n\nTo rule out firewall settings as a source of your issue, ensure that you can curl each of the IPs from machines in your cluster.\n\n\nIf all of the IPs can be reached, the etcd log can provide more clues:\n\n\njournalctl -u etcd-member\n\n\n\nCommunicating with discovery.etcd.io\n\u00b6\n\n\nIf your Flatcar Container Linux cluster can't communicate out to the public internet, \nhttps://discovery.etcd.io\n won't work and you'll have to run your own discovery endpoint, which is described below.\n\n\nSetting advertised client addresses correctly\n\u00b6\n\n\nEach etcd instance submits the list of \n-initial-advertise-peer-urls\n of each etcd instance to the configured discovery service. It's important to select an address that \nall\n peers in the cluster can communicate with. If you are configuring a list of addresses, make sure each member can communicate with at least one of the addresses.\n\n\nFor example, if you're located in two regions of a cloud provider, configuring a private \n10.x\n address will not work between the two regions, and communication will not be possible between all peers. The \n-listen-client-urls\n flag allows you to bind to a specific list of interfaces and ports (or all interfaces) to ensure your etcd traffic is routed properly.\n\n\nRunning your own discovery service\n\u00b6\n\n\nThe public discovery service is just an etcd cluster made available to the public internet. Since the discovery service conducts and stores the result of the first leader election, it needs to be consistent. You wouldn't want two machines in the same cluster to think they were both the leader.\n\n\nSince etcd is designed to this type of leader election, it was an obvious choice to use it for everyone's initial leader election. This means that it's easy to run your own etcd cluster for this purpose.\n\n\nIf you're interested in how discovery API works behind the scenes in etcd, read about \netcd clustering\n.",
            "title": "Clustering machines"
        },
        {
            "location": "/os/cluster-discovery/#flatcar-container-linux-cluster-discovery",
            "text": "",
            "title": "Flatcar Container Linux cluster discovery"
        },
        {
            "location": "/os/cluster-discovery/#overview",
            "text": "Flatcar Container Linux uses etcd, a service running on each machine, to handle coordination between software running on the cluster. For a group of Flatcar Container Linux machines to form a cluster, their etcd instances need to be connected.  A discovery service,  https://discovery.etcd.io , is provided as a free service to help connect etcd instances together by storing a list of peer addresses, metadata and the initial size of the cluster under a unique address, known as the discovery URL. You can generate them very easily:  $ curl -w \"\\n\" 'https://discovery.etcd.io/new?size=3'\nhttps://discovery.etcd.io/6a28e078895c5ec737174db2419bb2f3  The discovery URL can be provided to each Flatcar Container Linux machine via  Container Linux Configs . The rest of this guide will explain what's happening behind the scenes, but if you're trying to get clustered as quickly as possible, all you need to do is provide a  fresh, unique  discovery token in your config.  Boot each one of the machines with identical Container Linux Config and they should be automatically clustered:  etcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls: http://{PRIVATE_IPV4}:2379,http://{PRIVATE_IPV4}:4001\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://{PRIVATE_IPV4}:2380  Specific documentation are provided for each platform's guide. Not all providers support the  {PRIVATE_IPV4}  variable substitution.",
            "title": "Overview"
        },
        {
            "location": "/os/cluster-discovery/#new-clusters",
            "text": "Starting a Flatcar Container Linux cluster requires one of the new machines to become the first leader of the cluster. The initial leader is stored as metadata with the discovery URL in order to inform the other members of the new cluster. Let's walk through a timeline a new three-machine Flatcar Container Linux cluster discovering each other:   All three machines are booted via a cloud-provider with the same config in the user-data.  Machine 1 starts up first. It requests information about the cluster from the discovery token and submits its  -initial-advertise-peer-urls  address  10.10.10.1 .  No state is recorded into the discovery URL metadata, so machine 1 becomes the leader and records the state as  started .  Machine 2 boots and submits its  -initial-advertise-peer-urls  address  10.10.10.2 . It also reads back the list of existing peers (only  10.10.10.1 ) and attempts to connect to the address listed.  Machine 2 connects to Machine 1 and is now part of the cluster as a follower.  Machine 3 boots and submits its  -initial-advertise-peer-urls  address  10.10.10.3 . It reads back the list of peers ( 10.10.10.1  and  10.10.10.2 ) and selects one of the addresses to try first. When it connects to a machine in the cluster, the machine is given a full list of the existing other members of the cluster.  The cluster is now bootstrapped with an initial leader and two followers.   There are a few interesting things happening during this process.  First, each machine is configured with the same discovery URL and etcd figured out what to do. This allows you to load the same Container Linux Config into an auto-scaling group and it will work whether it is the first or 30 th  machine in the group.  Second, machine 3 only needed to use one of the addresses stored in the discovery URL to connect to the cluster. Since etcd uses the Raft consensus algorithm, existing machines in the cluster already maintain a list of healthy members in order for the algorithm to function properly. This list is given to the new machine and it starts normal operations with each of the other cluster members.  Third, if you specified  ?size=3  upon discovery URL creation, any other machines that join the cluster in the future will automatically start as etcd proxies.",
            "title": "New clusters"
        },
        {
            "location": "/os/cluster-discovery/#common-problems-with-cluster-discovery",
            "text": "",
            "title": "Common problems with cluster discovery"
        },
        {
            "location": "/os/cluster-discovery/#existing-clusters",
            "text": "Do not use the public discovery service to reconfigure a running etcd cluster.  The public discovery service is a convenience for bootstrapping new clusters, especially on cloud providers with dynamic IP assignment, but is not designed for the later case when the cluster is running and member IPs are known.  To promote proxy members or join new members into an existing etcd cluster, configure static discovery and add members. The  etcd cluster reconfiguration guide  details the steps for performing this reconfiguration on Flatcar Container Linux systems that were originally deployed with public discovery. The more general  etcd cluster reconfiguration document  explains the operations for removing and adding cluster members in a cluster already configured with static discovery.",
            "title": "Existing clusters"
        },
        {
            "location": "/os/cluster-discovery/#stale-tokens",
            "text": "A common problem with cluster discovery is attempting to boot a new cluster with a stale discovery URL. As explained above, the initial leader election is recorded into the URL, which indicates that the new etcd instance should be joining an existing cluster.  If you provide a stale discovery URL, the new machines will attempt to connect to each of the old peer addresses, which will fail since they don't exist, and the bootstrapping process will fail.  If you're thinking, why can't the new machines just form a new cluster if they're all down. There's a really great reason for this \u2014 if an etcd peer was in a network partition, it would look exactly like the \"full-down\" situation and starting a new cluster would form a split-brain. Since etcd will never be able to determine whether a token has been reused or not, it must assume the worst and abort the cluster discovery.  If you're running into problems with your discovery URL, there are a few sources of information that can help you see what's going on. First, you can open the URL in a browser to see what information etcd is using to bootstrap itself:  {\n  action: \"get\",\n  node: {\n    key: \"/_etcd/registry/506f6c1bc729377252232a0121247119\",\n    dir: true,\n    nodes: [\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/0d79b4791be9688332cc05367366551e\",\n        value: \"http://10.183.202.105:7001\",\n        expiration: \"2014-08-17T16:21:37.426001686Z\",\n        ttl: 576008,\n        modifiedIndex: 72783864,\n        createdIndex: 72783864\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/c72c63ffce6680737ea2b670456aaacd\",\n        value: \"http://10.65.177.56:7001\",\n        expiration: \"2014-08-17T12:05:57.717243529Z\",\n        ttl: 560669,\n        modifiedIndex: 72626400,\n        createdIndex: 72626400\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/f7a93d1f0cd4d318c9ad0b624afb9cf9\",\n        value: \"http://10.29.193.50:7001\",\n        expiration: \"2014-08-17T17:18:25.045563473Z\",\n        ttl: 579416,\n        modifiedIndex: 72821950,\n        createdIndex: 72821950\n      }\n    ],\n    modifiedIndex: 69367741,\n    createdIndex: 69367741\n  }\n}  To rule out firewall settings as a source of your issue, ensure that you can curl each of the IPs from machines in your cluster.  If all of the IPs can be reached, the etcd log can provide more clues:  journalctl -u etcd-member",
            "title": "Stale tokens"
        },
        {
            "location": "/os/cluster-discovery/#communicating-with-discoveryetcdio",
            "text": "If your Flatcar Container Linux cluster can't communicate out to the public internet,  https://discovery.etcd.io  won't work and you'll have to run your own discovery endpoint, which is described below.",
            "title": "Communicating with discovery.etcd.io"
        },
        {
            "location": "/os/cluster-discovery/#setting-advertised-client-addresses-correctly",
            "text": "Each etcd instance submits the list of  -initial-advertise-peer-urls  of each etcd instance to the configured discovery service. It's important to select an address that  all  peers in the cluster can communicate with. If you are configuring a list of addresses, make sure each member can communicate with at least one of the addresses.  For example, if you're located in two regions of a cloud provider, configuring a private  10.x  address will not work between the two regions, and communication will not be possible between all peers. The  -listen-client-urls  flag allows you to bind to a specific list of interfaces and ports (or all interfaces) to ensure your etcd traffic is routed properly.",
            "title": "Setting advertised client addresses correctly"
        },
        {
            "location": "/os/cluster-discovery/#running-your-own-discovery-service",
            "text": "The public discovery service is just an etcd cluster made available to the public internet. Since the discovery service conducts and stores the result of the first leader election, it needs to be consistent. You wouldn't want two machines in the same cluster to think they were both the leader.  Since etcd is designed to this type of leader election, it was an obvious choice to use it for everyone's initial leader election. This means that it's easy to run your own etcd cluster for this purpose.  If you're interested in how discovery API works behind the scenes in etcd, read about  etcd clustering .",
            "title": "Running your own discovery service"
        },
        {
            "location": "/os/verify-images/",
            "text": "Verify Flatcar Container Linux images with GPG\n\u00b6\n\n\nKinvolk publishes new Flatcar Container Linux images for each release across a variety of platforms and hosting providers. Each channel has its own set of images (\nstable\n, \nbeta\n, \nalpha\n, \nedge\n) that are posted to our storage site. Along with each image, a signature is generated from the \nFlatcar Container Linux Image Signing Key\n and posted.\n\n\nAfter downloading your image, you should verify it with \ngpg\n tool. First, download the image signing key:\n\n\ncurl -L -O https://flatcar-linux.org/security/image-signing-key/Flatcar_Image_Signing_Key.asc\n\n\n\nNext, import the public key and verify that the ID matches the website: \nFlatcar Image Signing Key\n\n\ngpg --import --keyid-format LONG Flatcar_Image_Signing_Key.asc\ngpg: key 50E0885593D2DCB4: public key \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\ngpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model\ngpg: depth: 0  valid:   2  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 2u\n\n\n\nNow we're ready to download an image and it's signature, ending in .sig. We're using the QEMU image in this example:\n\n\ncurl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\ncurl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\n\n\n\nVerify image with \ngpg\n tool:\n\n\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\ngpg: Signature made Tue Jun 23 09:39:04 2015 CEST using RSA key ID E5676EFC\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\"\n\n\n\nThe \nGood signature\n message indicates that the file signature is valid. Go launch some machines now that we've successfully verified that this Flatcar Container Linux image isn't corrupt, that it was authored by Kinvolk, and wasn't tampered with in transit.",
            "title": "Verify Flatcar Container Linux Images with GPG"
        },
        {
            "location": "/os/verify-images/#verify-flatcar-container-linux-images-with-gpg",
            "text": "Kinvolk publishes new Flatcar Container Linux images for each release across a variety of platforms and hosting providers. Each channel has its own set of images ( stable ,  beta ,  alpha ,  edge ) that are posted to our storage site. Along with each image, a signature is generated from the  Flatcar Container Linux Image Signing Key  and posted.  After downloading your image, you should verify it with  gpg  tool. First, download the image signing key:  curl -L -O https://flatcar-linux.org/security/image-signing-key/Flatcar_Image_Signing_Key.asc  Next, import the public key and verify that the ID matches the website:  Flatcar Image Signing Key  gpg --import --keyid-format LONG Flatcar_Image_Signing_Key.asc\ngpg: key 50E0885593D2DCB4: public key \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\ngpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model\ngpg: depth: 0  valid:   2  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 2u  Now we're ready to download an image and it's signature, ending in .sig. We're using the QEMU image in this example:  curl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\ncurl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig  Verify image with  gpg  tool:  gpg --verify flatcar_production_qemu_image.img.bz2.sig\ngpg: Signature made Tue Jun 23 09:39:04 2015 CEST using RSA key ID E5676EFC\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\"  The  Good signature  message indicates that the file signature is valid. Go launch some machines now that we've successfully verified that this Flatcar Container Linux image isn't corrupt, that it was authored by Kinvolk, and wasn't tampered with in transit.",
            "title": "Verify Flatcar Container Linux images with GPG"
        },
        {
            "location": "/os/network-config-with-networkd/",
            "text": "Network configuration with networkd\n\u00b6\n\n\nFlatcar Container Linux machines are preconfigured with \nnetworking customized\n for each platform. You can write your own networkd units to replace or override the units created for each platform. This article covers a subset of networkd functionality. You can view the \nfull docs here\n.\n\n\nDrop a networkd unit in \n/etc/systemd/network/\n or inject a unit on boot via a Container Linux Config. Files placed manually on the filesystem will need to reload networkd afterwards with \nsudo systemctl restart systemd-networkd\n. Network units injected via a Container Linux Config will be written to the system before networkd is started, so there are no work-arounds needed.\n\n\nLet's take a look at two common situations: using a static IP and turning off DHCP.\n\n\nStatic networking\n\u00b6\n\n\nTo configure a static IP on \nenp2s0\n, create \nstatic.network\n:\n\n\n[Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4\n\n\n\nPlace the file in \n/etc/systemd/network/\n. To apply the configuration, run:\n\n\nsudo systemctl restart systemd-networkd\n\n\n\nContainer Linux Config\n\u00b6\n\n\nSetting up static networking in your Container Linux Config can be done by writing out the network unit. Be sure to modify the \n[Match]\n section with the name of your desired interface, and replace the IPs:\n\n\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n\n\n\nTurn off DHCP on specific interface\n\u00b6\n\n\nIf you'd like to use DHCP on all interfaces except \nenp2s0\n, create two files. They'll be checked in lexical order, as described in the \nfull network docs\n. Any interfaces matching during earlier files will be ignored during later files.\n\n\n10-static.network\n\u00b6\n\n\n[Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4\n\n\n\nPut your settings-of-last-resort in \n20-dhcp.network\n. For example, any interfaces matching \nen*\n that weren't matched in \n10-static.network\n will be configured with DHCP:\n\n\n20-dhcp.network\n\u00b6\n\n\n[Match]\nName=en*\n\n[Network]\nDHCP=yes\n\n\n\nTo apply the configuration, run \nsudo systemctl restart systemd-networkd\n. Check the status with \nsystemctl status systemd-networkd\n and read the full log with \njournalctl -u systemd-networkd\n.\n\n\nTurn off IPv6 on specific interfaces\n\u00b6\n\n\nWhile IPv6 can be disabled globally at boot by appending \nipv6.disable=1\n to the kernel command line, networkd supports disabling IPv6 on a per-interface basis. When a network unit's \n[Network]\n section has either \nLinkLocalAddressing=ipv4\n or \nLinkLocalAddressing=no\n, networkd will not try to configure IPv6 on the matching interfaces.\n\n\nNote however that even when using the above option, networkd will still be expecting to receive router advertisements if IPv6 is not disabled globally. If IPv6 traffic is not being received by the interface (e.g. due to \nsysctl\n or \nip6tables\n settings), it will remain in the \nconfiguring\n state and potentially cause timeouts for services waiting for the network to be fully configured. To avoid this, the \nIPv6AcceptRA=no\n option should also be set in the \n[Network]\n section.\n\n\nA network unit file's \n[Network]\n section should therefore contain the following to disable IPv6 on its matching interfaces.\n\n\n[Network]\nLinkLocalAddressing=no\nIPv6AcceptRA=no\n\n\n\nConfigure static routes\n\u00b6\n\n\nSpecify static routes in a systemd network unit's \n[Route]\n section. In this example, we create a unit file, \n10-static.network\n, and define in it a static route to the \n172.16.0.0/24\n subnet:\n\n\n10-static.network\n\u00b6\n\n\n[Route]\nGateway=192.168.122.1\nDestination=172.16.0.0/24\n\n\n\nTo specify the same route in a Container Linux Config, create the systemd network unit there instead:\n\n\nnetworkd:\n  units:\n    - name: 10-static.network\n      contents: |\n        [Route]\n        Gateway=192.168.122.1\n        Destination=172.16.0.0/24\n\n\n\nConfigure multiple IP addresses\n\u00b6\n\n\nTo configure multiple IP addresses on one interface, we define multiple \nAddress\n keys in the network unit. In the example below, we've also defined a different gateway for each IP address.\n\n\n20-multi_ip.network\n\u00b6\n\n\n[Match]\nName=eth0\n\n[Network]\nDNS=8.8.8.8\nAddress=10.0.0.101/24\nGateway=10.0.0.1\nAddress=10.0.1.101/24\nGateway=10.0.1.1\n\n\n\nTo do the same thing through a Container Linux Config:\n\n\nnetworkd:\n  units:\n    - name: 20-multi_ip.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=8.8.8.8\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n        Address=10.0.1.101/24\n        Gateway=10.0.1.1\n\n\n\nDebugging networkd\n\u00b6\n\n\nIf you've faced some problems with networkd you can enable debug mode following the instructions below.\n\n\nEnable debugging manually\n\u00b6\n\n\nmkdir -p /etc/systemd/system/systemd-networkd.service.d/\n\n\n\nCreate \nDrop-In\n \n/etc/systemd/system/systemd-networkd.service.d/10-debug.conf\n with following content:\n\n\n[Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nAnd restart \nsystemd-networkd\n service:\n\n\nsystemctl daemon-reload\nsystemctl restart systemd-networkd\njournalctl -b -u systemd-networkd\n\n\n\nEnable debugging through a Container Linux Config\n\u00b6\n\n\nDefine a \nDrop-In\n in a \nContainer Linux Config\n:\n\n\nsystemd:\n  units:\n    - name: systemd-networkd.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nFurther reading\n\u00b6\n\n\nIf you're interested in more general networkd features, check out the \nfull documentation\n.\n\n\nGetting Started with systemd\n\n\nReading the System Log",
            "title": "Using networkd to customize networking"
        },
        {
            "location": "/os/network-config-with-networkd/#network-configuration-with-networkd",
            "text": "Flatcar Container Linux machines are preconfigured with  networking customized  for each platform. You can write your own networkd units to replace or override the units created for each platform. This article covers a subset of networkd functionality. You can view the  full docs here .  Drop a networkd unit in  /etc/systemd/network/  or inject a unit on boot via a Container Linux Config. Files placed manually on the filesystem will need to reload networkd afterwards with  sudo systemctl restart systemd-networkd . Network units injected via a Container Linux Config will be written to the system before networkd is started, so there are no work-arounds needed.  Let's take a look at two common situations: using a static IP and turning off DHCP.",
            "title": "Network configuration with networkd"
        },
        {
            "location": "/os/network-config-with-networkd/#static-networking",
            "text": "To configure a static IP on  enp2s0 , create  static.network :  [Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4  Place the file in  /etc/systemd/network/ . To apply the configuration, run:  sudo systemctl restart systemd-networkd",
            "title": "Static networking"
        },
        {
            "location": "/os/network-config-with-networkd/#container-linux-config",
            "text": "Setting up static networking in your Container Linux Config can be done by writing out the network unit. Be sure to modify the  [Match]  section with the name of your desired interface, and replace the IPs:  networkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/network-config-with-networkd/#turn-off-dhcp-on-specific-interface",
            "text": "If you'd like to use DHCP on all interfaces except  enp2s0 , create two files. They'll be checked in lexical order, as described in the  full network docs . Any interfaces matching during earlier files will be ignored during later files.",
            "title": "Turn off DHCP on specific interface"
        },
        {
            "location": "/os/network-config-with-networkd/#10-staticnetwork",
            "text": "[Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4  Put your settings-of-last-resort in  20-dhcp.network . For example, any interfaces matching  en*  that weren't matched in  10-static.network  will be configured with DHCP:",
            "title": "10-static.network"
        },
        {
            "location": "/os/network-config-with-networkd/#20-dhcpnetwork",
            "text": "[Match]\nName=en*\n\n[Network]\nDHCP=yes  To apply the configuration, run  sudo systemctl restart systemd-networkd . Check the status with  systemctl status systemd-networkd  and read the full log with  journalctl -u systemd-networkd .",
            "title": "20-dhcp.network"
        },
        {
            "location": "/os/network-config-with-networkd/#turn-off-ipv6-on-specific-interfaces",
            "text": "While IPv6 can be disabled globally at boot by appending  ipv6.disable=1  to the kernel command line, networkd supports disabling IPv6 on a per-interface basis. When a network unit's  [Network]  section has either  LinkLocalAddressing=ipv4  or  LinkLocalAddressing=no , networkd will not try to configure IPv6 on the matching interfaces.  Note however that even when using the above option, networkd will still be expecting to receive router advertisements if IPv6 is not disabled globally. If IPv6 traffic is not being received by the interface (e.g. due to  sysctl  or  ip6tables  settings), it will remain in the  configuring  state and potentially cause timeouts for services waiting for the network to be fully configured. To avoid this, the  IPv6AcceptRA=no  option should also be set in the  [Network]  section.  A network unit file's  [Network]  section should therefore contain the following to disable IPv6 on its matching interfaces.  [Network]\nLinkLocalAddressing=no\nIPv6AcceptRA=no",
            "title": "Turn off IPv6 on specific interfaces"
        },
        {
            "location": "/os/network-config-with-networkd/#configure-static-routes",
            "text": "Specify static routes in a systemd network unit's  [Route]  section. In this example, we create a unit file,  10-static.network , and define in it a static route to the  172.16.0.0/24  subnet:",
            "title": "Configure static routes"
        },
        {
            "location": "/os/network-config-with-networkd/#10-staticnetwork_1",
            "text": "[Route]\nGateway=192.168.122.1\nDestination=172.16.0.0/24  To specify the same route in a Container Linux Config, create the systemd network unit there instead:  networkd:\n  units:\n    - name: 10-static.network\n      contents: |\n        [Route]\n        Gateway=192.168.122.1\n        Destination=172.16.0.0/24",
            "title": "10-static.network"
        },
        {
            "location": "/os/network-config-with-networkd/#configure-multiple-ip-addresses",
            "text": "To configure multiple IP addresses on one interface, we define multiple  Address  keys in the network unit. In the example below, we've also defined a different gateway for each IP address.",
            "title": "Configure multiple IP addresses"
        },
        {
            "location": "/os/network-config-with-networkd/#20-multi_ipnetwork",
            "text": "[Match]\nName=eth0\n\n[Network]\nDNS=8.8.8.8\nAddress=10.0.0.101/24\nGateway=10.0.0.1\nAddress=10.0.1.101/24\nGateway=10.0.1.1  To do the same thing through a Container Linux Config:  networkd:\n  units:\n    - name: 20-multi_ip.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=8.8.8.8\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n        Address=10.0.1.101/24\n        Gateway=10.0.1.1",
            "title": "20-multi_ip.network"
        },
        {
            "location": "/os/network-config-with-networkd/#debugging-networkd",
            "text": "If you've faced some problems with networkd you can enable debug mode following the instructions below.",
            "title": "Debugging networkd"
        },
        {
            "location": "/os/network-config-with-networkd/#enable-debugging-manually",
            "text": "mkdir -p /etc/systemd/system/systemd-networkd.service.d/  Create  Drop-In   /etc/systemd/system/systemd-networkd.service.d/10-debug.conf  with following content:  [Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug  And restart  systemd-networkd  service:  systemctl daemon-reload\nsystemctl restart systemd-networkd\njournalctl -b -u systemd-networkd",
            "title": "Enable debugging manually"
        },
        {
            "location": "/os/network-config-with-networkd/#enable-debugging-through-a-container-linux-config",
            "text": "Define a  Drop-In  in a  Container Linux Config :  systemd:\n  units:\n    - name: systemd-networkd.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug",
            "title": "Enable debugging through a Container Linux Config"
        },
        {
            "location": "/os/network-config-with-networkd/#further-reading",
            "text": "If you're interested in more general networkd features, check out the  full documentation .  Getting Started with systemd  Reading the System Log",
            "title": "Further reading"
        },
        {
            "location": "/os/using-systemd-drop-in-units/",
            "text": "Using systemd drop-in units\n\u00b6\n\n\nThere are two methods of overriding default Flatcar Container Linux settings in unit files: copying the unit file from \n/usr/lib64/systemd/system\n to \n/etc/systemd/system\n and modifying the chosen settings. Alternatively, one can create a directory named \nunit.d\n within \n/etc/systemd/system\n and place a drop-in file \nname.conf\n there that only changes the specific settings one is interested in. Note that multiple such drop-in files are read if present.\n\n\nThe advantage of the first method is that one easily overrides the complete unit, the default Flatcar Container Linux unit is not parsed at all anymore. It has the disadvantage that improvements to the unit file supplied by Flatcar Container Linux are not automatically incorporated on updates.\n\n\nThe advantage of the second method is that one only overrides the settings one specifically wants, where updates to the original Flatcar Container Linux unit automatically apply. This has the disadvantage that some future Flatcar Container Linux updates might be incompatible with the local changes, but the risk is much lower.\n\n\nNote that for drop-in files, if one wants to remove entries from a setting that is parsed as a list (and is not a dependency), such as \nConditionPathExists=\n (or e.g. \nExecStart=\n in service units), one needs to first clear the list before re-adding all entries except the one that is to be removed. See below for an example.\n\n\nThis also applies for user instances of systemd, but with different locations for the unit files. See the section on unit load paths in \nofficial systemd doc\n for further details.\n\n\nExample: customizing locksmithd.service\n\u00b6\n\n\nLet's review \n/usr/lib64/systemd/system/locksmithd.service\n unit (you can find it using this command: \nsystemctl list-units | grep locksmithd\n) with the following contents:\n\n\n[Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nLet's walk through increasing the \nRestartSec\n parameter via both methods:\n\n\nOverride only specific option\n\u00b6\n\n\nYou can create a drop-in file \n/etc/systemd/system/locksmithd.service.d/10-restart_60s.conf\n with the following contents:\n\n\n[Service]\nRestartSec=60s\n\n\n\nThen reload systemd, scanning for new or changed units:\n\n\nsystemctl daemon-reload\n\n\n\n\nAnd restart modified service if necessary (in our example we have changed only \nRestartSec\n option, but if you want to change environment variables, \nExecStart\n or other run options you have to restart service):\n\n\nsystemctl restart locksmithd.service\n\n\n\nHere is how that could be implemented within a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      dropins:\n        - name: 10-restart_60s.conf\n          contents: |\n            [Service]\n            RestartSec=60s\n\n\n\nThis change is small and targeted. It is the easiest way to tweak unit's parameters.\n\n\nOverride the whole unit file\n\u00b6\n\n\nAnother way is to override whole systemd unit. Copy default unit file \n/usr/lib64/systemd/system/locksmithd.service\n to \n/etc/systemd/system/locksmithd.service\n and change the chosen settings:\n\n\n[Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=60s\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nContainer Linux Config example:\n\n\nsystemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Cluster reboot manager\n        After=update-engine.service\n        ConditionVirtualization=!container\n        ConditionPathExists=!/usr/.noupdate\n\n        [Service]\n        CPUShares=16\n        MemoryLimit=32M\n        PrivateDevices=true\n        Environment=GOMAXPROCS=1\n        EnvironmentFile=-/usr/share/coreos/update.conf\n        EnvironmentFile=-/etc/coreos/update.conf\n        ExecStart=/usr/lib/locksmith/locksmithd\n        Restart=on-failure\n        RestartSec=60s\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nList drop-ins\n\u00b6\n\n\nTo see all runtime drop-in changes for system units run the command below:\n\n\nsystemd-delta --type=extended\n\n\n\nOther systemd examples\n\u00b6\n\n\nFor another real systemd examples, check out these documents:\n\n\nCustomizing Docker\n\n\nCustomizing the SSH Daemon\n\n\nUsing Environment Variables In systemd Units\n\n\nMore Information\n\u00b6\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs\n\n\nsystemd.target Docs",
            "title": "Using systemd drop-in units"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#using-systemd-drop-in-units",
            "text": "There are two methods of overriding default Flatcar Container Linux settings in unit files: copying the unit file from  /usr/lib64/systemd/system  to  /etc/systemd/system  and modifying the chosen settings. Alternatively, one can create a directory named  unit.d  within  /etc/systemd/system  and place a drop-in file  name.conf  there that only changes the specific settings one is interested in. Note that multiple such drop-in files are read if present.  The advantage of the first method is that one easily overrides the complete unit, the default Flatcar Container Linux unit is not parsed at all anymore. It has the disadvantage that improvements to the unit file supplied by Flatcar Container Linux are not automatically incorporated on updates.  The advantage of the second method is that one only overrides the settings one specifically wants, where updates to the original Flatcar Container Linux unit automatically apply. This has the disadvantage that some future Flatcar Container Linux updates might be incompatible with the local changes, but the risk is much lower.  Note that for drop-in files, if one wants to remove entries from a setting that is parsed as a list (and is not a dependency), such as  ConditionPathExists=  (or e.g.  ExecStart=  in service units), one needs to first clear the list before re-adding all entries except the one that is to be removed. See below for an example.  This also applies for user instances of systemd, but with different locations for the unit files. See the section on unit load paths in  official systemd doc  for further details.",
            "title": "Using systemd drop-in units"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#example-customizing-locksmithdservice",
            "text": "Let's review  /usr/lib64/systemd/system/locksmithd.service  unit (you can find it using this command:  systemctl list-units | grep locksmithd ) with the following contents:  [Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target  Let's walk through increasing the  RestartSec  parameter via both methods:",
            "title": "Example: customizing locksmithd.service"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#override-only-specific-option",
            "text": "You can create a drop-in file  /etc/systemd/system/locksmithd.service.d/10-restart_60s.conf  with the following contents:  [Service]\nRestartSec=60s  Then reload systemd, scanning for new or changed units:  systemctl daemon-reload  And restart modified service if necessary (in our example we have changed only  RestartSec  option, but if you want to change environment variables,  ExecStart  or other run options you have to restart service):  systemctl restart locksmithd.service  Here is how that could be implemented within a Container Linux Config:  systemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      dropins:\n        - name: 10-restart_60s.conf\n          contents: |\n            [Service]\n            RestartSec=60s  This change is small and targeted. It is the easiest way to tweak unit's parameters.",
            "title": "Override only specific option"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#override-the-whole-unit-file",
            "text": "Another way is to override whole systemd unit. Copy default unit file  /usr/lib64/systemd/system/locksmithd.service  to  /etc/systemd/system/locksmithd.service  and change the chosen settings:  [Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=60s\n\n[Install]\nWantedBy=multi-user.target  Container Linux Config example:  systemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Cluster reboot manager\n        After=update-engine.service\n        ConditionVirtualization=!container\n        ConditionPathExists=!/usr/.noupdate\n\n        [Service]\n        CPUShares=16\n        MemoryLimit=32M\n        PrivateDevices=true\n        Environment=GOMAXPROCS=1\n        EnvironmentFile=-/usr/share/coreos/update.conf\n        EnvironmentFile=-/etc/coreos/update.conf\n        ExecStart=/usr/lib/locksmith/locksmithd\n        Restart=on-failure\n        RestartSec=60s\n\n        [Install]\n        WantedBy=multi-user.target",
            "title": "Override the whole unit file"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#list-drop-ins",
            "text": "To see all runtime drop-in changes for system units run the command below:  systemd-delta --type=extended",
            "title": "List drop-ins"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#other-systemd-examples",
            "text": "For another real systemd examples, check out these documents:  Customizing Docker  Customizing the SSH Daemon  Using Environment Variables In systemd Units",
            "title": "Other systemd examples"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#more-information",
            "text": "systemd.service Docs  systemd.unit Docs  systemd.target Docs",
            "title": "More Information"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/",
            "text": "Using environment variables in systemd units\n\u00b6\n\n\nEnvironment directive\n\u00b6\n\n\nsystemd has an Environment directive which sets environment variables for executed processes. It takes a space-separated list of variable assignments. This option may be specified more than once in which case all listed variables will be set. If the same variable is set twice, the later setting will override the earlier setting. If the empty string is assigned to this option, the list of environment variables is reset, all prior assignments have no effect. Environments directives are used in built-in Flatcar Container Linux systemd units, for example in etcd2 and flannel.\n\n\nWith the example below, you can configure your etcd2 daemon to use encryption. Just create \n/etc/systemd/system/etcd2.service.d/30-certificates.conf\n \ndrop-in\n for etcd2.service:\n\n\n[Service]\n# Client Env Vars\nEnvironment=ETCD_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_CERT_FILE=/path/to/server.crt\nEnvironment=ETCD_KEY_FILE=/path/to/server.key\n# Peer Env Vars\nEnvironment=ETCD_PEER_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_PEER_CERT_FILE=/path/to/peers.crt\nEnvironment=ETCD_PEER_KEY_FILE=/path/to/peers.key\n\n\n\nThen run \nsudo systemctl daemon-reload\n and \nsudo systemctl restart etcd2.service\n to apply new environments to etcd2 daemon. You can read more about etcd2 certificates \nhere\n.\n\n\nEnvironmentFile directive\n\u00b6\n\n\nEnvironmentFile similar to Environment directive but reads the environment variables from a text file. The text file should contain new-line-separated variable assignments.\n\n\nFor example, in Flatcar Container Linux, the \ncoreos-metadata.service\n service creates \n/run/metadata/coreos\n. This environment file can be included by other services in order to inject dynamic configuration. Here's an example of the environment file when run on DigitalOcean (the IP addresses have been removed):\n\n\nCOREOS_DIGITALOCEAN_IPV4_ANCHOR_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV4_PRIVATE_0=X.X.X.X\nCOREOS_DIGITALOCEAN_HOSTNAME=test.example.com\nCOREOS_DIGITALOCEAN_IPV4_PUBLIC_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV6_PUBLIC_0=X:X:X:X:X:X:X:X\n\n\n\nThis environment file can then be sourced and its variables used. Here is an example drop-in for \netcd-member.service\n which starts \ncoreos-metadata.service\n and then uses the generated results:\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/bin/etcd2 \\\n  --advertise-client-urls=http://${COREOS_DIGITALOCEAN_IPV4_PUBLIC_0}:2379 \\\n  --initial-advertise-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --listen-client-urls=http://0.0.0.0:2379 \\\n  --listen-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --initial-cluster=%m=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380\n\n\n\nOther examples\n\u00b6\n\n\nUse host IP addresses and EnvironmentFile\n\u00b6\n\n\nYou can also write your host IP addresses into \n/etc/network-environment\n file using \nthis\n utility. Then you can run your Docker containers following way:\n\n\n[Unit]\nDescription=Nginx service\nRequires=etcd2.service\nAfter=etcd2.service\n[Service]\n# Get network environmental variables\nEnvironmentFile=/etc/network-environment\nExecStartPre=-/usr/bin/docker kill nginx\nExecStartPre=-/usr/bin/docker rm nginx\nExecStartPre=/usr/bin/docker pull nginx\nExecStartPre=/usr/bin/etcdctl set /services/nginx '{\"host\": \"%H\", \"ipv4_addr\": ${DEFAULT_IPV4}, \"port\": 80}'\nExecStart=/usr/bin/docker run --rm --name nginx -p ${DEFAULT_IPV4}:80:80 nginx\nExecStop=/usr/bin/docker stop nginx\nExecStopPost=/usr/bin/etcdctl rm /services/nginx\n\n\n\nThis unit file will run nginx Docker container and bind it to specific IP address and port.\n\n\nSystem wide environment variables\n\u00b6\n\n\nYou can define system wide environment variables using a \nContainer Linux Config\n as explained below:\n\n\nstorage:\n  files:\n    - path: /etc/systemd/system.conf.d/10-default-env.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Manager]\n          DefaultEnvironment=HTTP_PROXY=http://192.168.0.1:3128\n    - path: /etc/profile.env\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          export HTTP_PROXY=http://192.168.0.1:3128\n\n\n\nWhere:\n\n\n\n\n/etc/systemd/system.conf.d/10-default-env.conf\n config file will set default environment variables for all systemd units.\n\n\n/etc/profile.env\n will set environment variables for all users logged in Flatcar Container Linux.\n\n\n\n\netcd2.service unit advanced example\n\u00b6\n\n\nA \ncomplete example\n of combining environment variables and systemd \ndrop-ins\n to reconfigure an existing machine running etcd.\n\n\nMore systemd examples\n\u00b6\n\n\nFor more systemd examples, check out these documents:\n\n\nCustomizing Docker\n\n\nCustomizing the SSH Daemon\n\n\nUsing systemd Drop-In Units\n\n\netcd Cluster Runtime Reconfiguration on Flatcar Container Linux\n\n\nMore Information\n\u00b6\n\n\nsystemd.exec Docs\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs",
            "title": "Using environment variables in systemd units"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#using-environment-variables-in-systemd-units",
            "text": "",
            "title": "Using environment variables in systemd units"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#environment-directive",
            "text": "systemd has an Environment directive which sets environment variables for executed processes. It takes a space-separated list of variable assignments. This option may be specified more than once in which case all listed variables will be set. If the same variable is set twice, the later setting will override the earlier setting. If the empty string is assigned to this option, the list of environment variables is reset, all prior assignments have no effect. Environments directives are used in built-in Flatcar Container Linux systemd units, for example in etcd2 and flannel.  With the example below, you can configure your etcd2 daemon to use encryption. Just create  /etc/systemd/system/etcd2.service.d/30-certificates.conf   drop-in  for etcd2.service:  [Service]\n# Client Env Vars\nEnvironment=ETCD_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_CERT_FILE=/path/to/server.crt\nEnvironment=ETCD_KEY_FILE=/path/to/server.key\n# Peer Env Vars\nEnvironment=ETCD_PEER_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_PEER_CERT_FILE=/path/to/peers.crt\nEnvironment=ETCD_PEER_KEY_FILE=/path/to/peers.key  Then run  sudo systemctl daemon-reload  and  sudo systemctl restart etcd2.service  to apply new environments to etcd2 daemon. You can read more about etcd2 certificates  here .",
            "title": "Environment directive"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#environmentfile-directive",
            "text": "EnvironmentFile similar to Environment directive but reads the environment variables from a text file. The text file should contain new-line-separated variable assignments.  For example, in Flatcar Container Linux, the  coreos-metadata.service  service creates  /run/metadata/coreos . This environment file can be included by other services in order to inject dynamic configuration. Here's an example of the environment file when run on DigitalOcean (the IP addresses have been removed):  COREOS_DIGITALOCEAN_IPV4_ANCHOR_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV4_PRIVATE_0=X.X.X.X\nCOREOS_DIGITALOCEAN_HOSTNAME=test.example.com\nCOREOS_DIGITALOCEAN_IPV4_PUBLIC_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV6_PUBLIC_0=X:X:X:X:X:X:X:X  This environment file can then be sourced and its variables used. Here is an example drop-in for  etcd-member.service  which starts  coreos-metadata.service  and then uses the generated results:  [Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/bin/etcd2 \\\n  --advertise-client-urls=http://${COREOS_DIGITALOCEAN_IPV4_PUBLIC_0}:2379 \\\n  --initial-advertise-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --listen-client-urls=http://0.0.0.0:2379 \\\n  --listen-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --initial-cluster=%m=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380",
            "title": "EnvironmentFile directive"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#other-examples",
            "text": "",
            "title": "Other examples"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#use-host-ip-addresses-and-environmentfile",
            "text": "You can also write your host IP addresses into  /etc/network-environment  file using  this  utility. Then you can run your Docker containers following way:  [Unit]\nDescription=Nginx service\nRequires=etcd2.service\nAfter=etcd2.service\n[Service]\n# Get network environmental variables\nEnvironmentFile=/etc/network-environment\nExecStartPre=-/usr/bin/docker kill nginx\nExecStartPre=-/usr/bin/docker rm nginx\nExecStartPre=/usr/bin/docker pull nginx\nExecStartPre=/usr/bin/etcdctl set /services/nginx '{\"host\": \"%H\", \"ipv4_addr\": ${DEFAULT_IPV4}, \"port\": 80}'\nExecStart=/usr/bin/docker run --rm --name nginx -p ${DEFAULT_IPV4}:80:80 nginx\nExecStop=/usr/bin/docker stop nginx\nExecStopPost=/usr/bin/etcdctl rm /services/nginx  This unit file will run nginx Docker container and bind it to specific IP address and port.",
            "title": "Use host IP addresses and EnvironmentFile"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#system-wide-environment-variables",
            "text": "You can define system wide environment variables using a  Container Linux Config  as explained below:  storage:\n  files:\n    - path: /etc/systemd/system.conf.d/10-default-env.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Manager]\n          DefaultEnvironment=HTTP_PROXY=http://192.168.0.1:3128\n    - path: /etc/profile.env\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          export HTTP_PROXY=http://192.168.0.1:3128  Where:   /etc/systemd/system.conf.d/10-default-env.conf  config file will set default environment variables for all systemd units.  /etc/profile.env  will set environment variables for all users logged in Flatcar Container Linux.",
            "title": "System wide environment variables"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#etcd2service-unit-advanced-example",
            "text": "A  complete example  of combining environment variables and systemd  drop-ins  to reconfigure an existing machine running etcd.",
            "title": "etcd2.service unit advanced example"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#more-systemd-examples",
            "text": "For more systemd examples, check out these documents:  Customizing Docker  Customizing the SSH Daemon  Using systemd Drop-In Units  etcd Cluster Runtime Reconfiguration on Flatcar Container Linux",
            "title": "More systemd examples"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#more-information",
            "text": "systemd.exec Docs  systemd.service Docs  systemd.unit Docs",
            "title": "More Information"
        },
        {
            "location": "/os/configuring-dns/",
            "text": "DNS Configuration\n\u00b6\n\n\nBy default, DNS resolution on Flatcar Container Linux is handled through \n/etc/resolv.conf\n, which is a symlink to \n/run/systemd/resolve/resolv.conf\n. This file is managed by \nsystemd-resolved\n. Normally, \nsystemd-resolved\n gets DNS IP addresses from \nsystemd-networkd\n, either via DHCP or static configuration. DNS IP addresses can also be set via \nsystemd-resolved\n's \nresolved.conf\n. See \nNetwork configuration with networkd\n for more information on \nsystemd-networkd\n.\n\n\nUsing a local DNS cache\n\u00b6\n\n\nsystemd-resolved\n includes a caching DNS resolver. To use it for DNS resolution and caching, you must enable it via \nnsswitch.conf\n by adding \nresolve\n to the \nhosts\n section.\n\n\nHere is an example \nContainer Linux Config\n snippet to do that:\n\n\nstorage:\n  files:\n    - path: /etc/nsswitch.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          # /etc/nsswitch.conf:\n\n          passwd:      files usrfiles\n          shadow:      files usrfiles\n          group:       files usrfiles\n\n          hosts:       files usrfiles resolve dns\n          networks:    files usrfiles dns\n\n          services:    files usrfiles\n          protocols:   files usrfiles\n          rpc:         files usrfiles\n\n          ethers:      files\n          netmasks:    files\n          netgroup:    files\n          bootparams:  files\n          automount:   files\n          aliases:     files\n\n\n\nOnly nss-aware applications can take advantage of the \nsystemd-resolved\n cache. Notably, this means that statically linked Go programs and programs running within Docker/rkt will use \n/etc/resolv.conf\n only, and will not use the \nsystemd-resolve\n cache.",
            "title": "Configuring DNS"
        },
        {
            "location": "/os/configuring-dns/#dns-configuration",
            "text": "By default, DNS resolution on Flatcar Container Linux is handled through  /etc/resolv.conf , which is a symlink to  /run/systemd/resolve/resolv.conf . This file is managed by  systemd-resolved . Normally,  systemd-resolved  gets DNS IP addresses from  systemd-networkd , either via DHCP or static configuration. DNS IP addresses can also be set via  systemd-resolved 's  resolved.conf . See  Network configuration with networkd  for more information on  systemd-networkd .",
            "title": "DNS Configuration"
        },
        {
            "location": "/os/configuring-dns/#using-a-local-dns-cache",
            "text": "systemd-resolved  includes a caching DNS resolver. To use it for DNS resolution and caching, you must enable it via  nsswitch.conf  by adding  resolve  to the  hosts  section.  Here is an example  Container Linux Config  snippet to do that:  storage:\n  files:\n    - path: /etc/nsswitch.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          # /etc/nsswitch.conf:\n\n          passwd:      files usrfiles\n          shadow:      files usrfiles\n          group:       files usrfiles\n\n          hosts:       files usrfiles resolve dns\n          networks:    files usrfiles dns\n\n          services:    files usrfiles\n          protocols:   files usrfiles\n          rpc:         files usrfiles\n\n          ethers:      files\n          netmasks:    files\n          netgroup:    files\n          bootparams:  files\n          automount:   files\n          aliases:     files  Only nss-aware applications can take advantage of the  systemd-resolved  cache. Notably, this means that statically linked Go programs and programs running within Docker/rkt will use  /etc/resolv.conf  only, and will not use the  systemd-resolve  cache.",
            "title": "Using a local DNS cache"
        },
        {
            "location": "/os/configuring-date-and-timezone/",
            "text": "Configuring date and time zone\n\u00b6\n\n\nBy default, Flatcar Container Linux machines keep time in the Coordinated Universal Time (UTC) zone and synchronize their clocks with the Network Time Protocol (NTP). This page contains information about customizing those defaults, explains the change in NTP client daemons in recent Flatcar Container Linux versions, and offers advice on best practices for timekeeping in Flatcar Container Linux clusters.\n\n\nViewing and changing time and date\n\u00b6\n\n\nThe \ntimedatectl(1)\n command displays and sets the date, time, and time zone.\n\n\n$ timedatectl status\n      Local time: Wed 2015-08-26 19:29:12 UTC\n  Universal time: Wed 2015-08-26 19:29:12 UTC\n        RTC time: Wed 2015-08-26 19:29:12\n       Time zone: UTC (UTC, +0000)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: n/a\n\n\n\nRecommended: UTC time\n\u00b6\n\n\nTo avoid time zone confusion and the complexities of adjusting clocks for daylight saving time (or not) in accordance with regional custom, we recommend that all machines in Flatcar Container Linux clusters use UTC. This is the default time zone. To reset a machine to this default:\n\n\n$ sudo timedatectl set-timezone UTC\n\n\n\nChanging the time zone\n\u00b6\n\n\nIf your site or application requires a different system time zone, start by listing the available options:\n\n\n$ timedatectl list-timezones\nAfrica/Abidjan\nAfrica/Accra\nAfrica/Addis_Ababa\n\u2026\n\n\n\nPick a time zone from the list and set it:\n\n\n$ sudo timedatectl set-timezone America/New_York\n\n\n\nCheck the changes:\n\n\n$ timedatectl\n      Local time: Wed 2015-08-26 15:44:07 EDT\n  Universal time: Wed 2015-08-26 19:44:07 UTC\n        RTC time: Wed 2015-08-26 19:44:07\n       Time zone: America/New_York (EDT, -0400)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: yes\n Last DST change: DST began at\n                  Sun 2015-03-08 01:59:59 EST\n                  Sun 2015-03-08 03:00:00 EDT\n Next DST change: DST ends (the clock jumps one hour backwards) at\n                  Sun 2015-11-01 01:59:59 EDT\n                  Sun 2015-11-01 01:00:00 EST\n\n\n\nTime synchronization\n\u00b6\n\n\nFlatcar Container Linux clusters use NTP to synchronize the clocks of member nodes, and all machines start an NTP client at boot. Flatcar Container Linux versions later than \n681.0.0\n use \nsystemd-timesyncd(8)\n as the default NTP client. Earlier versions used \nntpd(8)\n. Use \nsystemctl\n to check which service is running:\n\n\n$ systemctl status systemd-timesyncd ntpd\n\u25cf systemd-timesyncd.service - Network Time Synchronization\n   Loaded: loaded (/usr/lib64/systemd/system/systemd-timesyncd.service; disabled; vendor preset: disabled)\n   Active: active (running) since Thu 2015-05-14 05:43:20 UTC; 5 days ago\n     Docs: man:systemd-timesyncd.service(8)\n Main PID: 480 (systemd-timesyn)\n   Status: \"Using Time Server 169.254.169.254:123 (169.254.169.254).\"\n   Memory: 448.0K\n   CGroup: /system.slice/systemd-timesyncd.service\n           \u2514\u2500480 /usr/lib/systemd/systemd-timesyncd\n\n\u25cf ntpd.service - Network Time Service\n   Loaded: loaded (/usr/lib64/systemd/system/ntpd.service; disabled; vendor preset: disabled)\n   Active: inactive (dead)\n\n\n\nRecommended NTP sources\n\u00b6\n\n\nUnless you have a highly reliable and precise time server pool, use your cloud provider's NTP source, or, on bare metal, the default Flatcar Container Linux NTP servers:\n\n\n0.flatcar.pool.ntp.org\n1.flatcar.pool.ntp.org\n2.flatcar.pool.ntp.org\n3.flatcar.pool.ntp.org\n\n\n\nChanging NTP time sources\n\u00b6\n\n\nSystemd-timesyncd\n can discover NTP servers from DHCP, individual \nnetwork\n configs, the file \ntimesyncd.conf\n, or the default \n*.flatcar.pool.ntp.org\n pool.\n\n\nThe default behavior uses NTP servers provided by DHCP. To disable this, write a configuration listing your preferred NTP servers into the file \n/etc/systemd/network/50-dhcp-no-ntp.conf\n:\n\n\n[Network]\nDHCP=v4\nNTP=0.pool.example.com 1.pool.example.com\n\n[DHCP]\nUseMTU=true\nUseDomains=true\nUseNTP=false\n\n\n\nThen restart the network daemon:\n\n\n$ sudo systemctl restart systemd-networkd\n\n\n\nNTP time sources can be set in \ntimesyncd.conf\n with a \nContainer Linux Config\n snippet like:\n\n\nstorage:\n  files:\n    - path: /etc/systemd/timesyncd.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Time]\n          NTP=0.pool.example.com 1.pool.example.com\n\n\n\nSwitching from timesyncd to ntpd\n\u00b6\n\n\nOn Flatcar Container Linux 681.0.0 or later, you can switch from \nsystemd-timesyncd\n back to \nntpd\n with the following commands:\n\n\n$ sudo systemctl stop systemd-timesyncd\n$ sudo systemctl mask systemd-timesyncd\n$ sudo systemctl enable ntpd\n$ sudo systemctl start ntpd\n\n\n\nor with this Container Linux Config snippet:\n\n\nsystemd:\n  units:\n    - name: systemd-timesyncd.service\n      mask: true\n    - name: ntpd.service\n      enable: true\n\n\n\nBecause \ntimesyncd\n and \nntpd\n are mutually exclusive, it's important to \nmask\n the \nsystemd-tinesyncd\n service. \nSystemctl disable\n or \nstop\n alone will not prevent a default service from starting again.\n\n\nConfiguring ntpd\n\u00b6\n\n\nThe \nntpd\n service reads all configuration from the file \n/etc/ntp.conf\n. It does not use DHCP or other configuration sources. To use a different set of NTP servers, replace the \n/etc/ntp.conf\n symlink with something like the following:\n\n\nserver 0.pool.example.com\nserver 1.pool.example.com\n\nrestrict default nomodify nopeer noquery limited kod\nrestrict 127.0.0.1\nrestrict [::1]\n\n\n\nThen ask \nntpd\n to reload its configuration:\n\n\n$ sudo systemctl reload ntpd\n\n\n\nOr, in a \nContainer Linux Config\n:\n\n\nstorage:\n  files:\n    - path: /etc/ntp.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          server 0.pool.example.com\n          server 1.pool.example.com\n\n          # - Allow only time queries, at a limited rate.\n          # - Allow all local queries (IPv4, IPv6)\n          restrict default nomodify nopeer noquery limited kod\n          restrict 127.0.0.1\n          restrict [::1]",
            "title": "Configuring date & timezone"
        },
        {
            "location": "/os/configuring-date-and-timezone/#configuring-date-and-time-zone",
            "text": "By default, Flatcar Container Linux machines keep time in the Coordinated Universal Time (UTC) zone and synchronize their clocks with the Network Time Protocol (NTP). This page contains information about customizing those defaults, explains the change in NTP client daemons in recent Flatcar Container Linux versions, and offers advice on best practices for timekeeping in Flatcar Container Linux clusters.",
            "title": "Configuring date and time zone"
        },
        {
            "location": "/os/configuring-date-and-timezone/#viewing-and-changing-time-and-date",
            "text": "The  timedatectl(1)  command displays and sets the date, time, and time zone.  $ timedatectl status\n      Local time: Wed 2015-08-26 19:29:12 UTC\n  Universal time: Wed 2015-08-26 19:29:12 UTC\n        RTC time: Wed 2015-08-26 19:29:12\n       Time zone: UTC (UTC, +0000)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: n/a",
            "title": "Viewing and changing time and date"
        },
        {
            "location": "/os/configuring-date-and-timezone/#recommended-utc-time",
            "text": "To avoid time zone confusion and the complexities of adjusting clocks for daylight saving time (or not) in accordance with regional custom, we recommend that all machines in Flatcar Container Linux clusters use UTC. This is the default time zone. To reset a machine to this default:  $ sudo timedatectl set-timezone UTC",
            "title": "Recommended: UTC time"
        },
        {
            "location": "/os/configuring-date-and-timezone/#changing-the-time-zone",
            "text": "If your site or application requires a different system time zone, start by listing the available options:  $ timedatectl list-timezones\nAfrica/Abidjan\nAfrica/Accra\nAfrica/Addis_Ababa\n\u2026  Pick a time zone from the list and set it:  $ sudo timedatectl set-timezone America/New_York  Check the changes:  $ timedatectl\n      Local time: Wed 2015-08-26 15:44:07 EDT\n  Universal time: Wed 2015-08-26 19:44:07 UTC\n        RTC time: Wed 2015-08-26 19:44:07\n       Time zone: America/New_York (EDT, -0400)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: yes\n Last DST change: DST began at\n                  Sun 2015-03-08 01:59:59 EST\n                  Sun 2015-03-08 03:00:00 EDT\n Next DST change: DST ends (the clock jumps one hour backwards) at\n                  Sun 2015-11-01 01:59:59 EDT\n                  Sun 2015-11-01 01:00:00 EST",
            "title": "Changing the time zone"
        },
        {
            "location": "/os/configuring-date-and-timezone/#time-synchronization",
            "text": "Flatcar Container Linux clusters use NTP to synchronize the clocks of member nodes, and all machines start an NTP client at boot. Flatcar Container Linux versions later than  681.0.0  use  systemd-timesyncd(8)  as the default NTP client. Earlier versions used  ntpd(8) . Use  systemctl  to check which service is running:  $ systemctl status systemd-timesyncd ntpd\n\u25cf systemd-timesyncd.service - Network Time Synchronization\n   Loaded: loaded (/usr/lib64/systemd/system/systemd-timesyncd.service; disabled; vendor preset: disabled)\n   Active: active (running) since Thu 2015-05-14 05:43:20 UTC; 5 days ago\n     Docs: man:systemd-timesyncd.service(8)\n Main PID: 480 (systemd-timesyn)\n   Status: \"Using Time Server 169.254.169.254:123 (169.254.169.254).\"\n   Memory: 448.0K\n   CGroup: /system.slice/systemd-timesyncd.service\n           \u2514\u2500480 /usr/lib/systemd/systemd-timesyncd\n\n\u25cf ntpd.service - Network Time Service\n   Loaded: loaded (/usr/lib64/systemd/system/ntpd.service; disabled; vendor preset: disabled)\n   Active: inactive (dead)",
            "title": "Time synchronization"
        },
        {
            "location": "/os/configuring-date-and-timezone/#recommended-ntp-sources",
            "text": "Unless you have a highly reliable and precise time server pool, use your cloud provider's NTP source, or, on bare metal, the default Flatcar Container Linux NTP servers:  0.flatcar.pool.ntp.org\n1.flatcar.pool.ntp.org\n2.flatcar.pool.ntp.org\n3.flatcar.pool.ntp.org",
            "title": "Recommended NTP sources"
        },
        {
            "location": "/os/configuring-date-and-timezone/#changing-ntp-time-sources",
            "text": "Systemd-timesyncd  can discover NTP servers from DHCP, individual  network  configs, the file  timesyncd.conf , or the default  *.flatcar.pool.ntp.org  pool.  The default behavior uses NTP servers provided by DHCP. To disable this, write a configuration listing your preferred NTP servers into the file  /etc/systemd/network/50-dhcp-no-ntp.conf :  [Network]\nDHCP=v4\nNTP=0.pool.example.com 1.pool.example.com\n\n[DHCP]\nUseMTU=true\nUseDomains=true\nUseNTP=false  Then restart the network daemon:  $ sudo systemctl restart systemd-networkd  NTP time sources can be set in  timesyncd.conf  with a  Container Linux Config  snippet like:  storage:\n  files:\n    - path: /etc/systemd/timesyncd.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Time]\n          NTP=0.pool.example.com 1.pool.example.com",
            "title": "Changing NTP time sources"
        },
        {
            "location": "/os/configuring-date-and-timezone/#switching-from-timesyncd-to-ntpd",
            "text": "On Flatcar Container Linux 681.0.0 or later, you can switch from  systemd-timesyncd  back to  ntpd  with the following commands:  $ sudo systemctl stop systemd-timesyncd\n$ sudo systemctl mask systemd-timesyncd\n$ sudo systemctl enable ntpd\n$ sudo systemctl start ntpd  or with this Container Linux Config snippet:  systemd:\n  units:\n    - name: systemd-timesyncd.service\n      mask: true\n    - name: ntpd.service\n      enable: true  Because  timesyncd  and  ntpd  are mutually exclusive, it's important to  mask  the  systemd-tinesyncd  service.  Systemctl disable  or  stop  alone will not prevent a default service from starting again.",
            "title": "Switching from timesyncd to ntpd"
        },
        {
            "location": "/os/configuring-date-and-timezone/#configuring-ntpd",
            "text": "The  ntpd  service reads all configuration from the file  /etc/ntp.conf . It does not use DHCP or other configuration sources. To use a different set of NTP servers, replace the  /etc/ntp.conf  symlink with something like the following:  server 0.pool.example.com\nserver 1.pool.example.com\n\nrestrict default nomodify nopeer noquery limited kod\nrestrict 127.0.0.1\nrestrict [::1]  Then ask  ntpd  to reload its configuration:  $ sudo systemctl reload ntpd  Or, in a  Container Linux Config :  storage:\n  files:\n    - path: /etc/ntp.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          server 0.pool.example.com\n          server 1.pool.example.com\n\n          # - Allow only time queries, at a limited rate.\n          # - Allow all local queries (IPv4, IPv6)\n          restrict default nomodify nopeer noquery limited kod\n          restrict 127.0.0.1\n          restrict [::1]",
            "title": "Configuring ntpd"
        },
        {
            "location": "/os/adding-users/",
            "text": "Adding users\n\u00b6\n\n\nYou can create user accounts on a Flatcar Container Linux machine manually with \nuseradd\n or via a Container Linux Config when the machine is created.\n\n\nAdd Users via Container Linux Configs\n\u00b6\n\n\nIn your Container Linux Config, you can specify many \ndifferent parameters\n for each user. Here's an example:\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n    - name: elroy\n      password_hash: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n      groups: [ sudo, docker ]\n\n\n\nAdd user manually\n\u00b6\n\n\nIf you'd like to add a user manually, SSH to the machine and use the \nuseradd\n tool. To create the user \nuser\n, run:\n\n\nsudo useradd -p \"*\" -U -m user1 -G sudo\n\n\n\nThe \n\"*\"\n creates a user that cannot login with a password but can log in via SSH key. \n-U\n creates a group for the user, \n-G\n adds the user to the existing \nsudo\n group and \n-m\n creates a home directory. If you'd like to add a password for the user, run:\n\n\n$ sudo passwd user1\nNew password:\nRe-enter new password:\npasswd: password changed.\n\n\n\nTo assign an SSH key, run:\n\n\nupdate-ssh-keys -u user1 -a user1 user1.pem\n\n\n\nGrant sudo Access\n\u00b6\n\n\nIf you trust the user, you can grant administrative privileges using \nvisudo\n.\u00a0\nvisudo\n checks the file syntax before actually overwriting the \nsudoers\n file. This command should be run as root to avoid losing sudo access in the event of a failure. Instead of editing \n/etc/sudo.conf\n directly you will create a new file under the \n/etc/sudoers.d/\n directory. When you run visudo, it is required that you specify which file you are attempting to edit with the \n-f\n argument:\u00a0\n\n\n# visudo -f /etc/sudoers.d/user1\n\n\n\nAdd a the line:\n\n\nuser1 ALL=(ALL) NOPASSWD: ALL\n\n\n\nCheck that sudo has been granted:\n\n\n# su user1\n$ cat /etc/sudoers.d/user1\ncat: /etc/sudoers.d/user1: Permission denied\n\n$ sudo cat /etc/sudoers.d/user1\nuser1 ALL=(ALL) NOPASSWD: ALL",
            "title": "Adding users"
        },
        {
            "location": "/os/adding-users/#adding-users",
            "text": "You can create user accounts on a Flatcar Container Linux machine manually with  useradd  or via a Container Linux Config when the machine is created.",
            "title": "Adding users"
        },
        {
            "location": "/os/adding-users/#add-users-via-container-linux-configs",
            "text": "In your Container Linux Config, you can specify many  different parameters  for each user. Here's an example:  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n    - name: elroy\n      password_hash: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n      groups: [ sudo, docker ]",
            "title": "Add Users via Container Linux Configs"
        },
        {
            "location": "/os/adding-users/#add-user-manually",
            "text": "If you'd like to add a user manually, SSH to the machine and use the  useradd  tool. To create the user  user , run:  sudo useradd -p \"*\" -U -m user1 -G sudo  The  \"*\"  creates a user that cannot login with a password but can log in via SSH key.  -U  creates a group for the user,  -G  adds the user to the existing  sudo  group and  -m  creates a home directory. If you'd like to add a password for the user, run:  $ sudo passwd user1\nNew password:\nRe-enter new password:\npasswd: password changed.  To assign an SSH key, run:  update-ssh-keys -u user1 -a user1 user1.pem",
            "title": "Add user manually"
        },
        {
            "location": "/os/adding-users/#grant-sudo-access",
            "text": "If you trust the user, you can grant administrative privileges using  visudo .\u00a0 visudo  checks the file syntax before actually overwriting the  sudoers  file. This command should be run as root to avoid losing sudo access in the event of a failure. Instead of editing  /etc/sudo.conf  directly you will create a new file under the  /etc/sudoers.d/  directory. When you run visudo, it is required that you specify which file you are attempting to edit with the  -f  argument:\u00a0  # visudo -f /etc/sudoers.d/user1  Add a the line:  user1 ALL=(ALL) NOPASSWD: ALL  Check that sudo has been granted:  # su user1\n$ cat /etc/sudoers.d/user1\ncat: /etc/sudoers.d/user1: Permission denied\n\n$ sudo cat /etc/sudoers.d/user1\nuser1 ALL=(ALL) NOPASSWD: ALL",
            "title": "Grant sudo Access"
        },
        {
            "location": "/os/other-settings/",
            "text": "Tips and other settings\n\u00b6\n\n\nLoading kernel modules\n\u00b6\n\n\nMost Linux kernel modules get automatically loaded as-needed but there are a some situations where this doesn't work. Problems can arise if there is boot-time dependencies are sensitive to exactly when the module gets loaded. Module auto-loading can be broken all-together if the operation requiring the module happens inside of a container. \niptables\n and other netfilter features can easily encounter both of these issues. To force a module to be loaded early during boot simply list them in a file under \n/etc/modules-load.d\n. The file name must end in \n.conf\n.\n\n\necho nf_conntrack > /etc/modules-load.d/nf.conf\n\n\n\nOr, using a Container Linux Config:\n\n\nstorage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: nf_conntrack\n\n\n\nLoading kernel modules with options\n\u00b6\n\n\nThe following section demonstrates how to provide module options when loading. After these configs are processed, the dummy module is loaded into the kernel, and five dummy interfaces are added to the network stack.\n\n\nFurther details can be found in the systemd man pages:\n\nmodules-load.d(5)\n\n\nsystemd-modules-load.service(8)\n\n\nmodprobe.d(5)\n\n\nThis example Container Linux Config loads the \ndummy\n network interface module with an option specifying the number of interfaces the module should create when loaded (\nnumdummies=5\n):\n\n\nstorage:\n  files:\n    - path: /etc/modprobe.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: options dummy numdummies=5\n    - path: /etc/modules-load.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: dummy\n\n\n\nTuning sysctl parameters\n\u00b6\n\n\nThe Linux kernel offers a plethora of knobs under \n/proc/sys\n to control the availability of different features and tune performance parameters. For one-shot changes values can be written directly to the files under \n/proc/sys\n but persistent settings must be written to \n/etc/sysctl.d\n:\n\n\necho net.netfilter.nf_conntrack_max=131072 > /etc/sysctl.d/nf.conf\nsysctl --system\n\n\n\nSome parameters, such as the conntrack one above, are only available after the module they control has been loaded. To ensure any modules are loaded in advance use \nmodules-load.d\n as described above. A complete Container Linux Config using both would look like:\n\n\nstorage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          nf_conntrack\n    - path: /etc/sysctl.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.netfilter.nf_conntrack_max=131072\n\n\n\nFurther details can be found in the systemd man pages:\n\nsysctl.d(5)\n\n\nsystemd-sysctl.service(8)\n\n\nAdding custom kernel boot options\n\u00b6\n\n\nThe Flatcar Container Linux bootloader parses the configuration file \n/usr/share/oem/grub.cfg\n, where custom kernel boot options may be set.\n\n\nThe \n/usr/share/oem/grub.cfg\n file can be configured with Ignition. Note that Ignition runs after GRUB. Therefore, the GRUB configuration won't take effect until the next reboot of the node. \n\n\nHere's an example configuration:\n\n\nstorage:\n  filesystems:\n    - name: \"OEM\"\n      mount:\n        device: \"/dev/disk/by-label/OEM\"\n        format: \"ext4\"\n  files:\n    - filesystem: \"OEM\"\n      path: \"/grub.cfg\"\n      mode: 0644\n      append: true\n      contents:\n        inline: |\n          set linux_append=\"$linux_append flatcar.autologin=tty1\"\n\n\n\nEnable Flatcar Container Linux autologin\n\u00b6\n\n\nTo login without a password on every boot, edit \n/usr/share/oem/grub.cfg\n to add the line:\n\n\nset linux_append=\"$linux_append flatcar.autologin=tty1\"\n\n\n\nEnable systemd debug logging\n\u00b6\n\n\nEdit \n/usr/share/oem/grub.cfg\n to add the following line, enabling systemd's most verbose \ndebug\n-level logging:\n\n\nset linux_append=\"$linux_append systemd.log_level=debug\"\n\n\n\nMask a systemd unit\n\u00b6\n\n\nCompletely disable the \nsystemd-networkd.service\n unit by adding this line to \n/usr/share/oem/grub.cfg\n:\n\n\nset linux_append=\"$linux_append systemd.mask=systemd-networkd.service\"\n\n\n\nAdding custom messages to MOTD\n\u00b6\n\n\nWhen logging in interactively, a brief message (the \"Message of the Day (MOTD)\") reports the Flatcar Container Linux release channel, version, and a list of any services or systemd units that have failed. Additional text can be added by dropping text files into \n/etc/motd.d\n. The directory may need to be created first, and the drop-in file name must end in \n.conf\n. Flatcar Container Linux versions 555.0.0 and greater support customization of the MOTD.\n\n\nmkdir -p /etc/motd.d\necho \"This machine is dedicated to computing Pi\" > /etc/motd.d/pi.conf\n\n\n\nOr via a Container Linux Config:\n\n\nstorage:\n  files:\n    - path: /etc/motd.d/pi.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: This machine is dedicated to computing Pi\n\n\n\nPrevent login prompts from clearing the console\n\u00b6\n\n\nThe system boot messages that are printed to the console will be cleared when systemd starts a login prompt. In order to preserve these messages, the \ngetty\n services will need to have their \nTTYVTDisallocate\n setting disabled. This can be achieved with a drop-in for the template unit, \ngetty@.service\n. Note that the console will still scroll so the login prompt is at the top of the screen, but the boot messages will be available by scrolling.\n\n\nmkdir -p '/etc/systemd/system/getty@.service.d'\necho -e '[Service]\\nTTYVTDisallocate=no' > '/etc/systemd/system/getty@.service.d/no-disallocate.conf'\n\n\n\nOr via a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: getty@.service\n      dropins:\n        - name: no-disallocate.conf\n          contents: |\n            [Service]\n            TTYVTDisallocate=no\n\n\n\nWhen the \nTTYVTDisallocate\n setting is disabled, the console scrollback is not cleared on logout, not even by the \nclear\n command in the default \n.bash_logout\n file. Scrollback must be cleared explicitly, e.g. by running \necho -en '\\033[3J' > /dev/console\n as the root user.",
            "title": "Kernel modules / sysctl parameters"
        },
        {
            "location": "/os/other-settings/#tips-and-other-settings",
            "text": "",
            "title": "Tips and other settings"
        },
        {
            "location": "/os/other-settings/#loading-kernel-modules",
            "text": "Most Linux kernel modules get automatically loaded as-needed but there are a some situations where this doesn't work. Problems can arise if there is boot-time dependencies are sensitive to exactly when the module gets loaded. Module auto-loading can be broken all-together if the operation requiring the module happens inside of a container.  iptables  and other netfilter features can easily encounter both of these issues. To force a module to be loaded early during boot simply list them in a file under  /etc/modules-load.d . The file name must end in  .conf .  echo nf_conntrack > /etc/modules-load.d/nf.conf  Or, using a Container Linux Config:  storage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: nf_conntrack",
            "title": "Loading kernel modules"
        },
        {
            "location": "/os/other-settings/#loading-kernel-modules-with-options",
            "text": "The following section demonstrates how to provide module options when loading. After these configs are processed, the dummy module is loaded into the kernel, and five dummy interfaces are added to the network stack.  Further details can be found in the systemd man pages: modules-load.d(5)  systemd-modules-load.service(8)  modprobe.d(5)  This example Container Linux Config loads the  dummy  network interface module with an option specifying the number of interfaces the module should create when loaded ( numdummies=5 ):  storage:\n  files:\n    - path: /etc/modprobe.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: options dummy numdummies=5\n    - path: /etc/modules-load.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: dummy",
            "title": "Loading kernel modules with options"
        },
        {
            "location": "/os/other-settings/#tuning-sysctl-parameters",
            "text": "The Linux kernel offers a plethora of knobs under  /proc/sys  to control the availability of different features and tune performance parameters. For one-shot changes values can be written directly to the files under  /proc/sys  but persistent settings must be written to  /etc/sysctl.d :  echo net.netfilter.nf_conntrack_max=131072 > /etc/sysctl.d/nf.conf\nsysctl --system  Some parameters, such as the conntrack one above, are only available after the module they control has been loaded. To ensure any modules are loaded in advance use  modules-load.d  as described above. A complete Container Linux Config using both would look like:  storage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          nf_conntrack\n    - path: /etc/sysctl.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.netfilter.nf_conntrack_max=131072  Further details can be found in the systemd man pages: sysctl.d(5)  systemd-sysctl.service(8)",
            "title": "Tuning sysctl parameters"
        },
        {
            "location": "/os/other-settings/#adding-custom-kernel-boot-options",
            "text": "The Flatcar Container Linux bootloader parses the configuration file  /usr/share/oem/grub.cfg , where custom kernel boot options may be set.  The  /usr/share/oem/grub.cfg  file can be configured with Ignition. Note that Ignition runs after GRUB. Therefore, the GRUB configuration won't take effect until the next reboot of the node.   Here's an example configuration:  storage:\n  filesystems:\n    - name: \"OEM\"\n      mount:\n        device: \"/dev/disk/by-label/OEM\"\n        format: \"ext4\"\n  files:\n    - filesystem: \"OEM\"\n      path: \"/grub.cfg\"\n      mode: 0644\n      append: true\n      contents:\n        inline: |\n          set linux_append=\"$linux_append flatcar.autologin=tty1\"",
            "title": "Adding custom kernel boot options"
        },
        {
            "location": "/os/other-settings/#enable-flatcar-container-linux-autologin",
            "text": "To login without a password on every boot, edit  /usr/share/oem/grub.cfg  to add the line:  set linux_append=\"$linux_append flatcar.autologin=tty1\"",
            "title": "Enable Flatcar Container Linux autologin"
        },
        {
            "location": "/os/other-settings/#enable-systemd-debug-logging",
            "text": "Edit  /usr/share/oem/grub.cfg  to add the following line, enabling systemd's most verbose  debug -level logging:  set linux_append=\"$linux_append systemd.log_level=debug\"",
            "title": "Enable systemd debug logging"
        },
        {
            "location": "/os/other-settings/#mask-a-systemd-unit",
            "text": "Completely disable the  systemd-networkd.service  unit by adding this line to  /usr/share/oem/grub.cfg :  set linux_append=\"$linux_append systemd.mask=systemd-networkd.service\"",
            "title": "Mask a systemd unit"
        },
        {
            "location": "/os/other-settings/#adding-custom-messages-to-motd",
            "text": "When logging in interactively, a brief message (the \"Message of the Day (MOTD)\") reports the Flatcar Container Linux release channel, version, and a list of any services or systemd units that have failed. Additional text can be added by dropping text files into  /etc/motd.d . The directory may need to be created first, and the drop-in file name must end in  .conf . Flatcar Container Linux versions 555.0.0 and greater support customization of the MOTD.  mkdir -p /etc/motd.d\necho \"This machine is dedicated to computing Pi\" > /etc/motd.d/pi.conf  Or via a Container Linux Config:  storage:\n  files:\n    - path: /etc/motd.d/pi.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: This machine is dedicated to computing Pi",
            "title": "Adding custom messages to MOTD"
        },
        {
            "location": "/os/other-settings/#prevent-login-prompts-from-clearing-the-console",
            "text": "The system boot messages that are printed to the console will be cleared when systemd starts a login prompt. In order to preserve these messages, the  getty  services will need to have their  TTYVTDisallocate  setting disabled. This can be achieved with a drop-in for the template unit,  getty@.service . Note that the console will still scroll so the login prompt is at the top of the screen, but the boot messages will be available by scrolling.  mkdir -p '/etc/systemd/system/getty@.service.d'\necho -e '[Service]\\nTTYVTDisallocate=no' > '/etc/systemd/system/getty@.service.d/no-disallocate.conf'  Or via a Container Linux Config:  systemd:\n  units:\n    - name: getty@.service\n      dropins:\n        - name: no-disallocate.conf\n          contents: |\n            [Service]\n            TTYVTDisallocate=no  When the  TTYVTDisallocate  setting is disabled, the console scrollback is not cleared on logout, not even by the  clear  command in the default  .bash_logout  file. Scrollback must be cleared explicitly, e.g. by running  echo -en '\\033[3J' > /dev/console  as the root user.",
            "title": "Prevent login prompts from clearing the console"
        },
        {
            "location": "/os/adding-disk-space/",
            "text": "Adding disk space to your Flatcar Container Linux machine\n\u00b6\n\n\nOn a Flatcar Container Linux machine, the operating system itself is mounted as a read-only partition at \n/usr\n. The root partition provides read-write storage by default and on a fresh install is mostly blank. The default size of this partition depends on the platform but it is usually between 3GB and 16GB. If more space is required simply extend the virtual machine's disk image and Flatcar Container Linux will fix the partition table and resize the root partition to fill the disk on the next boot.\n\n\nAmazon EC2\n\u00b6\n\n\nAmazon doesn't support directly resizing volumes, you must take a snapshot and create a new volume based on that snapshot. Refer to the AWS EC2 documentation on \nexpanding EBS volumes\n for detailed instructions.\n\n\nQEMU (qemu-img)\n\u00b6\n\n\nEven if you are not using Qemu itself the qemu-img tool is the easiest to use. It will work on raw, qcow2, vmdk, and most other formats. The command accepts either an absolute size or a relative size by by adding \n+\n prefix. Unit suffixes such as \nG\n or \nM\n are also supported.\n\n\n# Increase the disk size by 5GB\nqemu-img resize flatcar_production_qemu_image.img +5G\n\n\n\nVMware\n\u00b6\n\n\nThe interface available for resizing disks in VMware varies depending on the product. See this \nKnowledge Base article\n for details. Most products include a tool called \nvmware-vdiskmanager\n. The size must be the absolute disk size, relative sizes are not supported so be careful to only increase the size, not shrink it. The unit suffixes \nGb\n and \nMb\n are supported.\n\n\n# Set the disk size to 20GB\nvmware-vdiskmanager -x 20Gb flatcar_developer_vmware_insecure.vmx\n\n\n\nVirtualBox\n\u00b6\n\n\nUse qemu-img or vmware-vdiskmanager as described above. VirtualBox does not support resizing VMDK disk images, only VDI and VHD disks. Meanwhile VirtualBox only supports using VMDK disk images with the OVF config file format used for importing/exporting virtual machines.\n\n\nIf you have have no other options you can try converting the VMDK disk image to a VDI image and configuring a new virtual machine with it:\n\n\nVBoxManage clonehd old.vmdk new.vdi --format VDI\nVBoxManage modifyhd new.vdi --resize 20480",
            "title": "Adding disk space"
        },
        {
            "location": "/os/adding-disk-space/#adding-disk-space-to-your-flatcar-container-linux-machine",
            "text": "On a Flatcar Container Linux machine, the operating system itself is mounted as a read-only partition at  /usr . The root partition provides read-write storage by default and on a fresh install is mostly blank. The default size of this partition depends on the platform but it is usually between 3GB and 16GB. If more space is required simply extend the virtual machine's disk image and Flatcar Container Linux will fix the partition table and resize the root partition to fill the disk on the next boot.",
            "title": "Adding disk space to your Flatcar Container Linux machine"
        },
        {
            "location": "/os/adding-disk-space/#amazon-ec2",
            "text": "Amazon doesn't support directly resizing volumes, you must take a snapshot and create a new volume based on that snapshot. Refer to the AWS EC2 documentation on  expanding EBS volumes  for detailed instructions.",
            "title": "Amazon EC2"
        },
        {
            "location": "/os/adding-disk-space/#qemu-qemu-img",
            "text": "Even if you are not using Qemu itself the qemu-img tool is the easiest to use. It will work on raw, qcow2, vmdk, and most other formats. The command accepts either an absolute size or a relative size by by adding  +  prefix. Unit suffixes such as  G  or  M  are also supported.  # Increase the disk size by 5GB\nqemu-img resize flatcar_production_qemu_image.img +5G",
            "title": "QEMU (qemu-img)"
        },
        {
            "location": "/os/adding-disk-space/#vmware",
            "text": "The interface available for resizing disks in VMware varies depending on the product. See this  Knowledge Base article  for details. Most products include a tool called  vmware-vdiskmanager . The size must be the absolute disk size, relative sizes are not supported so be careful to only increase the size, not shrink it. The unit suffixes  Gb  and  Mb  are supported.  # Set the disk size to 20GB\nvmware-vdiskmanager -x 20Gb flatcar_developer_vmware_insecure.vmx",
            "title": "VMware"
        },
        {
            "location": "/os/adding-disk-space/#virtualbox",
            "text": "Use qemu-img or vmware-vdiskmanager as described above. VirtualBox does not support resizing VMDK disk images, only VDI and VHD disks. Meanwhile VirtualBox only supports using VMDK disk images with the OVF config file format used for importing/exporting virtual machines.  If you have have no other options you can try converting the VMDK disk image to a VDI image and configuring a new virtual machine with it:  VBoxManage clonehd old.vmdk new.vdi --format VDI\nVBoxManage modifyhd new.vdi --resize 20480",
            "title": "VirtualBox"
        },
        {
            "location": "/os/mounting-storage/",
            "text": "Mounting storage\n\u00b6\n\n\nContainer Linux Configs can be used to format and attach additional filesystems to Flatcar Container Linux nodes, whether such storage is provided by an underlying cloud platform, physical disk, SAN, or NAS system. This is done by specifying how partitions should be mounted in the config, and then using a \nsystemd mount unit\n to mount the partition. By \nsystemd convention\n, mount unit names derive from the target mount point, with interior slashes replaced by dashes, and the \n.mount\n extension appended. A unit mounting onto \n/var/www\n is thus named \nvar-www.mount\n.\n\n\nMount units name the source filesystem and target mount point, and optionally the filesystem type. \nSystemd\n mounts filesystems defined in such units at boot time. The following example formats an \nEC2 ephemeral disk\n and then mounts it at the node's \n/media/ephemeral\n directory. The mount unit is therefore named \nmedia-ephemeral.mount\n.\n\n\nstorage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target\n\n\n\nUse attached storage for Docker\n\u00b6\n\n\nDocker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Be aware that some cloud providers treat certain disks as ephemeral and you will lose all Docker images contained on that disk.\n\n\nWe're going to format a device as ext4 and then mount it to \n/var/lib/docker\n, where Docker stores images. Be sure to hardcode the correct device or look for a device by label:\n\n\nstorage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Description=Mount ephemeral to /var/lib/docker\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/var/lib/docker\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target\n    - name: docker.service\n      dropins:\n        - name: 10-wait-docker.conf\n          contents: |\n            [Unit]\n            After=var-lib-docker.mount\n            Requires=var-lib-docker.mount\n\n\n\nCreating and mounting a btrfs volume file\n\u00b6\n\n\nFlatcar Container Linux uses ext4 + overlayfs to provide a layered filesystem for the root partition. If you'd like to use btrfs for your Docker containers, you can do so with two systemd units: one that creates and formats a btrfs volume file and another that mounts it.\n\n\nIn this example, we are going to mount a new 25GB btrfs volume file to \n/var/lib/docker\n. One can verify that Docker is using the btrfs storage driver once the Docker service has started by executing \nsudo docker info\n. We recommend allocating \nno more than 85%\n of the available disk space for a btrfs filesystem as journald will also require space on the host filesystem.\n\n\nsystemd:\n  units:\n    - name: format-var-lib-docker.service\n      contents: |\n        [Unit]\n        Before=docker.service var-lib-docker.mount\n        RequiresMountsFor=/var/lib\n        ConditionPathExists=!/var/lib/docker.btrfs\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/truncate --size=25G /var/lib/docker.btrfs\n        ExecStart=/usr/sbin/mkfs.btrfs /var/lib/docker.btrfs\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=docker.service\n        After=format-var-lib-docker.service\n        Requires=format-var-lib-docker.service\n        [Mount]\n        What=/var/lib/docker.btrfs\n        Where=/var/lib/docker\n        Type=btrfs\n        Options=loop,discard\n        [Install]\n        RequiredBy=docker.service\n\n\n\nNote the declaration of \nConditionPathExists=!/var/lib/docker.btrfs\n. Without this line, systemd would reformat the btrfs filesystem every time the machine starts.\n\n\nMounting NFS exports\n\u00b6\n\n\nThis Container Linux Config excerpt mounts an NFS export onto the Flatcar Container Linux node's \n/var/www\n.\n\n\nsystemd:\n  units:\n    - name: var-www.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=remote-fs.target\n        [Mount]\n        What=nfs.example.com:/var/www\n        Where=/var/www\n        Type=nfs\n        [Install]\n        WantedBy=remote-fs.target\n\n\n\nTo declare that another service depends on this mount, name the mount unit in the dependent unit's \nAfter\n and \nRequires\n properties:\n\n\n[Unit]\nAfter=var-www.mount\nRequires=var-www.mount\n\n\n\nIf the mount fails, dependent units will not start.\n\n\nFurther reading\n\u00b6\n\n\nCheck the \nsystemd mount\n docs\n to learn about the available options. Examples specific to \nEC2\n, \nGoogle Compute Engine\n can be used as a starting point.",
            "title": "Mounting storage"
        },
        {
            "location": "/os/mounting-storage/#mounting-storage",
            "text": "Container Linux Configs can be used to format and attach additional filesystems to Flatcar Container Linux nodes, whether such storage is provided by an underlying cloud platform, physical disk, SAN, or NAS system. This is done by specifying how partitions should be mounted in the config, and then using a  systemd mount unit  to mount the partition. By  systemd convention , mount unit names derive from the target mount point, with interior slashes replaced by dashes, and the  .mount  extension appended. A unit mounting onto  /var/www  is thus named  var-www.mount .  Mount units name the source filesystem and target mount point, and optionally the filesystem type.  Systemd  mounts filesystems defined in such units at boot time. The following example formats an  EC2 ephemeral disk  and then mounts it at the node's  /media/ephemeral  directory. The mount unit is therefore named  media-ephemeral.mount .  storage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target",
            "title": "Mounting storage"
        },
        {
            "location": "/os/mounting-storage/#use-attached-storage-for-docker",
            "text": "Docker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Be aware that some cloud providers treat certain disks as ephemeral and you will lose all Docker images contained on that disk.  We're going to format a device as ext4 and then mount it to  /var/lib/docker , where Docker stores images. Be sure to hardcode the correct device or look for a device by label:  storage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Description=Mount ephemeral to /var/lib/docker\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/var/lib/docker\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target\n    - name: docker.service\n      dropins:\n        - name: 10-wait-docker.conf\n          contents: |\n            [Unit]\n            After=var-lib-docker.mount\n            Requires=var-lib-docker.mount",
            "title": "Use attached storage for Docker"
        },
        {
            "location": "/os/mounting-storage/#creating-and-mounting-a-btrfs-volume-file",
            "text": "Flatcar Container Linux uses ext4 + overlayfs to provide a layered filesystem for the root partition. If you'd like to use btrfs for your Docker containers, you can do so with two systemd units: one that creates and formats a btrfs volume file and another that mounts it.  In this example, we are going to mount a new 25GB btrfs volume file to  /var/lib/docker . One can verify that Docker is using the btrfs storage driver once the Docker service has started by executing  sudo docker info . We recommend allocating  no more than 85%  of the available disk space for a btrfs filesystem as journald will also require space on the host filesystem.  systemd:\n  units:\n    - name: format-var-lib-docker.service\n      contents: |\n        [Unit]\n        Before=docker.service var-lib-docker.mount\n        RequiresMountsFor=/var/lib\n        ConditionPathExists=!/var/lib/docker.btrfs\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/truncate --size=25G /var/lib/docker.btrfs\n        ExecStart=/usr/sbin/mkfs.btrfs /var/lib/docker.btrfs\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=docker.service\n        After=format-var-lib-docker.service\n        Requires=format-var-lib-docker.service\n        [Mount]\n        What=/var/lib/docker.btrfs\n        Where=/var/lib/docker\n        Type=btrfs\n        Options=loop,discard\n        [Install]\n        RequiredBy=docker.service  Note the declaration of  ConditionPathExists=!/var/lib/docker.btrfs . Without this line, systemd would reformat the btrfs filesystem every time the machine starts.",
            "title": "Creating and mounting a btrfs volume file"
        },
        {
            "location": "/os/mounting-storage/#mounting-nfs-exports",
            "text": "This Container Linux Config excerpt mounts an NFS export onto the Flatcar Container Linux node's  /var/www .  systemd:\n  units:\n    - name: var-www.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=remote-fs.target\n        [Mount]\n        What=nfs.example.com:/var/www\n        Where=/var/www\n        Type=nfs\n        [Install]\n        WantedBy=remote-fs.target  To declare that another service depends on this mount, name the mount unit in the dependent unit's  After  and  Requires  properties:  [Unit]\nAfter=var-www.mount\nRequires=var-www.mount  If the mount fails, dependent units will not start.",
            "title": "Mounting NFS exports"
        },
        {
            "location": "/os/mounting-storage/#further-reading",
            "text": "Check the  systemd mount  docs  to learn about the available options. Examples specific to  EC2 ,  Google Compute Engine  can be used as a starting point.",
            "title": "Further reading"
        },
        {
            "location": "/os/power-management/",
            "text": "Tuning Flatcar Container Linux power management\n\u00b6\n\n\nCPU governor\n\u00b6\n\n\nBy default, Flatcar Container Linux uses the \"performance\" CPU governor meaning that the CPU operates at the maximum frequency regardless of load. This is reasonable for a system that is under constant load or cannot tolerate increased latency. On the other hand, if the system is idle much of the time and latency is not a concern, power savings may be desired.\n\n\nSeveral governors are available:\n\n\n\n\n\n\n\n\nGovernor\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nperformance\n\n\nDefault. Operate at the maximum frequency\n\n\n\n\n\n\nondemand\n\n\nDynamically scale frequency at 75% cpu load\n\n\n\n\n\n\nconservative\n\n\nDynamically scale frequency at 95% cpu load\n\n\n\n\n\n\npowersave\n\n\nOperate at the minimum frequency\n\n\n\n\n\n\nuserspace\n\n\nControlled by a userspace application via the \nscaling_setspeed\n file\n\n\n\n\n\n\n\n\nThe \"conservative\" governor can be used instead using the following shell commands:\n\n\nmodprobe cpufreq_conservative\necho \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor > /dev/null\n\n\n\nThis can be configured with a \nContainer Linux Config\n as well:\n\n\nsystemd:\n  units:\n    - name: cpu-governor.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable CPU power saving\n\n        [Service]\n        Type=oneshot\n        RemainAfterExit=yes\n        ExecStart=/usr/sbin/modprobe cpufreq_conservative\n        ExecStart=/usr/bin/sh -c '/usr/bin/echo \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor'\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nMore information on further tuning each governor is available in the \nKernel Documentation",
            "title": "Power management"
        },
        {
            "location": "/os/power-management/#tuning-flatcar-container-linux-power-management",
            "text": "",
            "title": "Tuning Flatcar Container Linux power management"
        },
        {
            "location": "/os/power-management/#cpu-governor",
            "text": "By default, Flatcar Container Linux uses the \"performance\" CPU governor meaning that the CPU operates at the maximum frequency regardless of load. This is reasonable for a system that is under constant load or cannot tolerate increased latency. On the other hand, if the system is idle much of the time and latency is not a concern, power savings may be desired.  Several governors are available:     Governor  Description      performance  Default. Operate at the maximum frequency    ondemand  Dynamically scale frequency at 75% cpu load    conservative  Dynamically scale frequency at 95% cpu load    powersave  Operate at the minimum frequency    userspace  Controlled by a userspace application via the  scaling_setspeed  file     The \"conservative\" governor can be used instead using the following shell commands:  modprobe cpufreq_conservative\necho \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor > /dev/null  This can be configured with a  Container Linux Config  as well:  systemd:\n  units:\n    - name: cpu-governor.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable CPU power saving\n\n        [Service]\n        Type=oneshot\n        RemainAfterExit=yes\n        ExecStart=/usr/sbin/modprobe cpufreq_conservative\n        ExecStart=/usr/bin/sh -c '/usr/bin/echo \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor'\n\n        [Install]\n        WantedBy=multi-user.target  More information on further tuning each governor is available in the  Kernel Documentation",
            "title": "CPU governor"
        },
        {
            "location": "/os/registry-authentication/",
            "text": "Using authentication for a registry\n\u00b6\n\n\nMany container image registries require authentication. This document explains how to configure container management software like Docker, Kubernetes, rkt, and Mesos to authenticate with and pull containers from registries like \nQuay\n and \nDocker Hub\n.\n\n\nUsing a Quay robot for registry auth\n\u00b6\n\n\nThe recommended way to authenticate container manager software with \nquay.io\n is via a \nQuay Robot\n. The robot account acts as an authentication token with some nice features, including:\n\n\n\n\nReadymade repository authentication configuration files\n\n\nCredentials are limited to specific repositories\n\n\nChoose from read, write, or admin privileges\n\n\nToken regeneration\n\n\n\n\n\n\nQuay robots provide config files for Kubernetes, Docker, Mesos, and rkt, along with instructions for using each. Find this information in the \nRobot Accounts\n tab under your Quay user settings. For more information, see the \nQuay robot documentation\n.\n\n\nManual registry auth setup\n\u00b6\n\n\nIf you are using a registry other than Quay (e.g., Docker Hub, Docker Store, etc) you will need to manually configure your credentials with your container-runtime or orchestration tool.\n\n\nDocker\n\u00b6\n\n\nThe Docker client uses an interactive command to authenticate with a centralized service.\n\n\n$ docker login -u <username> -p <password> https://registry.example.io\n\n\n\nThis command creates the file \n$HOME/.docker/config.json\n, formatted like the following example:\n\n\n/home/core/.docker/config.json:\n\n\n{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        },\n        \"quay.io\": {\n            \"xxxx\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n        },\n        \"https://registry.example.io/v0/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        }\n    }\n}\n\n\n\nOn Flatcar Container Linux, this process can be automated by writing out the config file during system provisioning \nwith a Container Linux Config\n. Since the config is written to the \ncore\n user's home directory, ensure that your systemd units run as that user, by adding, e.g., \nUser=core\n.\n\n\nDocker also offers the ability to configure a credentials store, such as your operating system's keychain. This is outlined  in the \nDocker login documentation\n.\n\n\nKubernetes\n\u00b6\n\n\nKubernetes uses \nSecrets\n to store registry credentials.\n\n\nWhen manually configuring authentication with \nany\n registry in Kubernetes (including Quay and Docker Hub) the following command is used to generate the Kubernetes registry-auth secret:\n\n\n$ kubectl create secret docker-registry my-favorite-registry-secret --docker-username=giffee_lover_93 --docker-password='passphrases are great!' --docker-email='giffee.lover.93@example.com' --docker-server=registry.example.io\nsecret \"my-favorite-registry-secret\" created\n\n\n\nIf you prefer you can store this in a YAML file by adding the \n--dry-run\n and \n-o yaml\n flag to the end of your command and copying or redirecting the output to a file:\n\n\n$ kubectl create secret docker-registry my-favorite-registry [...] --dry-run -o yaml | tee credentials.yaml\n\n\n\napiVersion: v1\ndata:\n  .dockercfg: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx==\nkind: Secret\nmetadata:\n  creationTimestamp: null \n  name: my-favorite-registry-secret\ntype: kubernetes.io/dockercfg\n\n\n\n$ kubectl create -f credentials.yaml\nsecret \"my-favorite-registry-secret\" created\n\n\n\nYou can check that this secret is loaded with with the \nkubectl get\n command:\n\n\n$ kubectl get my-favorite-registry-secret\nNAME                            TYPE                      DATA      AGE\nmy-favorite-registry-secret     kubernetes.io/dockercfg   1         30m\n\n\n\nThe secret can be used in a Pod spec with the \nimagePullSecrets\n variable:\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: somepod\n  namespace: all\nspec:\n  containers:\n    - name: web\n      image: registry.example.io/v0/giffee_lover_93/somerepo\n\n  imagePullSecrets:\n    - name: my-favorite-registry-secret\n\n\n\nFor more information, check the \ndocker-registry Kubernetes secret\n and \nKubernetes imagePullSecrets\n documentation.\n\n\nrkt\n\u00b6\n\n\nrkt stores registry-authentication in a JSON file stored in the directory \n/etc/rkt/auth.d/\n. \n\n\n/etc/rkt/auth.d/registry.example.io.json\n\n\n{\n  \"rktKind\": \"auth\",\n  \"rktVersion\": \"v1\",\n  \"domains\": [\n    \"https://registry.example.io/v0/\"\n  ],\n  \"type\": \"basic\",\n  \"credentials\": {\n    \"user\": \"giffeeLover93\",\n    \"password\": \"passphrases are great!\"\n  }\n}\n\n\n\nWhile you \ncan\n embed your password in plaintext in this file, you should try using a disposable token instead. Check your registry documentation to see if it offers token-based authentication.\n\n\nNow rkt will authenticate with \nhttps://registry.example.io/v0/\n using the provided credentials to fetch images.\n\n\nFor more information about rkt credentials, see the \nrkt configuration docs\n.\n\n\nJust like with the Docker config, this file can be copied to \n/etc/rkt/auth.d/registry.example.io.json\n on a Flatcar Container Linux node during system provisioning with \na Container Linux Config\n.\n\n\nMesos\n\u00b6\n\n\nMesos uses a gzip-compressed archive of a \n.docker/config.json\n (directory and file) to access private repositories.\n\n\nOnce you have followed the above steps to \ncreate the docker registry auth config file\n create your Mesos configuration using \ntar\n:\n\n\n$ tar cxf ~/.docker/config.json\n\n\n\nThe archive secret is referenced via the \nuris\n field in a container specification file:\n\n\n{\n  \"id\": \"/some/name/or/id\",\n  \"cpus\": 1,\n  \"mem\": 1024,\n  \"instances\": 1,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"https://registry.example.io/v0/giffee_lover_93/some-image\",\n      \"network\": \"HOST\"\n    }\n  },\n\n  \"uris\":  [\n      \"file:///path/to/registry.example.io.tar.gz\"\n  ]\n}\n\n\n\nMore thorough information about configuring Mesos registry authentication can be found on the \n'Using a Private Docker Registry'\n documentation.\n\n\nCopying the config file with a Container Linux Config\n\u00b6\n\n\nContainer Linux Configs\n can be used to provision a Flatcar Container Linux node on first boot. Here we will use it to copy registry authentication config files to their appropriate destination on disk. This provides immediate access to your private Docker Hub and Quay image repositories without the need for manual intervention. The same Container Linux Config file can be used to copy registry auth configs onto an entire cluster of Flatcar Container Linux nodes.\n\n\nHere is an example of using a Container Linux Config to write the .docker/config.json registry auth configuration file mentioned above to the appropriate path on the Flatcar Container Linux node:\n\n\nstorage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          {\n            \"auths\": {\n              \"quay.io\": {\n                \"auth\": \"AbCdEfGhIj\",\n                \"email\": \"your.email@example.com\"\n              }\n            }\n          }\n\n\n\nContainer Linux Configs can also download a file from a remote location and verify its integrity with a SHA512 hash:\n\n\nstorage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        remote:\n          url: http://internal.infra.example.com/cluster-docker-config.json\n          verification:\n            hash:\n              function: sha512\n              sum: 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef\n\n\n\nFor details, check out the \nContainer Linux Config examples\n.",
            "title": "Registry authentication"
        },
        {
            "location": "/os/registry-authentication/#using-authentication-for-a-registry",
            "text": "Many container image registries require authentication. This document explains how to configure container management software like Docker, Kubernetes, rkt, and Mesos to authenticate with and pull containers from registries like  Quay  and  Docker Hub .",
            "title": "Using authentication for a registry"
        },
        {
            "location": "/os/registry-authentication/#using-a-quay-robot-for-registry-auth",
            "text": "The recommended way to authenticate container manager software with  quay.io  is via a  Quay Robot . The robot account acts as an authentication token with some nice features, including:   Readymade repository authentication configuration files  Credentials are limited to specific repositories  Choose from read, write, or admin privileges  Token regeneration    Quay robots provide config files for Kubernetes, Docker, Mesos, and rkt, along with instructions for using each. Find this information in the  Robot Accounts  tab under your Quay user settings. For more information, see the  Quay robot documentation .",
            "title": "Using a Quay robot for registry auth"
        },
        {
            "location": "/os/registry-authentication/#manual-registry-auth-setup",
            "text": "If you are using a registry other than Quay (e.g., Docker Hub, Docker Store, etc) you will need to manually configure your credentials with your container-runtime or orchestration tool.",
            "title": "Manual registry auth setup"
        },
        {
            "location": "/os/registry-authentication/#docker",
            "text": "The Docker client uses an interactive command to authenticate with a centralized service.  $ docker login -u <username> -p <password> https://registry.example.io  This command creates the file  $HOME/.docker/config.json , formatted like the following example:  /home/core/.docker/config.json:  {\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        },\n        \"quay.io\": {\n            \"xxxx\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n        },\n        \"https://registry.example.io/v0/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        }\n    }\n}  On Flatcar Container Linux, this process can be automated by writing out the config file during system provisioning  with a Container Linux Config . Since the config is written to the  core  user's home directory, ensure that your systemd units run as that user, by adding, e.g.,  User=core .  Docker also offers the ability to configure a credentials store, such as your operating system's keychain. This is outlined  in the  Docker login documentation .",
            "title": "Docker"
        },
        {
            "location": "/os/registry-authentication/#kubernetes",
            "text": "Kubernetes uses  Secrets  to store registry credentials.  When manually configuring authentication with  any  registry in Kubernetes (including Quay and Docker Hub) the following command is used to generate the Kubernetes registry-auth secret:  $ kubectl create secret docker-registry my-favorite-registry-secret --docker-username=giffee_lover_93 --docker-password='passphrases are great!' --docker-email='giffee.lover.93@example.com' --docker-server=registry.example.io\nsecret \"my-favorite-registry-secret\" created  If you prefer you can store this in a YAML file by adding the  --dry-run  and  -o yaml  flag to the end of your command and copying or redirecting the output to a file:  $ kubectl create secret docker-registry my-favorite-registry [...] --dry-run -o yaml | tee credentials.yaml  apiVersion: v1\ndata:\n  .dockercfg: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx==\nkind: Secret\nmetadata:\n  creationTimestamp: null \n  name: my-favorite-registry-secret\ntype: kubernetes.io/dockercfg  $ kubectl create -f credentials.yaml\nsecret \"my-favorite-registry-secret\" created  You can check that this secret is loaded with with the  kubectl get  command:  $ kubectl get my-favorite-registry-secret\nNAME                            TYPE                      DATA      AGE\nmy-favorite-registry-secret     kubernetes.io/dockercfg   1         30m  The secret can be used in a Pod spec with the  imagePullSecrets  variable:  apiVersion: v1\nkind: Pod\nmetadata:\n  name: somepod\n  namespace: all\nspec:\n  containers:\n    - name: web\n      image: registry.example.io/v0/giffee_lover_93/somerepo\n\n  imagePullSecrets:\n    - name: my-favorite-registry-secret  For more information, check the  docker-registry Kubernetes secret  and  Kubernetes imagePullSecrets  documentation.",
            "title": "Kubernetes"
        },
        {
            "location": "/os/registry-authentication/#rkt",
            "text": "rkt stores registry-authentication in a JSON file stored in the directory  /etc/rkt/auth.d/ .   /etc/rkt/auth.d/registry.example.io.json  {\n  \"rktKind\": \"auth\",\n  \"rktVersion\": \"v1\",\n  \"domains\": [\n    \"https://registry.example.io/v0/\"\n  ],\n  \"type\": \"basic\",\n  \"credentials\": {\n    \"user\": \"giffeeLover93\",\n    \"password\": \"passphrases are great!\"\n  }\n}  While you  can  embed your password in plaintext in this file, you should try using a disposable token instead. Check your registry documentation to see if it offers token-based authentication.  Now rkt will authenticate with  https://registry.example.io/v0/  using the provided credentials to fetch images.  For more information about rkt credentials, see the  rkt configuration docs .  Just like with the Docker config, this file can be copied to  /etc/rkt/auth.d/registry.example.io.json  on a Flatcar Container Linux node during system provisioning with  a Container Linux Config .",
            "title": "rkt"
        },
        {
            "location": "/os/registry-authentication/#mesos",
            "text": "Mesos uses a gzip-compressed archive of a  .docker/config.json  (directory and file) to access private repositories.  Once you have followed the above steps to  create the docker registry auth config file  create your Mesos configuration using  tar :  $ tar cxf ~/.docker/config.json  The archive secret is referenced via the  uris  field in a container specification file:  {\n  \"id\": \"/some/name/or/id\",\n  \"cpus\": 1,\n  \"mem\": 1024,\n  \"instances\": 1,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"https://registry.example.io/v0/giffee_lover_93/some-image\",\n      \"network\": \"HOST\"\n    }\n  },\n\n  \"uris\":  [\n      \"file:///path/to/registry.example.io.tar.gz\"\n  ]\n}  More thorough information about configuring Mesos registry authentication can be found on the  'Using a Private Docker Registry'  documentation.",
            "title": "Mesos"
        },
        {
            "location": "/os/registry-authentication/#copying-the-config-file-with-a-container-linux-config",
            "text": "Container Linux Configs  can be used to provision a Flatcar Container Linux node on first boot. Here we will use it to copy registry authentication config files to their appropriate destination on disk. This provides immediate access to your private Docker Hub and Quay image repositories without the need for manual intervention. The same Container Linux Config file can be used to copy registry auth configs onto an entire cluster of Flatcar Container Linux nodes.  Here is an example of using a Container Linux Config to write the .docker/config.json registry auth configuration file mentioned above to the appropriate path on the Flatcar Container Linux node:  storage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          {\n            \"auths\": {\n              \"quay.io\": {\n                \"auth\": \"AbCdEfGhIj\",\n                \"email\": \"your.email@example.com\"\n              }\n            }\n          }  Container Linux Configs can also download a file from a remote location and verify its integrity with a SHA512 hash:  storage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        remote:\n          url: http://internal.infra.example.com/cluster-docker-config.json\n          verification:\n            hash:\n              function: sha512\n              sum: 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef  For details, check out the  Container Linux Config examples .",
            "title": "Copying the config file with a Container Linux Config"
        },
        {
            "location": "/os/iscsi/",
            "text": "iSCSI on Flatcar Container Linux\n\u00b6\n\n\niSCSI\n is a protocol which provides block-level access to storage devices over IP.\nThis allows applications to treat remote storage devices as if they were local disks.\niSCSI handles taking requests from clients and carrying them out on the remote SCSI devices.\n\n\nFlatcar Container Linux has integrated support for mounting devices.\nThis guide covers iSCSI configuration manually or automatically with \nContainer Linux Configs\n.\n\n\nManual iSCSI configuration\n\u00b6\n\n\nSet the Flatcar Container Linux iSCSI initiator name\n\u00b6\n\n\niSCSI clients each have a unique initiator name.\nFlatcar Container Linux generates a unique initiator name on each install and stores it in \n/etc/iscsi/initiatorname.iscsi\n.\nThis may be replaced if necessary.\n\n\nConfigure the global iSCSI credentials\n\u00b6\n\n\nIf all iSCSI mounts on a Flatcar Container Linux system use the same credentials, these may be configured locally by editing \n/etc/iscsi/iscsid.conf\n and setting the \nnode.session.auth.username\n and \nnode.session.auth.password\n fields.\nIf the iSCSI target is configured to support mutual authentication (allowing the initiator to verify that it is speaking to the correct client), these should be set in \nnode.session.auth.username_in\n and \nnode.session.auth.password_in\n.\n\n\nStart the iSCSI daemon\n\u00b6\n\n\nsystemctl start iscsid\n\n\n\nDiscover available iSCSI targets\n\u00b6\n\n\nTo discover targets, run:\n\n\n$ iscsiadm -m discovery -t sendtargets -p target_ip:target_port\n\n\n\nProvide target-specific credentials\n\u00b6\n\n\nFor each unique \n--targetname\n, first enter the username:\n\n\n$ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.username \\\n  --value=my_username\n\n\n\nAnd then the password:\n\n\n$ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.password \\\n  --value=my_secret_passphrase\n\n\n\nLog into an iSCSI target\n\u00b6\n\n\nThe following command will log into all discovered targets.\n\n\n$ iscsiadm -m node --login\n\n\n\nThen, to log into a specific target use:\n\n\n$ iscsiadm -m node --targetname=custom_target --login\n\n\n\nEnable automatic iSCSI login at boot\n\u00b6\n\n\nIf you want to connect to iSCSI targets automatically at boot you first need to enable the systemd service:\n\n\n$ systemctl enable iscsid\n\n\n\nAutomatic iSCSI configuration\n\u00b6\n\n\nTo configure and start iSCSI automatically after a machine is provisioned, credentials need to be written to disk and the iSCSI service started.\n\n\nA Container Linux Config will be used to write the file \n/etc/iscsi/iscsid.conf\n to disk:\n\n\n/etc/iscsi/iscsid.conf\n\u00b6\n\n\n\n\n\nisns.address = host_ip\nisns.port = host_port\nnode.session.auth.username = my_username\nnode.session.auth.password = my_secret_password\ndiscovery.sendtargets.auth.username = my_username\ndiscovery.sendtargets.auth.password = my_secret_password\n\n\n\nThe Container Linux Config\n\u00b6\n\n\nsystemd:\n  units:\n    - name: iscsid.service\n      enable: true\nstorage:\n  files:\n    - path: /etc/iscsi/iscsid.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          isns.address = host_ip\n          isns.port = host_port\n          node.session.auth.username = my_username\n          node.session.auth.password = my_secret_password\n          discovery.sendtargets.auth.username = my_username\n          discovery.sendtargets.auth.password = my_secret_password\n\n\n\nMounting iSCSI targets\n\u00b6\n\n\nSee the \nmounting storage docs\n for an example.",
            "title": "iSCSI configuration"
        },
        {
            "location": "/os/iscsi/#iscsi-on-flatcar-container-linux",
            "text": "iSCSI  is a protocol which provides block-level access to storage devices over IP.\nThis allows applications to treat remote storage devices as if they were local disks.\niSCSI handles taking requests from clients and carrying them out on the remote SCSI devices.  Flatcar Container Linux has integrated support for mounting devices.\nThis guide covers iSCSI configuration manually or automatically with  Container Linux Configs .",
            "title": "iSCSI on Flatcar Container Linux"
        },
        {
            "location": "/os/iscsi/#manual-iscsi-configuration",
            "text": "",
            "title": "Manual iSCSI configuration"
        },
        {
            "location": "/os/iscsi/#set-the-flatcar-container-linux-iscsi-initiator-name",
            "text": "iSCSI clients each have a unique initiator name.\nFlatcar Container Linux generates a unique initiator name on each install and stores it in  /etc/iscsi/initiatorname.iscsi .\nThis may be replaced if necessary.",
            "title": "Set the Flatcar Container Linux iSCSI initiator name"
        },
        {
            "location": "/os/iscsi/#configure-the-global-iscsi-credentials",
            "text": "If all iSCSI mounts on a Flatcar Container Linux system use the same credentials, these may be configured locally by editing  /etc/iscsi/iscsid.conf  and setting the  node.session.auth.username  and  node.session.auth.password  fields.\nIf the iSCSI target is configured to support mutual authentication (allowing the initiator to verify that it is speaking to the correct client), these should be set in  node.session.auth.username_in  and  node.session.auth.password_in .",
            "title": "Configure the global iSCSI credentials"
        },
        {
            "location": "/os/iscsi/#start-the-iscsi-daemon",
            "text": "systemctl start iscsid",
            "title": "Start the iSCSI daemon"
        },
        {
            "location": "/os/iscsi/#discover-available-iscsi-targets",
            "text": "To discover targets, run:  $ iscsiadm -m discovery -t sendtargets -p target_ip:target_port",
            "title": "Discover available iSCSI targets"
        },
        {
            "location": "/os/iscsi/#provide-target-specific-credentials",
            "text": "For each unique  --targetname , first enter the username:  $ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.username \\\n  --value=my_username  And then the password:  $ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.password \\\n  --value=my_secret_passphrase",
            "title": "Provide target-specific credentials"
        },
        {
            "location": "/os/iscsi/#log-into-an-iscsi-target",
            "text": "The following command will log into all discovered targets.  $ iscsiadm -m node --login  Then, to log into a specific target use:  $ iscsiadm -m node --targetname=custom_target --login",
            "title": "Log into an iSCSI target"
        },
        {
            "location": "/os/iscsi/#enable-automatic-iscsi-login-at-boot",
            "text": "If you want to connect to iSCSI targets automatically at boot you first need to enable the systemd service:  $ systemctl enable iscsid",
            "title": "Enable automatic iSCSI login at boot"
        },
        {
            "location": "/os/iscsi/#automatic-iscsi-configuration",
            "text": "To configure and start iSCSI automatically after a machine is provisioned, credentials need to be written to disk and the iSCSI service started.  A Container Linux Config will be used to write the file  /etc/iscsi/iscsid.conf  to disk:",
            "title": "Automatic iSCSI configuration"
        },
        {
            "location": "/os/iscsi/#etciscsiiscsidconf",
            "text": "isns.address = host_ip\nisns.port = host_port\nnode.session.auth.username = my_username\nnode.session.auth.password = my_secret_password\ndiscovery.sendtargets.auth.username = my_username\ndiscovery.sendtargets.auth.password = my_secret_password",
            "title": "/etc/iscsi/iscsid.conf"
        },
        {
            "location": "/os/iscsi/#the-container-linux-config",
            "text": "systemd:\n  units:\n    - name: iscsid.service\n      enable: true\nstorage:\n  files:\n    - path: /etc/iscsi/iscsid.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          isns.address = host_ip\n          isns.port = host_port\n          node.session.auth.username = my_username\n          node.session.auth.password = my_secret_password\n          discovery.sendtargets.auth.username = my_username\n          discovery.sendtargets.auth.password = my_secret_password",
            "title": "The Container Linux Config"
        },
        {
            "location": "/os/iscsi/#mounting-iscsi-targets",
            "text": "See the  mounting storage docs  for an example.",
            "title": "Mounting iSCSI targets"
        },
        {
            "location": "/os/adding-swap/",
            "text": "Adding swap in Flatcar Container Linux\n\u00b6\n\n\nSwap is the process of moving pages of memory to a designated part of the hard disk, freeing up space when needed. Swap can be used to alleviate problems with low-memory environments.\n\n\nBy default Flatcar Container Linux does not include a partition for swap, however one can configure their system to have swap, either by including a dedicated partition for it or creating a swapfile.\n\n\nManaging swap with systemd\n\u00b6\n\n\nsystemd provides a specialized \n.swap\n unit file type which may be used to activate swap. The below example shows how to add a swapfile and activate it using systemd.\n\n\nCreating a swapfile\n\u00b6\n\n\nThe following commands, run as root, will make a 1GiB file suitable for use as swap.\n\n\nmkdir -p /var/vm\nfallocate -l 1024m /var/vm/swapfile1\nchmod 600 /var/vm/swapfile1\nmkswap /var/vm/swapfile1\n\n\n\nCreating the systemd unit file\n\u00b6\n\n\nThe following systemd unit activates the swapfile we created. It should be written to \n/etc/systemd/system/var-vm-swapfile1.swap\n.\n\n\n[Unit]\nDescription=Turn on swap\n\n[Swap]\nWhat=/var/vm/swapfile1\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nEnable the unit and start using swap\n\u00b6\n\n\nUse \nsystemctl\n to enable the unit once created. The \nswappiness\n value may be modified if desired.\n\n\n$ systemctl enable --now var-vm-swapfile1.swap\n# Optionally\n$ echo 'vm.swappiness=10' | sudo tee /etc/sysctl.d/80-swappiness.conf\n$ systemctl restart systemd-sysctl\n\n\n\nSwap has been enabled and will be started automatically on subsequent reboots. We can verify that the swap is activated by running \nswapon\n:\n\n\n$ swapon\nNAME              TYPE       SIZE USED PRIO\n/var/vm/swapfile1 file      1024M   0B   -1\n\n\n\nProblems and Considerations\n\u00b6\n\n\nBtrfs and xfs\n\u00b6\n\n\nSwapfiles should not be created on btrfs or xfs volumes. For systems using btrfs or xfs, it is recommended to create a dedicated swap partition.\n\n\nPartition size\n\u00b6\n\n\nThe swapfile cannot be larger than the partition on which it is stored.\n\n\nChecking if a system can use a swapfile\n\u00b6\n\n\nUse the \ndf(1)\n command to verify that a partition has the right format and enough available space:\n\n\n$ df -Th\nFilesystem     Type      Size  Used Avail Use% Mounted on\n[...]\n/dev/sdXN      ext4      2.0G  3.0M  1.8G   1% /var\n\n\n\nThe block device mounted at \n/var/\n, \n/dev/sdXN\n, is the correct filesystem type and has enough space for a 1GiB swapfile.\n\n\nAdding swap with a Container Linux Config\n\u00b6\n\n\nThe following config sets up a 1GiB swapfile located at \n/var/vm/swapfile1\n.\n\n\nstorage:\n  files:\n  - path: /etc/sysctl.d/80-swappiness.conf\n    filesystem: root\n    contents:\n      inline: \"vm.swappiness=10\"\n\nsystemd:\n  units:\n    - name: var-vm-swapfile1.swap\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Turn on swap\n        Requires=create-swapfile.service\n        After=create-swapfile.service\n\n        [Swap]\n        What=/var/vm/swapfile1\n\n        [Install]\n        WantedBy=multi-user.target\n    - name: create-swapfile.service\n      contents: |\n        [Unit]\n        Description=Create a swapfile\n        RequiresMountsFor=/var\n        ConditionPathExists=!/var/vm/swapfile1\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/mkdir -p /var/vm\n        ExecStart=/usr/bin/fallocate -l 1024m /var/vm/swapfile1\n        ExecStart=/usr/bin/chmod 600 /var/vm/swapfile1\n        ExecStart=/usr/sbin/mkswap /var/vm/swapfile1\n        RemainAfterExit=true",
            "title": "Adding swap"
        },
        {
            "location": "/os/adding-swap/#adding-swap-in-flatcar-container-linux",
            "text": "Swap is the process of moving pages of memory to a designated part of the hard disk, freeing up space when needed. Swap can be used to alleviate problems with low-memory environments.  By default Flatcar Container Linux does not include a partition for swap, however one can configure their system to have swap, either by including a dedicated partition for it or creating a swapfile.",
            "title": "Adding swap in Flatcar Container Linux"
        },
        {
            "location": "/os/adding-swap/#managing-swap-with-systemd",
            "text": "systemd provides a specialized  .swap  unit file type which may be used to activate swap. The below example shows how to add a swapfile and activate it using systemd.",
            "title": "Managing swap with systemd"
        },
        {
            "location": "/os/adding-swap/#creating-a-swapfile",
            "text": "The following commands, run as root, will make a 1GiB file suitable for use as swap.  mkdir -p /var/vm\nfallocate -l 1024m /var/vm/swapfile1\nchmod 600 /var/vm/swapfile1\nmkswap /var/vm/swapfile1",
            "title": "Creating a swapfile"
        },
        {
            "location": "/os/adding-swap/#creating-the-systemd-unit-file",
            "text": "The following systemd unit activates the swapfile we created. It should be written to  /etc/systemd/system/var-vm-swapfile1.swap .  [Unit]\nDescription=Turn on swap\n\n[Swap]\nWhat=/var/vm/swapfile1\n\n[Install]\nWantedBy=multi-user.target",
            "title": "Creating the systemd unit file"
        },
        {
            "location": "/os/adding-swap/#enable-the-unit-and-start-using-swap",
            "text": "Use  systemctl  to enable the unit once created. The  swappiness  value may be modified if desired.  $ systemctl enable --now var-vm-swapfile1.swap\n# Optionally\n$ echo 'vm.swappiness=10' | sudo tee /etc/sysctl.d/80-swappiness.conf\n$ systemctl restart systemd-sysctl  Swap has been enabled and will be started automatically on subsequent reboots. We can verify that the swap is activated by running  swapon :  $ swapon\nNAME              TYPE       SIZE USED PRIO\n/var/vm/swapfile1 file      1024M   0B   -1",
            "title": "Enable the unit and start using swap"
        },
        {
            "location": "/os/adding-swap/#problems-and-considerations",
            "text": "",
            "title": "Problems and Considerations"
        },
        {
            "location": "/os/adding-swap/#btrfs-and-xfs",
            "text": "Swapfiles should not be created on btrfs or xfs volumes. For systems using btrfs or xfs, it is recommended to create a dedicated swap partition.",
            "title": "Btrfs and xfs"
        },
        {
            "location": "/os/adding-swap/#partition-size",
            "text": "The swapfile cannot be larger than the partition on which it is stored.",
            "title": "Partition size"
        },
        {
            "location": "/os/adding-swap/#checking-if-a-system-can-use-a-swapfile",
            "text": "Use the  df(1)  command to verify that a partition has the right format and enough available space:  $ df -Th\nFilesystem     Type      Size  Used Avail Use% Mounted on\n[...]\n/dev/sdXN      ext4      2.0G  3.0M  1.8G   1% /var  The block device mounted at  /var/ ,  /dev/sdXN , is the correct filesystem type and has enough space for a 1GiB swapfile.",
            "title": "Checking if a system can use a swapfile"
        },
        {
            "location": "/os/adding-swap/#adding-swap-with-a-container-linux-config",
            "text": "The following config sets up a 1GiB swapfile located at  /var/vm/swapfile1 .  storage:\n  files:\n  - path: /etc/sysctl.d/80-swappiness.conf\n    filesystem: root\n    contents:\n      inline: \"vm.swappiness=10\"\n\nsystemd:\n  units:\n    - name: var-vm-swapfile1.swap\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Turn on swap\n        Requires=create-swapfile.service\n        After=create-swapfile.service\n\n        [Swap]\n        What=/var/vm/swapfile1\n\n        [Install]\n        WantedBy=multi-user.target\n    - name: create-swapfile.service\n      contents: |\n        [Unit]\n        Description=Create a swapfile\n        RequiresMountsFor=/var\n        ConditionPathExists=!/var/vm/swapfile1\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/mkdir -p /var/vm\n        ExecStart=/usr/bin/fallocate -l 1024m /var/vm/swapfile1\n        ExecStart=/usr/bin/chmod 600 /var/vm/swapfile1\n        ExecStart=/usr/sbin/mkswap /var/vm/swapfile1\n        RemainAfterExit=true",
            "title": "Adding swap with a Container Linux Config"
        },
        {
            "location": "/os/booting-on-ecs/",
            "text": "Running Flatcar Container Linux with AWS EC2 Container Service\n\u00b6\n\n\nAmazon EC2 Container Service (ECS)\n is a container management service which provides a set of APIs for scheduling container workloads across EC2 clusters. It supports Flatcar Container Linux with Docker containers.\n\n\nYour Flatcar Container Linux machines communicate with ECS via an agent. The agent interacts with Docker to start new containers and gather information about running containers.\n\n\nSet up a new cluster\n\u00b6\n\n\nWhen booting your \nFlatcar Container Linux Machines on EC2\n, configure the ECS agent to be started via \nIgnition\n.\n\n\nBe sure to change \nECS_CLUSTER\n to the cluster name you've configured via the ECS CLI or leave it empty for the default. Here's a full config example:\n\n\nstorage:\n  files:\n    - path: /var/lib/iptables/rules-save\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          *nat\n          -A PREROUTING -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 127.0.0.1:51679\n          -A OUTPUT -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679\n          COMMIT\n    - path: /etc/sysctl.d/localnet.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.ipv4.conf.all.route_localnet=1\n\nsystemd:\n units:\n   - name: iptables-restore.service\n     enable: true\n   - name: systemd-sysctl.service\n     enable: true\n   - name: amazon-ecs-agent.service\n     enable: true\n     contents: |\n       [Unit]\n       Description=AWS ECS Agent\n       Documentation=https://docs.aws.amazon.com/AmazonECS/latest/developerguide/\n       Requires=docker.socket\n       After=docker.socket\n\n       [Service]\n       Environment=ECS_CLUSTER=your_cluster_name\n       Environment=ECS_LOGLEVEL=info\n       Environment=ECS_VERSION=latest\n       Restart=on-failure\n       RestartSec=30\n       RestartPreventExitStatus=5\n       SyslogIdentifier=ecs-agent\n       ExecStartPre=-/bin/mkdir -p /var/log/ecs /var/ecs-data /etc/ecs\n       ExecStartPre=-/usr/bin/touch /etc/ecs/ecs.config\n       ExecStartPre=-/usr/bin/docker kill ecs-agent\n       ExecStartPre=-/usr/bin/docker rm ecs-agent\n       ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent:${ECS_VERSION}\n       ExecStart=/usr/bin/docker run \\\n           --name ecs-agent \\\n           --env-file=/etc/ecs/ecs.config \\\n           --volume=/var/run/docker.sock:/var/run/docker.sock \\\n           --volume=/var/log/ecs:/log \\\n           --volume=/var/ecs-data:/data \\\n           --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro \\\n           --volume=/run/docker/execdriver/native:/var/lib/docker/execdriver/native:ro \\\n           --publish=127.0.0.1:51678:51678 \\\n           --publish=127.0.0.1:51679:51679 \\\n           --env=ECS_AVAILABLE_LOGGING_DRIVERS='[\"awslogs\",\"json-file\",\"journald\",\"logentries\",\"splunk\",\"syslog\"]'\n           --env=ECS_ENABLE_TASK_IAM_ROLE=true \\\n           --env=ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true \\\n           --env=ECS_LOGFILE=/log/ecs-agent.log \\\n           --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \\\n           --env=ECS_DATADIR=/data \\\n           --env=ECS_CLUSTER=${ECS_CLUSTER} \\\n           amazon/amazon-ecs-agent:${ECS_VERSION}\n\n       [Install]\n       WantedBy=multi-user.target\n\n\n\nThe example above pulls the latest official Amazon ECS agent container from the Docker Hub when the machine starts. If you ever need to update the agent, it\u2019s as simple as restarting the amazon-ecs-agent service or the Flatcar Container Linux machine.\n\n\nIf you want to configure SSH keys in order to log in, mount disks or configure other options, see the \nContainer Linux Configs documentation\n.\n\n\nFor more information on using ECS, check out the \nofficial Amazon documentation\n.\n\n\nFor more information on using ECS, check out the \nofficial Amazon documentation\n.",
            "title": "Amazon EC2 Container Service"
        },
        {
            "location": "/os/booting-on-ecs/#running-flatcar-container-linux-with-aws-ec2-container-service",
            "text": "Amazon EC2 Container Service (ECS)  is a container management service which provides a set of APIs for scheduling container workloads across EC2 clusters. It supports Flatcar Container Linux with Docker containers.  Your Flatcar Container Linux machines communicate with ECS via an agent. The agent interacts with Docker to start new containers and gather information about running containers.",
            "title": "Running Flatcar Container Linux with AWS EC2 Container Service"
        },
        {
            "location": "/os/booting-on-ecs/#set-up-a-new-cluster",
            "text": "When booting your  Flatcar Container Linux Machines on EC2 , configure the ECS agent to be started via  Ignition .  Be sure to change  ECS_CLUSTER  to the cluster name you've configured via the ECS CLI or leave it empty for the default. Here's a full config example:  storage:\n  files:\n    - path: /var/lib/iptables/rules-save\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          *nat\n          -A PREROUTING -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 127.0.0.1:51679\n          -A OUTPUT -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679\n          COMMIT\n    - path: /etc/sysctl.d/localnet.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.ipv4.conf.all.route_localnet=1\n\nsystemd:\n units:\n   - name: iptables-restore.service\n     enable: true\n   - name: systemd-sysctl.service\n     enable: true\n   - name: amazon-ecs-agent.service\n     enable: true\n     contents: |\n       [Unit]\n       Description=AWS ECS Agent\n       Documentation=https://docs.aws.amazon.com/AmazonECS/latest/developerguide/\n       Requires=docker.socket\n       After=docker.socket\n\n       [Service]\n       Environment=ECS_CLUSTER=your_cluster_name\n       Environment=ECS_LOGLEVEL=info\n       Environment=ECS_VERSION=latest\n       Restart=on-failure\n       RestartSec=30\n       RestartPreventExitStatus=5\n       SyslogIdentifier=ecs-agent\n       ExecStartPre=-/bin/mkdir -p /var/log/ecs /var/ecs-data /etc/ecs\n       ExecStartPre=-/usr/bin/touch /etc/ecs/ecs.config\n       ExecStartPre=-/usr/bin/docker kill ecs-agent\n       ExecStartPre=-/usr/bin/docker rm ecs-agent\n       ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent:${ECS_VERSION}\n       ExecStart=/usr/bin/docker run \\\n           --name ecs-agent \\\n           --env-file=/etc/ecs/ecs.config \\\n           --volume=/var/run/docker.sock:/var/run/docker.sock \\\n           --volume=/var/log/ecs:/log \\\n           --volume=/var/ecs-data:/data \\\n           --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro \\\n           --volume=/run/docker/execdriver/native:/var/lib/docker/execdriver/native:ro \\\n           --publish=127.0.0.1:51678:51678 \\\n           --publish=127.0.0.1:51679:51679 \\\n           --env=ECS_AVAILABLE_LOGGING_DRIVERS='[\"awslogs\",\"json-file\",\"journald\",\"logentries\",\"splunk\",\"syslog\"]'\n           --env=ECS_ENABLE_TASK_IAM_ROLE=true \\\n           --env=ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true \\\n           --env=ECS_LOGFILE=/log/ecs-agent.log \\\n           --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \\\n           --env=ECS_DATADIR=/data \\\n           --env=ECS_CLUSTER=${ECS_CLUSTER} \\\n           amazon/amazon-ecs-agent:${ECS_VERSION}\n\n       [Install]\n       WantedBy=multi-user.target  The example above pulls the latest official Amazon ECS agent container from the Docker Hub when the machine starts. If you ever need to update the agent, it\u2019s as simple as restarting the amazon-ecs-agent service or the Flatcar Container Linux machine.  If you want to configure SSH keys in order to log in, mount disks or configure other options, see the  Container Linux Configs documentation .  For more information on using ECS, check out the  official Amazon documentation .  For more information on using ECS, check out the  official Amazon documentation .",
            "title": "Set up a new cluster"
        },
        {
            "location": "/os/getting-started-with-systemd/",
            "text": "Getting started with systemd\n\u00b6\n\n\nsystemd is an init system that provides many powerful features for starting, stopping, and managing processes. Within Flatcar Container Linux, you will almost exclusively use systemd to manage the lifecycle of your Docker containers.\n\n\nTerminology\n\u00b6\n\n\nsystemd consists of two main concepts: a unit and a target. A unit is a configuration file that describes the properties of the process that you'd like to run. This is normally a \ndocker run\n command or something similar. A target is a grouping mechanism that allows systemd to start up groups of processes at the same time. This happens at every boot as processes are started at different run levels.\n\n\nsystemd is the first process started on Flatcar Container Linux and it reads different targets and starts the processes specified which allows the operating system to start. The target that you'll interact with is the \nmulti-user.target\n which holds all of the general use unit files for our containers.\n\n\nEach target is actually a collection of symlinks to our unit files. This is specified in the unit file by \nWantedBy=multi-user.target\n. Running \nsystemctl enable foo.service\n creates symlinks to the unit inside \nmulti-user.target.wants\n.\n\n\nUnit file\n\u00b6\n\n\nOn Flatcar Container Linux, unit files are located at \n/etc/systemd/system\n. Let's create a simple unit named \nhello.service\n:\n\n\n[Unit]\nDescription=MyApp\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill busybox1\nExecStartPre=-/usr/bin/docker rm busybox1\nExecStartPre=/usr/bin/docker pull busybox\nExecStart=/usr/bin/docker run --name busybox1 busybox /bin/sh -c \"trap 'exit 0' INT TERM; while true; do echo Hello World; sleep 1; done\"\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nThe \nDescription\n shows up in the systemd log and a few other places. Write something that will help you understand exactly what this does later on.\n\n\nAfter=docker.service\n and \nRequires=docker.service\n means this unit will only start after \ndocker.service\n is active. You can define as many of these as you want.\n\n\nExecStart=\n allows you to specify any command that you'd like to run when this unit is started. The pid assigned to this process is what systemd will monitor to determine whether the process has crashed or not. Do not run docker containers with \n-d\n as this will prevent the container from starting as a child of this pid. systemd will think the process has exited and the unit will be stopped.\n\n\nWantedBy=\n is the target that this unit is a part of.\n\n\nTo start a new unit, we need to tell systemd to create the symlink and then start the file:\n\n\n$ sudo systemctl enable /etc/systemd/system/hello.service\n$ sudo systemctl start hello.service\n\n\n\nTo verify the unit started, you can see the list of containers running with \ndocker ps\n and read the unit's output with \njournalctl\n:\n\n\n$ journalctl -f -u hello.service\n-- Logs begin at Fri 2014-02-07 00:05:55 UTC. --\nFeb 11 17:46:26 localhost docker[23470]: Hello World\nFeb 11 17:46:27 localhost docker[23470]: Hello World\nFeb 11 17:46:28 localhost docker[23470]: Hello World\n...\n\n\n\nOverview of systemctl\n\n\nReading the System Log\n\n\nAdvanced unit files\n\u00b6\n\n\nsystemd provides a high degree of functionality in your unit files. Here's a curated list of useful features listed in the order they'll occur in the lifecycle of a unit:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nExecStartPre\n\n\nCommands that will run before \nExecStart\n.\n\n\n\n\n\n\nExecStart\n\n\nMain commands to run for this unit.\n\n\n\n\n\n\nExecStartPost\n\n\nCommands that will run after all \nExecStart\n commands have completed.\n\n\n\n\n\n\nExecReload\n\n\nCommands that will run when this unit is reloaded via \nsystemctl reload foo.service\n\n\n\n\n\n\nExecStop\n\n\nCommands that will run when this unit is considered failed or if it is stopped via \nsystemctl stop foo.service\n\n\n\n\n\n\nExecStopPost\n\n\nCommands that will run after \nExecStop\n has completed.\n\n\n\n\n\n\nRestartSec\n\n\nThe amount of time to sleep before restarting a service. Useful to prevent your failed service from attempting to restart itself every 100ms.\n\n\n\n\n\n\n\n\nThe full list is located on the \nsystemd man page\n.\n\n\nLet's put a few of these concepts together to register new units within etcd. Imagine we had another container running that would read these values from etcd and act upon them.\n\n\nWe can use \nExecStartPre\n to scrub existing container state. The \ndocker kill\n will force any previous copy of this container to stop, which is useful if we restarted the unit but Docker didn't stop the container for some reason. The \n=-\n is systemd syntax to ignore errors for this command. We need to do this because Docker will return a non-zero exit code if we try to stop a container that doesn't exist. We don't consider this an error (because we want the container stopped) so we tell systemd to ignore the possible failure.\n\n\ndocker rm\n will remove the container and \ndocker pull\n will pull down the latest version. You can optionally pull down a specific version as a Docker tag: \ncoreos/apache:1.2.3\n\n\nExecStart\n is where the container is started from the container image that we pulled above.\n\n\nSince our container will be started in \nExecStart\n, it makes sense for our etcd command to run as \nExecStartPost\n to ensure that our container is started and functioning.\n\n\nWhen the service is told to stop, we need to stop the Docker container using its \n--name\n from the run command. We also need to clean up our etcd key when the container exits or the unit is failed by using \nExecStopPost\n.\n\n\n[Unit]\nDescription=My Advanced Service\nAfter=etcd2.service\nAfter=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill apache1\nExecStartPre=-/usr/bin/docker rm apache1\nExecStartPre=/usr/bin/docker pull coreos/apache\nExecStart=/usr/bin/docker run --name apache1 -p 8081:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running\nExecStop=/usr/bin/docker stop apache1\nExecStopPost=/usr/bin/etcdctl rm /domains/example.com/10.10.10.123:8081\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nWhile it's possible to manage the starting, stopping, and removal of the container in a single \nExecStart\n command by using \ndocker run --rm\n, it's a good idea to separate the container's lifecycle into \nExecStartPre\n, \nExecStart\n, and \nExecStop\n options as we've done above. This gives you a chance to inspect the container's state after it stops or fails.\n\n\nUnit specifiers\n\u00b6\n\n\nIn our last example we had to hardcode our IP address when we announced our container in etcd. That's not scalable and systemd has a few variables built in to help us out. Here's a few of the most useful:\n\n\n\n\n\n\n\n\nVariable\n\n\nMeaning\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%n\n\n\nFull unit name\n\n\nUseful if the name of your unit is unique enough to be used as an argument on a command.\n\n\n\n\n\n\n%m\n\n\nMachine ID\n\n\nUseful for namespacing etcd keys by machine. Example: \n/machines/%m/units\n\n\n\n\n\n\n%b\n\n\nBootID\n\n\nSimilar to the machine ID, but this value is random and changes on each boot\n\n\n\n\n\n\n%H\n\n\nHostname\n\n\nAllows you to run the same unit file across many machines. Useful for service discovery. Example: \n/domains/example.com/%H:8081\n\n\n\n\n\n\n\n\nA full list of specifiers can be found on the \nsystemd man page\n.\n\n\nInstantiated units\n\u00b6\n\n\nSince systemd is based on symlinks, there are a few interesting tricks you can leverage that are very powerful when used with containers. If you create multiple symlinks to the same unit file, the following variables become available to you:\n\n\n\n\n\n\n\n\nVariable\n\n\nMeaning\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%p\n\n\nPrefix name\n\n\nRefers to any string before \n@\n in your unit name.\n\n\n\n\n\n\n%i\n\n\nInstance name\n\n\nRefers to the string between the \n@\n and the suffix.\n\n\n\n\n\n\n\n\nIn our earlier example we had to hardcode our IP address when registering within etcd:\n\n\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running\n\n\n\nWe can enhance this by using \n%H\n and \n%i\n to dynamically announce the hostname and port. Specify the port after the \n@\n by using two unit files named \nfoo@123.service\n and \nfoo@456.service\n:\n\n\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running\n\n\n\nThis gives us the flexibility to use a single unit file to announce multiple copies of the same container on a single machine (no port overlap) and on multiple machines (no hostname overlap).\n\n\nMore information\n\u00b6\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs\n\n\nsystemd.target Docs",
            "title": "Using systemd to manage Docker containers"
        },
        {
            "location": "/os/getting-started-with-systemd/#getting-started-with-systemd",
            "text": "systemd is an init system that provides many powerful features for starting, stopping, and managing processes. Within Flatcar Container Linux, you will almost exclusively use systemd to manage the lifecycle of your Docker containers.",
            "title": "Getting started with systemd"
        },
        {
            "location": "/os/getting-started-with-systemd/#terminology",
            "text": "systemd consists of two main concepts: a unit and a target. A unit is a configuration file that describes the properties of the process that you'd like to run. This is normally a  docker run  command or something similar. A target is a grouping mechanism that allows systemd to start up groups of processes at the same time. This happens at every boot as processes are started at different run levels.  systemd is the first process started on Flatcar Container Linux and it reads different targets and starts the processes specified which allows the operating system to start. The target that you'll interact with is the  multi-user.target  which holds all of the general use unit files for our containers.  Each target is actually a collection of symlinks to our unit files. This is specified in the unit file by  WantedBy=multi-user.target . Running  systemctl enable foo.service  creates symlinks to the unit inside  multi-user.target.wants .",
            "title": "Terminology"
        },
        {
            "location": "/os/getting-started-with-systemd/#unit-file",
            "text": "On Flatcar Container Linux, unit files are located at  /etc/systemd/system . Let's create a simple unit named  hello.service :  [Unit]\nDescription=MyApp\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill busybox1\nExecStartPre=-/usr/bin/docker rm busybox1\nExecStartPre=/usr/bin/docker pull busybox\nExecStart=/usr/bin/docker run --name busybox1 busybox /bin/sh -c \"trap 'exit 0' INT TERM; while true; do echo Hello World; sleep 1; done\"\n\n[Install]\nWantedBy=multi-user.target  The  Description  shows up in the systemd log and a few other places. Write something that will help you understand exactly what this does later on.  After=docker.service  and  Requires=docker.service  means this unit will only start after  docker.service  is active. You can define as many of these as you want.  ExecStart=  allows you to specify any command that you'd like to run when this unit is started. The pid assigned to this process is what systemd will monitor to determine whether the process has crashed or not. Do not run docker containers with  -d  as this will prevent the container from starting as a child of this pid. systemd will think the process has exited and the unit will be stopped.  WantedBy=  is the target that this unit is a part of.  To start a new unit, we need to tell systemd to create the symlink and then start the file:  $ sudo systemctl enable /etc/systemd/system/hello.service\n$ sudo systemctl start hello.service  To verify the unit started, you can see the list of containers running with  docker ps  and read the unit's output with  journalctl :  $ journalctl -f -u hello.service\n-- Logs begin at Fri 2014-02-07 00:05:55 UTC. --\nFeb 11 17:46:26 localhost docker[23470]: Hello World\nFeb 11 17:46:27 localhost docker[23470]: Hello World\nFeb 11 17:46:28 localhost docker[23470]: Hello World\n...  Overview of systemctl  Reading the System Log",
            "title": "Unit file"
        },
        {
            "location": "/os/getting-started-with-systemd/#advanced-unit-files",
            "text": "systemd provides a high degree of functionality in your unit files. Here's a curated list of useful features listed in the order they'll occur in the lifecycle of a unit:     Name  Description      ExecStartPre  Commands that will run before  ExecStart .    ExecStart  Main commands to run for this unit.    ExecStartPost  Commands that will run after all  ExecStart  commands have completed.    ExecReload  Commands that will run when this unit is reloaded via  systemctl reload foo.service    ExecStop  Commands that will run when this unit is considered failed or if it is stopped via  systemctl stop foo.service    ExecStopPost  Commands that will run after  ExecStop  has completed.    RestartSec  The amount of time to sleep before restarting a service. Useful to prevent your failed service from attempting to restart itself every 100ms.     The full list is located on the  systemd man page .  Let's put a few of these concepts together to register new units within etcd. Imagine we had another container running that would read these values from etcd and act upon them.  We can use  ExecStartPre  to scrub existing container state. The  docker kill  will force any previous copy of this container to stop, which is useful if we restarted the unit but Docker didn't stop the container for some reason. The  =-  is systemd syntax to ignore errors for this command. We need to do this because Docker will return a non-zero exit code if we try to stop a container that doesn't exist. We don't consider this an error (because we want the container stopped) so we tell systemd to ignore the possible failure.  docker rm  will remove the container and  docker pull  will pull down the latest version. You can optionally pull down a specific version as a Docker tag:  coreos/apache:1.2.3  ExecStart  is where the container is started from the container image that we pulled above.  Since our container will be started in  ExecStart , it makes sense for our etcd command to run as  ExecStartPost  to ensure that our container is started and functioning.  When the service is told to stop, we need to stop the Docker container using its  --name  from the run command. We also need to clean up our etcd key when the container exits or the unit is failed by using  ExecStopPost .  [Unit]\nDescription=My Advanced Service\nAfter=etcd2.service\nAfter=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill apache1\nExecStartPre=-/usr/bin/docker rm apache1\nExecStartPre=/usr/bin/docker pull coreos/apache\nExecStart=/usr/bin/docker run --name apache1 -p 8081:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running\nExecStop=/usr/bin/docker stop apache1\nExecStopPost=/usr/bin/etcdctl rm /domains/example.com/10.10.10.123:8081\n\n[Install]\nWantedBy=multi-user.target  While it's possible to manage the starting, stopping, and removal of the container in a single  ExecStart  command by using  docker run --rm , it's a good idea to separate the container's lifecycle into  ExecStartPre ,  ExecStart , and  ExecStop  options as we've done above. This gives you a chance to inspect the container's state after it stops or fails.",
            "title": "Advanced unit files"
        },
        {
            "location": "/os/getting-started-with-systemd/#unit-specifiers",
            "text": "In our last example we had to hardcode our IP address when we announced our container in etcd. That's not scalable and systemd has a few variables built in to help us out. Here's a few of the most useful:     Variable  Meaning  Description      %n  Full unit name  Useful if the name of your unit is unique enough to be used as an argument on a command.    %m  Machine ID  Useful for namespacing etcd keys by machine. Example:  /machines/%m/units    %b  BootID  Similar to the machine ID, but this value is random and changes on each boot    %H  Hostname  Allows you to run the same unit file across many machines. Useful for service discovery. Example:  /domains/example.com/%H:8081     A full list of specifiers can be found on the  systemd man page .",
            "title": "Unit specifiers"
        },
        {
            "location": "/os/getting-started-with-systemd/#instantiated-units",
            "text": "Since systemd is based on symlinks, there are a few interesting tricks you can leverage that are very powerful when used with containers. If you create multiple symlinks to the same unit file, the following variables become available to you:     Variable  Meaning  Description      %p  Prefix name  Refers to any string before  @  in your unit name.    %i  Instance name  Refers to the string between the  @  and the suffix.     In our earlier example we had to hardcode our IP address when registering within etcd:  ExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running  We can enhance this by using  %H  and  %i  to dynamically announce the hostname and port. Specify the port after the  @  by using two unit files named  foo@123.service  and  foo@456.service :  ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running  This gives us the flexibility to use a single unit file to announce multiple copies of the same container on a single machine (no port overlap) and on multiple machines (no hostname overlap).",
            "title": "Instantiated units"
        },
        {
            "location": "/os/getting-started-with-systemd/#more-information",
            "text": "systemd.service Docs  systemd.unit Docs  systemd.target Docs",
            "title": "More information"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/",
            "text": "Using systemd and udev rules\n\u00b6\n\n\nIn our example we will use libvirt VM with Flatcar Container Linux and run systemd unit on disk attach event. First of all we have to create systemd unit file \n/etc/systemd/system/device-attach.service\n:\n\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/echo 'device has been attached'\n\n\n\nThis unit file will be triggered by our udev rule.\n\n\nThen we have to start \nudevadm monitor --environment\n to monitor kernel events.\n\n\nOnce you've attached virtio libvirt device (i.e. \nvirsh attach-disk coreos /dev/VG/test vdc\n) you'll see similar \nudevadm\n output:\n\n\nUDEV  [545.954641] add      /devices/pci0000:00/0000:00:18.0/virtio4/block/vdb (block)\n.ID_FS_TYPE_NEW=\nACTION=add\nDEVNAME=/dev/vdb\nDEVPATH=/devices/pci0000:00/0000:00:18.0/virtio4/block/vdb\nDEVTYPE=disk\nID_FS_TYPE=\nMAJOR=254\nMINOR=16\nSEQNUM=1327\nSUBSYSTEM=block\nUSEC_INITIALIZED=545954447\n\n\n\nAccording to text above udev generates event which contains directives (ACTION=add and SUBSYSTEM=block) we will use in our rule. It should look this way:\n\n\nACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"\n\n\n\nThat rule means that udev will trigger \ndevice-attach.service\n systemd unit on any block device attachment. Now when we use this command \nvirsh attach-disk coreos /dev/VG/test vdc\n on host machine, we should see \ndevice has been attached\n message in Flatcar Container Linux node's journal. This example should be similar to USB/SAS/SATA device attach.\n\n\nContainer Linux Config example\n\u00b6\n\n\nTo use the unit and udev rule with a Container Linux Config, modify this example as needed:\n\n\nstorage:\n  files:\n    - path: /etc/udev/rules.d/01-block.rules\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          ACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"\nsystemd:\n  units:\n    - name: device-attach.service\n      contents: |\n        [Unit]\n        Description=Notify about attached device\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo 'device has been attached'\n\n\n\nMore systemd examples\n\u00b6\n\n\nFor more systemd examples, check out these documents:\n\n\nCustomizing Docker\n\n\nCustomizing the SSH Daemon\n\n\nUsing systemd Drop-In Units\n\n\nMore information\n\u00b6\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs\n\n\nsystemd.target Docs\n\n\nudev Docs",
            "title": "Using systemd and udev rules"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#using-systemd-and-udev-rules",
            "text": "In our example we will use libvirt VM with Flatcar Container Linux and run systemd unit on disk attach event. First of all we have to create systemd unit file  /etc/systemd/system/device-attach.service :  [Service]\nType=oneshot\nExecStart=/usr/bin/echo 'device has been attached'  This unit file will be triggered by our udev rule.  Then we have to start  udevadm monitor --environment  to monitor kernel events.  Once you've attached virtio libvirt device (i.e.  virsh attach-disk coreos /dev/VG/test vdc ) you'll see similar  udevadm  output:  UDEV  [545.954641] add      /devices/pci0000:00/0000:00:18.0/virtio4/block/vdb (block)\n.ID_FS_TYPE_NEW=\nACTION=add\nDEVNAME=/dev/vdb\nDEVPATH=/devices/pci0000:00/0000:00:18.0/virtio4/block/vdb\nDEVTYPE=disk\nID_FS_TYPE=\nMAJOR=254\nMINOR=16\nSEQNUM=1327\nSUBSYSTEM=block\nUSEC_INITIALIZED=545954447  According to text above udev generates event which contains directives (ACTION=add and SUBSYSTEM=block) we will use in our rule. It should look this way:  ACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"  That rule means that udev will trigger  device-attach.service  systemd unit on any block device attachment. Now when we use this command  virsh attach-disk coreos /dev/VG/test vdc  on host machine, we should see  device has been attached  message in Flatcar Container Linux node's journal. This example should be similar to USB/SAS/SATA device attach.",
            "title": "Using systemd and udev rules"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#container-linux-config-example",
            "text": "To use the unit and udev rule with a Container Linux Config, modify this example as needed:  storage:\n  files:\n    - path: /etc/udev/rules.d/01-block.rules\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          ACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"\nsystemd:\n  units:\n    - name: device-attach.service\n      contents: |\n        [Unit]\n        Description=Notify about attached device\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo 'device has been attached'",
            "title": "Container Linux Config example"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#more-systemd-examples",
            "text": "For more systemd examples, check out these documents:  Customizing Docker  Customizing the SSH Daemon  Using systemd Drop-In Units",
            "title": "More systemd examples"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#more-information",
            "text": "systemd.service Docs  systemd.unit Docs  systemd.target Docs  udev Docs",
            "title": "More information"
        },
        {
            "location": "/os/switching-channels/",
            "text": "Switching release channels\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nBy design, the Flatcar Container Linux update engine does not execute downgrades. If you're switching from a channel with a higher Flatcar Container Linux version than the new channel, your machine won't be updated again until the new channel contains a higher version number.\n\n\n\n\nCustomizing channel configuration\n\u00b6\n\n\nThe update engine sources its configuration from \n/usr/share/flatcar/update.conf\n and \n/etc/flatcar/update.conf\n.\nThe former file contains the default hardcoded configuration from the running OS version. Its values cannot be edited, but they can be overridden by the ones in the latter file.\n\n\nTo switch a machine to a different channel, specify the new channel group in \n/etc/flatcar/update.conf\n:\n\n\nGROUP=beta\n\n\n\nIn order for the configuration override to take effect, the update engine must first be restarted:\n\n\nsudo systemctl restart update-engine\n\n\n\nDebugging\n\u00b6\n\n\nAfter the update engine is restarted, the machine should check for an update within an hour.\n\n\nThe live status of updates checking can queried via:\n\n\nupdate_engine_client --status\n\n\n\nThe update engine logs all update attempts, which can inspected in the system journal:\n\n\njournalctl -f -u update-engine\n\n\n\nFor reference, the OS version and channel for a running system can be determined via:\n\n\ncat /usr/share/flatcar/os-release\n\ncat /usr/share/flatcar/update.conf\n\n\n\nNote: while a manual channel switch is in progress, \n/usr/share/flatcar/update.conf\n shows the channel for the current OS while \n/etc/flatcar/update.conf\n shows the one for the next update.",
            "title": "Switching release channels"
        },
        {
            "location": "/os/switching-channels/#switching-release-channels",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  By design, the Flatcar Container Linux update engine does not execute downgrades. If you're switching from a channel with a higher Flatcar Container Linux version than the new channel, your machine won't be updated again until the new channel contains a higher version number.",
            "title": "Switching release channels"
        },
        {
            "location": "/os/switching-channels/#customizing-channel-configuration",
            "text": "The update engine sources its configuration from  /usr/share/flatcar/update.conf  and  /etc/flatcar/update.conf .\nThe former file contains the default hardcoded configuration from the running OS version. Its values cannot be edited, but they can be overridden by the ones in the latter file.  To switch a machine to a different channel, specify the new channel group in  /etc/flatcar/update.conf :  GROUP=beta  In order for the configuration override to take effect, the update engine must first be restarted:  sudo systemctl restart update-engine",
            "title": "Customizing channel configuration"
        },
        {
            "location": "/os/switching-channels/#debugging",
            "text": "After the update engine is restarted, the machine should check for an update within an hour.  The live status of updates checking can queried via:  update_engine_client --status  The update engine logs all update attempts, which can inspected in the system journal:  journalctl -f -u update-engine  For reference, the OS version and channel for a running system can be determined via:  cat /usr/share/flatcar/os-release\n\ncat /usr/share/flatcar/update.conf  Note: while a manual channel switch is in progress,  /usr/share/flatcar/update.conf  shows the channel for the current OS while  /etc/flatcar/update.conf  shows the one for the next update.",
            "title": "Debugging"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/",
            "text": "Scheduling tasks with systemd timers\n\u00b6\n\n\nFlatcar Container Linux uses systemd timers (\ncron\n replacement) to schedule tasks. Here we will show you how you can schedule a periodic job.\n\n\nLet's create an alternative for this \ncrontab\n job:\n\n\n*/10 * * * * /usr/bin/date >> /tmp/date\n\n\n\nTimers work directly with services' units. So we have to create \n/etc/systemd/system/date.service\n first:\n\n\n[Unit]\nDescription=Prints date into /tmp/date file\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'\n\n\n\nThen we have to create timer unit with the same name but with \n*.timer\n suffix \n/etc/systemd/system/date.timer\n:\n\n\n[Unit]\nDescription=Run date.service every 10 minutes\n\n[Timer]\nOnCalendar=*:0/10\n\n\n\nThis config will run \ndate.service\n every 10 minutes. You can also list all timers enabled in your system using \nsystemctl list-timers\n command or \nsystemctl list-timers --all\n to list all timers. Run \nsystemctl start date.timer\n to enable timer.\n\n\nYou can also create timer with different name, i.e. \ntask.timer\n. In this case you have specify service unit name:\n\n\nUnit=date.service\n\n\n\nContainer Linux Config\n\u00b6\n\n\nHere you'll find an example Container Linux Config demonstrating how to install systemd timers:\n\n\nsystemd:\n  units:\n    - name: date.service\n      contents: |\n        [Unit]\n        Description=Prints date into /tmp/date file\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'\n    - name: date.timer\n      enable: true\n      contents: |\n        [Unit]\n        Description=Run date.service every 10 minutes\n\n        [Timer]\n        OnCalendar=*:0/10\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nFurther reading\n\u00b6\n\n\nIf you're interested in more general systemd timers feature, check out the \nfull documentation\n.",
            "title": "Scheduling tasks with systemd"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/#scheduling-tasks-with-systemd-timers",
            "text": "Flatcar Container Linux uses systemd timers ( cron  replacement) to schedule tasks. Here we will show you how you can schedule a periodic job.  Let's create an alternative for this  crontab  job:  */10 * * * * /usr/bin/date >> /tmp/date  Timers work directly with services' units. So we have to create  /etc/systemd/system/date.service  first:  [Unit]\nDescription=Prints date into /tmp/date file\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'  Then we have to create timer unit with the same name but with  *.timer  suffix  /etc/systemd/system/date.timer :  [Unit]\nDescription=Run date.service every 10 minutes\n\n[Timer]\nOnCalendar=*:0/10  This config will run  date.service  every 10 minutes. You can also list all timers enabled in your system using  systemctl list-timers  command or  systemctl list-timers --all  to list all timers. Run  systemctl start date.timer  to enable timer.  You can also create timer with different name, i.e.  task.timer . In this case you have specify service unit name:  Unit=date.service",
            "title": "Scheduling tasks with systemd timers"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/#container-linux-config",
            "text": "Here you'll find an example Container Linux Config demonstrating how to install systemd timers:  systemd:\n  units:\n    - name: date.service\n      contents: |\n        [Unit]\n        Description=Prints date into /tmp/date file\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'\n    - name: date.timer\n      enable: true\n      contents: |\n        [Unit]\n        Description=Run date.service every 10 minutes\n\n        [Timer]\n        OnCalendar=*:0/10\n\n        [Install]\n        WantedBy=multi-user.target",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/#further-reading",
            "text": "If you're interested in more general systemd timers feature, check out the  full documentation .",
            "title": "Further reading"
        },
        {
            "location": "/os/customizing-sshd/",
            "text": "Customizing the SSH daemon\n\u00b6\n\n\nFlatcar Container Linux defaults to running an OpenSSH daemon using \nsystemd\n socket activation -- when a client connects to the port configured for SSH, \nsshd\n is started on the fly for that client using a \nsystemd\n unit derived automatically from a template. In some cases you may want to customize this daemon's authentication methods or other configuration. This guide will show you how to do that at boot time using a \nContainer Linux Config\n, and after building by modifying the \nsystemd\n unit file.\n\n\nAs a practical example, when a client fails to connect by not completing the TCP connection (e.g. because the \"client\" is actually a TCP port scanner), the MOTD may report failures of \nsystemd\n units (which will be named by the source IP that failed to connect) next time you log in to the Flatcar Container Linux host. These failures are not themselves harmful, but it is a good general practice to change how SSH listens, either by changing the IP address \nsshd\n listens to from the default setting (which listens on all configured interfaces), changing the default port, or both.\n\n\nCustomizing sshd with a Container Linux Config\n\u00b6\n\n\nIn this example we will disable logins for the \nroot\n user, only allow login for the \ncore\n user and disable password based authentication. For more details on what sections can be added to \n/etc/ssh/sshd_config\n see the \nOpenSSH manual\n.\nIf you're interested in additional security options, Mozilla provides a well-commented example of a \nhardened configuration\n.\n\n\nstorage:\n  files:\n    - path: /etc/ssh/sshd_config\n      filesystem: root\n      mode: 0600\n      contents:\n        inline: |\n          # Use most defaults for sshd configuration.\n          UsePrivilegeSeparation sandbox\n          Subsystem sftp internal-sftp\n          UseDNS no\n\n          PermitRootLogin no\n          AllowUsers core\n          AuthenticationMethods publickey\n\n\n\nChanging the sshd port\n\u00b6\n\n\nFlatcar Container Linux ships with socket-activated SSH daemon by default. The configuration for this can be found at \n/usr/lib/systemd/system/sshd.socket\n. We're going to override some of the default settings for this in the Container Linux Config provided at boot:\n\n\nsystemd:\n  units:\n    - name: sshd.socket\n      dropins:\n      - name: 10-sshd-port.conf\n        contents: |\n          [Socket]\n          ListenStream=\n          ListenStream=222\n\n\n\nsshd\n will now listen only on port 222 on all interfaces when the system is built.\n\n\nDisabling socket activation for sshd\n\u00b6\n\n\nIt may be desirable to disable socket-activation for sshd to ensure it will reliably accept connections even when systemd or dbus aren't operating correctly.\n\n\nTo configure sshd on Flatcar Container Linux without socket activation, a Container Linux Config file similar to the following may be used:\n\n\nsystemd:\n  units:\n  - name: sshd.service\n    enable: true\n  - name: sshd.socket\n    mask: true\n\n\n\nNote that in this configuration the port will be configured by updating the \n/etc/ssh/sshd_config\n file with the \nPort\n directive rather than via \nsshd.socket\n.\n\n\nFurther reading\n\u00b6\n\n\nRead the \nfull Container Linux Config\n guide for more details on working with Container Linux Configs, including setting user's ssh keys.\n\n\nCustomizing sshd after first boot\n\u00b6\n\n\nSince \nContainer Linux Configs\n are only applied on first boot, existing machines will have to be configured in a different way.\n\n\nThe following sections walk through applying the same changes documented above on a running machine.\n\n\nNote\n: To avoid incidentally locking yourself out of the machine, it's a good idea to double-check you're able to directly login to the machine's console, if applicable.\n\n\nCustomizing sshd_config\n\u00b6\n\n\nSince \n/etc/ssh/sshd_config\n is a symlink to a read only file in \n/usr\n, it\nneeds to be replaced with a regular file before it may be edited.\n\n\nThis, for example, can be done by running \nsudo sed -i '' /etc/ssh/sshd_config\n.\n\n\nAt this point, any configuration changes can easily be applied by editing the file \n/etc/ssh/sshd_config\n.\n\n\nChanging the sshd port\n\u00b6\n\n\nThe sshd.socket unit may be configured via systemd \ndropins\n.\n\n\nTo change how sshd listens, update the list of \nListenStream\ns in the \n[Socket]\n section of the dropin.\n\n\nNote\n: \nListenStream\n is a list of values with each line adding to the list. An empty value clears the list, which is why \nListenStream=\n is necessary to prevent it from \nalso\n listening on the default port \n22\n.\n\n\nTo change just the listened-to port (in this example, port 222), create a dropin at \n/etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n\n\n# /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222\n\n\n\nTo change the listened-to IP address (in this example, 10.20.30.40):\n\n\n# /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=10.20.30.40:22\nFreeBind=true\n\n\n\nYou can specify both an IP and an alternate port in a single \nListenStream\n line. IPv6 address bindings would be specified using the format \n[2001:db8::7]:22\n.\n\n\nNote\n: While specifying an IP address is optional, you must always specify the port, even if it is the default SSH port. The \nFreeBind\n option is used to allow the socket to be bound on addresses that are not yet configured on an interface, to avoid issues caused by delays in IP configuration at boot. (This option is required only if you are specifying an address.)\n\n\nMultiple ListenStream lines can be specified, in which case \nsshd\n will listen on all the specified sockets:\n\n\n# /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222\nListenStream=10.20.30.40:223\nFreeBind=true\n\n\n\nActivating changes\n\u00b6\n\n\nAfter creating the dropin file, the changes can be activated by doing a daemon-reload and restarting \nsshd.socket\n\n\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart sshd.socket\n\n\n\nWe now see that systemd is listening on the new sockets:\n\n\n$ systemctl status sshd.socket\n\u25cf sshd.socket - OpenSSH Server Socket\n   Loaded: loaded (/etc/systemd/system/sshd.socket; disabled; vendor preset: disabled)\n   Active: active (listening) since Wed 2015-10-14 21:04:31 UTC; 2min 45s ago\n   Listen: [::]:222 (Stream)\n           10.20.30.40:223 (Stream)\n Accepted: 1; Connected: 0\n...\n\n\n\nAnd if we attempt to connect to port 22 on our public IP, the connection is rejected, but port 222 works:\n\n\n$ ssh core@[public IP]\nssh: connect to host [public IP] port 22: Connection refused\n$ ssh -p 222 core@[public IP]\nFlatcar Container Linux by Kinvolk stable (1353.8.0)\ncore@machine $\n\n\n\nDisabling socket-activation for sshd\n\u00b6\n\n\nSimply mask the systemd.socket unit:\n\n\n# systemctl mask --now sshd.socket\n\n\n\nFinally, restart the sshd.service unit:\n\n\n# systemctl restart sshd.service\n\n\n\nFurther reading on systemd units\n\u00b6\n\n\nFor more information about configuring Flatcar Container Linux hosts with \nsystemd\n, see \nGetting Started with systemd\n.",
            "title": "Customizing the SSH daemon"
        },
        {
            "location": "/os/customizing-sshd/#customizing-the-ssh-daemon",
            "text": "Flatcar Container Linux defaults to running an OpenSSH daemon using  systemd  socket activation -- when a client connects to the port configured for SSH,  sshd  is started on the fly for that client using a  systemd  unit derived automatically from a template. In some cases you may want to customize this daemon's authentication methods or other configuration. This guide will show you how to do that at boot time using a  Container Linux Config , and after building by modifying the  systemd  unit file.  As a practical example, when a client fails to connect by not completing the TCP connection (e.g. because the \"client\" is actually a TCP port scanner), the MOTD may report failures of  systemd  units (which will be named by the source IP that failed to connect) next time you log in to the Flatcar Container Linux host. These failures are not themselves harmful, but it is a good general practice to change how SSH listens, either by changing the IP address  sshd  listens to from the default setting (which listens on all configured interfaces), changing the default port, or both.",
            "title": "Customizing the SSH daemon"
        },
        {
            "location": "/os/customizing-sshd/#customizing-sshd-with-a-container-linux-config",
            "text": "In this example we will disable logins for the  root  user, only allow login for the  core  user and disable password based authentication. For more details on what sections can be added to  /etc/ssh/sshd_config  see the  OpenSSH manual .\nIf you're interested in additional security options, Mozilla provides a well-commented example of a  hardened configuration .  storage:\n  files:\n    - path: /etc/ssh/sshd_config\n      filesystem: root\n      mode: 0600\n      contents:\n        inline: |\n          # Use most defaults for sshd configuration.\n          UsePrivilegeSeparation sandbox\n          Subsystem sftp internal-sftp\n          UseDNS no\n\n          PermitRootLogin no\n          AllowUsers core\n          AuthenticationMethods publickey",
            "title": "Customizing sshd with a Container Linux Config"
        },
        {
            "location": "/os/customizing-sshd/#changing-the-sshd-port",
            "text": "Flatcar Container Linux ships with socket-activated SSH daemon by default. The configuration for this can be found at  /usr/lib/systemd/system/sshd.socket . We're going to override some of the default settings for this in the Container Linux Config provided at boot:  systemd:\n  units:\n    - name: sshd.socket\n      dropins:\n      - name: 10-sshd-port.conf\n        contents: |\n          [Socket]\n          ListenStream=\n          ListenStream=222  sshd  will now listen only on port 222 on all interfaces when the system is built.",
            "title": "Changing the sshd port"
        },
        {
            "location": "/os/customizing-sshd/#disabling-socket-activation-for-sshd",
            "text": "It may be desirable to disable socket-activation for sshd to ensure it will reliably accept connections even when systemd or dbus aren't operating correctly.  To configure sshd on Flatcar Container Linux without socket activation, a Container Linux Config file similar to the following may be used:  systemd:\n  units:\n  - name: sshd.service\n    enable: true\n  - name: sshd.socket\n    mask: true  Note that in this configuration the port will be configured by updating the  /etc/ssh/sshd_config  file with the  Port  directive rather than via  sshd.socket .",
            "title": "Disabling socket activation for sshd"
        },
        {
            "location": "/os/customizing-sshd/#further-reading",
            "text": "Read the  full Container Linux Config  guide for more details on working with Container Linux Configs, including setting user's ssh keys.",
            "title": "Further reading"
        },
        {
            "location": "/os/customizing-sshd/#customizing-sshd-after-first-boot",
            "text": "Since  Container Linux Configs  are only applied on first boot, existing machines will have to be configured in a different way.  The following sections walk through applying the same changes documented above on a running machine.  Note : To avoid incidentally locking yourself out of the machine, it's a good idea to double-check you're able to directly login to the machine's console, if applicable.",
            "title": "Customizing sshd after first boot"
        },
        {
            "location": "/os/customizing-sshd/#customizing-sshd95config",
            "text": "Since  /etc/ssh/sshd_config  is a symlink to a read only file in  /usr , it\nneeds to be replaced with a regular file before it may be edited.  This, for example, can be done by running  sudo sed -i '' /etc/ssh/sshd_config .  At this point, any configuration changes can easily be applied by editing the file  /etc/ssh/sshd_config .",
            "title": "Customizing sshd_config"
        },
        {
            "location": "/os/customizing-sshd/#changing-the-sshd-port_1",
            "text": "The sshd.socket unit may be configured via systemd  dropins .  To change how sshd listens, update the list of  ListenStream s in the  [Socket]  section of the dropin.  Note :  ListenStream  is a list of values with each line adding to the list. An empty value clears the list, which is why  ListenStream=  is necessary to prevent it from  also  listening on the default port  22 .  To change just the listened-to port (in this example, port 222), create a dropin at  /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf  # /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222  To change the listened-to IP address (in this example, 10.20.30.40):  # /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=10.20.30.40:22\nFreeBind=true  You can specify both an IP and an alternate port in a single  ListenStream  line. IPv6 address bindings would be specified using the format  [2001:db8::7]:22 .  Note : While specifying an IP address is optional, you must always specify the port, even if it is the default SSH port. The  FreeBind  option is used to allow the socket to be bound on addresses that are not yet configured on an interface, to avoid issues caused by delays in IP configuration at boot. (This option is required only if you are specifying an address.)  Multiple ListenStream lines can be specified, in which case  sshd  will listen on all the specified sockets:  # /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222\nListenStream=10.20.30.40:223\nFreeBind=true",
            "title": "Changing the sshd port"
        },
        {
            "location": "/os/customizing-sshd/#activating-changes",
            "text": "After creating the dropin file, the changes can be activated by doing a daemon-reload and restarting  sshd.socket  $ sudo systemctl daemon-reload\n$ sudo systemctl restart sshd.socket  We now see that systemd is listening on the new sockets:  $ systemctl status sshd.socket\n\u25cf sshd.socket - OpenSSH Server Socket\n   Loaded: loaded (/etc/systemd/system/sshd.socket; disabled; vendor preset: disabled)\n   Active: active (listening) since Wed 2015-10-14 21:04:31 UTC; 2min 45s ago\n   Listen: [::]:222 (Stream)\n           10.20.30.40:223 (Stream)\n Accepted: 1; Connected: 0\n...  And if we attempt to connect to port 22 on our public IP, the connection is rejected, but port 222 works:  $ ssh core@[public IP]\nssh: connect to host [public IP] port 22: Connection refused\n$ ssh -p 222 core@[public IP]\nFlatcar Container Linux by Kinvolk stable (1353.8.0)\ncore@machine $",
            "title": "Activating changes"
        },
        {
            "location": "/os/customizing-sshd/#disabling-socket-activation-for-sshd_1",
            "text": "Simply mask the systemd.socket unit:  # systemctl mask --now sshd.socket  Finally, restart the sshd.service unit:  # systemctl restart sshd.service",
            "title": "Disabling socket-activation for sshd"
        },
        {
            "location": "/os/customizing-sshd/#further-reading-on-systemd-units",
            "text": "For more information about configuring Flatcar Container Linux hosts with  systemd , see  Getting Started with systemd .",
            "title": "Further reading on systemd units"
        },
        {
            "location": "/os/sssd/",
            "text": "Configuring SSSD on Flatcar Container Linux\n\u00b6\n\n\nFlatcar Container Linux ships with the System Security Services Daemon, allowing integration between Flatcar Container Linux and enterprise authentication services.\n\n\nConfiguring SSSD\n\u00b6\n\n\nEdit /etc/sssd/sssd.conf. This configuration file is fully documented \nhere\n. For example, to configure SSSD to use an IPA server called ipa.example.com, sssd.conf should read:\n\n\n[sssd]\nconfig_file_version = 2\nservices = nss, pam\ndomains = LDAP\n[nss]\n[pam]\n[domain/LDAP]\nid_provider = ldap\nauth_provider = ldap\nldap_schema = ipa\nldap_uri = ldap://ipa.example.com\n\n\n\nStart SSSD\n\u00b6\n\n\nsudo systemctl start sssd\n\n\n\nMake SSSD available on future reboots\n\u00b6\n\n\nsudo systemctl enable sssd",
            "title": "Configuring SSSD on Flatcar Container Linux"
        },
        {
            "location": "/os/sssd/#configuring-sssd-on-flatcar-container-linux",
            "text": "Flatcar Container Linux ships with the System Security Services Daemon, allowing integration between Flatcar Container Linux and enterprise authentication services.",
            "title": "Configuring SSSD on Flatcar Container Linux"
        },
        {
            "location": "/os/sssd/#configuring-sssd",
            "text": "Edit /etc/sssd/sssd.conf. This configuration file is fully documented  here . For example, to configure SSSD to use an IPA server called ipa.example.com, sssd.conf should read:  [sssd]\nconfig_file_version = 2\nservices = nss, pam\ndomains = LDAP\n[nss]\n[pam]\n[domain/LDAP]\nid_provider = ldap\nauth_provider = ldap\nldap_schema = ipa\nldap_uri = ldap://ipa.example.com",
            "title": "Configuring SSSD"
        },
        {
            "location": "/os/sssd/#start-sssd",
            "text": "sudo systemctl start sssd",
            "title": "Start SSSD"
        },
        {
            "location": "/os/sssd/#make-sssd-available-on-future-reboots",
            "text": "sudo systemctl enable sssd",
            "title": "Make SSSD available on future reboots"
        },
        {
            "location": "/os/hardening-guide/",
            "text": "Flatcar Container Linux hardening guide\n\u00b6\n\n\nThis guide covers the basics of securing a Flatcar Container Linux instance. Flatcar Container Linux has a very slim network profile and the only service that listens by default on Flatcar Container Linux is sshd on port 22 on all interfaces. There are also some defaults for local users and services that should be considered.\n\n\nRemote listening services\n\u00b6\n\n\nDisabling sshd\n\u00b6\n\n\nTo disable sshd from listening you can stop the socket:\n\n\nsystemctl mask sshd.socket --now\n\n\n\nIf you wish to make further customizations see our \ncustomize sshd guide\n.\n\n\nRemote non-listening services\n\u00b6\n\n\netcd and Locksmith\n\u00b6\n\n\netcd and Locksmith should be secured and authenticated using TLS if you are using these services. Please see the relevant guides for details.\n\n\n\n\netcd security guide\n\n\n\n\nLocal services\n\u00b6\n\n\nLocal users\n\u00b6\n\n\nFlatcar Container Linux has a single default user account called \"core\". Generally this user is the one that gets ssh keys added to it via a Container Linux Config for administrators to login. The core user, by default, has access to the wheel group which grants sudo access. You can change this by removing the core user from wheel by running this command: \ngpasswd -d core wheel\n.\n\n\nDocker daemon\n\u00b6\n\n\nThe docker daemon is accessible via a unix domain socket at \n/run/docker.sock\n. Users in the \"docker\" group have access to this service and access to the docker socket grants similar capabilities to sudo. The core user, by default, has access to the docker group. You can change this by removing the core user from docker by running this command: \ngpasswd -d core docker\n.\n\n\nrkt fetch\n\u00b6\n\n\nUsers in the \"rkt\" group have access to the rkt container image store. A user may download new images and place them in the store if they belong to this group. This could be used as an attack vector to insert images that are later executed as root by the rkt container runtime. The core user, by default, has access to the rkt group. You can change this by removing the core user from rkt by running this command: \ngpasswd -d core rkt\n.\n\n\nAdditional hardening\n\u00b6\n\n\nDisabling Simultaneous Multi-Threading\n\u00b6\n\n\nRecent Intel CPU vulnerabilities cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.\n\n\nThe \nSMT on Container Linux guide\n provides guidance and instructions for disabling SMT.\n\n\nSELinux\n\u00b6\n\n\nSELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.\n\n\nFlatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. The \nSELinux on Flatcar Container Linux guide\n covers the process of checking containers for SELinux policy compatibility and switching SELinux into enforcing mode.",
            "title": "Hardening a Flatcar Container Linux machine"
        },
        {
            "location": "/os/hardening-guide/#flatcar-container-linux-hardening-guide",
            "text": "This guide covers the basics of securing a Flatcar Container Linux instance. Flatcar Container Linux has a very slim network profile and the only service that listens by default on Flatcar Container Linux is sshd on port 22 on all interfaces. There are also some defaults for local users and services that should be considered.",
            "title": "Flatcar Container Linux hardening guide"
        },
        {
            "location": "/os/hardening-guide/#remote-listening-services",
            "text": "",
            "title": "Remote listening services"
        },
        {
            "location": "/os/hardening-guide/#disabling-sshd",
            "text": "To disable sshd from listening you can stop the socket:  systemctl mask sshd.socket --now  If you wish to make further customizations see our  customize sshd guide .",
            "title": "Disabling sshd"
        },
        {
            "location": "/os/hardening-guide/#remote-non-listening-services",
            "text": "",
            "title": "Remote non-listening services"
        },
        {
            "location": "/os/hardening-guide/#etcd-and-locksmith",
            "text": "etcd and Locksmith should be secured and authenticated using TLS if you are using these services. Please see the relevant guides for details.   etcd security guide",
            "title": "etcd and Locksmith"
        },
        {
            "location": "/os/hardening-guide/#local-services",
            "text": "",
            "title": "Local services"
        },
        {
            "location": "/os/hardening-guide/#local-users",
            "text": "Flatcar Container Linux has a single default user account called \"core\". Generally this user is the one that gets ssh keys added to it via a Container Linux Config for administrators to login. The core user, by default, has access to the wheel group which grants sudo access. You can change this by removing the core user from wheel by running this command:  gpasswd -d core wheel .",
            "title": "Local users"
        },
        {
            "location": "/os/hardening-guide/#docker-daemon",
            "text": "The docker daemon is accessible via a unix domain socket at  /run/docker.sock . Users in the \"docker\" group have access to this service and access to the docker socket grants similar capabilities to sudo. The core user, by default, has access to the docker group. You can change this by removing the core user from docker by running this command:  gpasswd -d core docker .",
            "title": "Docker daemon"
        },
        {
            "location": "/os/hardening-guide/#rkt-fetch",
            "text": "Users in the \"rkt\" group have access to the rkt container image store. A user may download new images and place them in the store if they belong to this group. This could be used as an attack vector to insert images that are later executed as root by the rkt container runtime. The core user, by default, has access to the rkt group. You can change this by removing the core user from rkt by running this command:  gpasswd -d core rkt .",
            "title": "rkt fetch"
        },
        {
            "location": "/os/hardening-guide/#additional-hardening",
            "text": "",
            "title": "Additional hardening"
        },
        {
            "location": "/os/hardening-guide/#disabling-simultaneous-multi-threading",
            "text": "Recent Intel CPU vulnerabilities cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.  The  SMT on Container Linux guide  provides guidance and instructions for disabling SMT.",
            "title": "Disabling Simultaneous Multi-Threading"
        },
        {
            "location": "/os/hardening-guide/#selinux",
            "text": "SELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.  Flatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. The  SELinux on Flatcar Container Linux guide  covers the process of checking containers for SELinux policy compatibility and switching SELinux into enforcing mode.",
            "title": "SELinux"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/",
            "text": "Checking hardware and firmware support for Flatcar Container Linux Trusted Computing\n\u00b6\n\n\nTrusted Computing requires support in both system hardware and firmware. This document specifies the required support and explains how to determine if a physical machine has the features needed to enable Trusted Computing in Flatcar Container Linux.\n\n\n1. Check for Trusted Platform Module\n\u00b6\n\n\nTrusted Computing depends on the presence of a Trusted Platform Module (TPM). The TPM is a motherboard component responsible for storing the state of the system boot process, and providing a secure communication channel over which this state can be verified. To check for the presence of a TPM, install the latest Alpha version of Flatcar Container Linux and try to list the TPM device file in the \n/sys\n system control filesystem:\n\n\n# ls /sys/class/tpm/tpm0\n\n\nIf this returns an error, the system either does not have a TPM, or it is not enabled in the system firmware. Firmware configuration varies by system. Consult vendor documentation for details.\n\n\n2. Check TPM version\n\u00b6\n\n\nVersion 1.2 TPMs are currently supported. Read the TPM device ID file to discover the TPM version:\n\n\n# cat /sys/class/tpm/tpm0/device/id\n\n\nThe contents of the \nid\n file vary for supported version 1.2 TPMs. It is simplest to check that the file does \nnot\n contain the known string for unsupported version 2.0 TPMs, \nMSFT0101\n. Almost any other non-zero, non-error output from reading the \nid\n file indicates a supported version 1.2 TPM.\n\n\nSupport for version 2.0 TPMs identified with the \nMSFT0101\n string will be added in a future Flatcar Container Linux release.\n\n\n3. Check TPM is enabled and active\n\u00b6\n\n\nThe TPM device provides control files in the \n/sys\n filesystem, as seen above. Read the \nenabled\n and \nactive\n files to check TPM status:\n\n\n# cat /sys/class/tpm/tpm0/device/enabled\n# cat /sys/class/tpm/tpm0/device/active\n\n\n\nIf either of these commands prints \"0\", reconfigure the TPM by writing a code for TPM activation at the next system boot to the PPI \nrequest\n file:\n\n\n# echo 6 > /sys/class/tpm/tpm0/device/ppi/request\n\n\nReboot the system and check TPM status again, as in Step 3.\n\n\n4. Check boot measurement\n\u00b6\n\n\nThe Flatcar Container Linux bootloader will record the state of boot components during the boot process \u2014 \nmeasuring\n each part, in TPM parlance, and storing the result in its Platform Configuration Registers (PCR). Verify that this measurement has been successful by reading the TPM device's \npcrs\n file, a textual representation of the contents of all PCRs:\n\n\n# cat /sys/class/tpm/tpm0/device/pcrs\n\n\nBoot component measurements are recorded in PCRs 9 through 13. These positions in \npcrs\n should all contain meaningful values; that is, values that are neither \n0\n:\n\n\n00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n\nnor \nmax\n:\n\n\nFF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF\n\n\nTrusted\n\u00b6\n\n\nA system that passes each of the above tests supports Flatcar Container Linux Trusted Computing and is actively measuring the boot process over the secure TPM channel.",
            "title": "Trusted Computing Hardware Requirements"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#checking-hardware-and-firmware-support-for-flatcar-container-linux-trusted-computing",
            "text": "Trusted Computing requires support in both system hardware and firmware. This document specifies the required support and explains how to determine if a physical machine has the features needed to enable Trusted Computing in Flatcar Container Linux.",
            "title": "Checking hardware and firmware support for Flatcar Container Linux Trusted Computing"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#1-check-for-trusted-platform-module",
            "text": "Trusted Computing depends on the presence of a Trusted Platform Module (TPM). The TPM is a motherboard component responsible for storing the state of the system boot process, and providing a secure communication channel over which this state can be verified. To check for the presence of a TPM, install the latest Alpha version of Flatcar Container Linux and try to list the TPM device file in the  /sys  system control filesystem:  # ls /sys/class/tpm/tpm0  If this returns an error, the system either does not have a TPM, or it is not enabled in the system firmware. Firmware configuration varies by system. Consult vendor documentation for details.",
            "title": "1. Check for Trusted Platform Module"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#2-check-tpm-version",
            "text": "Version 1.2 TPMs are currently supported. Read the TPM device ID file to discover the TPM version:  # cat /sys/class/tpm/tpm0/device/id  The contents of the  id  file vary for supported version 1.2 TPMs. It is simplest to check that the file does  not  contain the known string for unsupported version 2.0 TPMs,  MSFT0101 . Almost any other non-zero, non-error output from reading the  id  file indicates a supported version 1.2 TPM.  Support for version 2.0 TPMs identified with the  MSFT0101  string will be added in a future Flatcar Container Linux release.",
            "title": "2. Check TPM version"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#3-check-tpm-is-enabled-and-active",
            "text": "The TPM device provides control files in the  /sys  filesystem, as seen above. Read the  enabled  and  active  files to check TPM status:  # cat /sys/class/tpm/tpm0/device/enabled\n# cat /sys/class/tpm/tpm0/device/active  If either of these commands prints \"0\", reconfigure the TPM by writing a code for TPM activation at the next system boot to the PPI  request  file:  # echo 6 > /sys/class/tpm/tpm0/device/ppi/request  Reboot the system and check TPM status again, as in Step 3.",
            "title": "3. Check TPM is enabled and active"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#4-check-boot-measurement",
            "text": "The Flatcar Container Linux bootloader will record the state of boot components during the boot process \u2014  measuring  each part, in TPM parlance, and storing the result in its Platform Configuration Registers (PCR). Verify that this measurement has been successful by reading the TPM device's  pcrs  file, a textual representation of the contents of all PCRs:  # cat /sys/class/tpm/tpm0/device/pcrs  Boot component measurements are recorded in PCRs 9 through 13. These positions in  pcrs  should all contain meaningful values; that is, values that are neither  0 :  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  nor  max :  FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF",
            "title": "4. Check boot measurement"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#trusted",
            "text": "A system that passes each of the above tests supports Flatcar Container Linux Trusted Computing and is actively measuring the boot process over the secure TPM channel.",
            "title": "Trusted"
        },
        {
            "location": "/os/adding-certificate-authorities/",
            "text": "Custom certificate authorities\n\u00b6\n\n\nFlatcar Container Linux supports custom Certificate Authorities (CAs) in addition to the default list of trusted CAs. Adding your own CA allows you to:\n\n\n\n\nUse a corporate wildcard certificate\n\n\nUse your own CA to communicate with an installation of CoreUpdate\n\n\n\n\nThe setup process for any of these use-cases is the same:\n\n\n\n\n\n\nCopy the PEM-encoded certificate authority file (usually with a \n.pem\n file name extension) to \n/etc/ssl/certs\n\n\n\n\n\n\nRun the \nupdate-ca-certificates\n script to update the system bundle of Certificate Authorities. All programs running on the system will now trust the added CA.\n\n\n\n\n\n\nMore information\n\u00b6\n\n\nGenerate Self-Signed Certificates\n\n\netcd Security Model",
            "title": "Adding Cert Authorities"
        },
        {
            "location": "/os/adding-certificate-authorities/#custom-certificate-authorities",
            "text": "Flatcar Container Linux supports custom Certificate Authorities (CAs) in addition to the default list of trusted CAs. Adding your own CA allows you to:   Use a corporate wildcard certificate  Use your own CA to communicate with an installation of CoreUpdate   The setup process for any of these use-cases is the same:    Copy the PEM-encoded certificate authority file (usually with a  .pem  file name extension) to  /etc/ssl/certs    Run the  update-ca-certificates  script to update the system bundle of Certificate Authorities. All programs running on the system will now trust the added CA.",
            "title": "Custom certificate authorities"
        },
        {
            "location": "/os/adding-certificate-authorities/#more-information",
            "text": "Generate Self-Signed Certificates  etcd Security Model",
            "title": "More information"
        },
        {
            "location": "/os/selinux/",
            "text": "SELinux on Flatcar Container Linux\n\u00b6\n\n\nSELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux and rkt. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.\n\n\nFlatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. This allows deployers to verify container operation before enabling SELinux enforcement. This document covers the process of checking containers for SELinux policy compatibility, and switching SELinux into \nenforcing\n mode.\n\n\nCheck a container's compatibility with SELinux policy\n\u00b6\n\n\nTo verify whether the current SELinux policy would inhibit your containers, enable SELinux logging. In the following set of commands, we delete the rules that suppress this logging by default, and copy the policy store from Flatcar Container Linux's read-only \n/usr\n to a writable file system location.\n\n\n$ rm /etc/audit/rules.d/80-selinux.rules\n$ rm /etc/audit/rules.d/99-default.rules\n$ rm /etc/selinux/mcs\n$ cp -a /usr/lib/selinux/mcs /etc/selinux\n$ rm /var/lib/selinux\n$ cp -a /usr/lib/selinux/policy /var/lib/selinux\n$ semodule -DB\n$ systemctl restart audit-rules\n\n\n\nNow run your container. Check the system logs for any messages containing \navc: denied\n. Such messages indicate that an \nenforcing\n SELinux would prevent the container from performing the logged operation. Please open an issue at \nflatcar-linux/Flatcar\n, including the full avc log message.\n\n\nEnable SELinux enforcement\n\u00b6\n\n\nOnce satisfied that your container workload is compatible with the SELinux policy, you can temporarily enable enforcement by running the following command as root:\n\n\n$ setenforce 1\n\n\nA reboot will reset SELinux to \npermissive\n mode.\n\n\nMake SELinux enforcement permanent\n\u00b6\n\n\nTo enable SELinux enforcement across reboots, replace the symbolic link \n/etc/selinux/config\n with the file it targets, so that the file can be written. You can use the \nreadlink\n command to dereference the link, as shown in the following one-liner:\n\n\n$ cp --remove-destination $(readlink -f /etc/selinux/config) /etc/selinux/config\n\n\nNow, edit \n/etc/selinux/config\n to replace \nSELINUX=permissive\n with \nSELINUX=enforcing\n.\n\n\nLimitations\n\u00b6\n\n\nSELinux enforcement is currently incompatible with Btrfs volumes and volumes that are shared between multiple containers.",
            "title": "Using SELinux"
        },
        {
            "location": "/os/selinux/#selinux-on-flatcar-container-linux",
            "text": "SELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux and rkt. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.  Flatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. This allows deployers to verify container operation before enabling SELinux enforcement. This document covers the process of checking containers for SELinux policy compatibility, and switching SELinux into  enforcing  mode.",
            "title": "SELinux on Flatcar Container Linux"
        },
        {
            "location": "/os/selinux/#check-a-containers-compatibility-with-selinux-policy",
            "text": "To verify whether the current SELinux policy would inhibit your containers, enable SELinux logging. In the following set of commands, we delete the rules that suppress this logging by default, and copy the policy store from Flatcar Container Linux's read-only  /usr  to a writable file system location.  $ rm /etc/audit/rules.d/80-selinux.rules\n$ rm /etc/audit/rules.d/99-default.rules\n$ rm /etc/selinux/mcs\n$ cp -a /usr/lib/selinux/mcs /etc/selinux\n$ rm /var/lib/selinux\n$ cp -a /usr/lib/selinux/policy /var/lib/selinux\n$ semodule -DB\n$ systemctl restart audit-rules  Now run your container. Check the system logs for any messages containing  avc: denied . Such messages indicate that an  enforcing  SELinux would prevent the container from performing the logged operation. Please open an issue at  flatcar-linux/Flatcar , including the full avc log message.",
            "title": "Check a container's compatibility with SELinux policy"
        },
        {
            "location": "/os/selinux/#enable-selinux-enforcement",
            "text": "Once satisfied that your container workload is compatible with the SELinux policy, you can temporarily enable enforcement by running the following command as root:  $ setenforce 1  A reboot will reset SELinux to  permissive  mode.",
            "title": "Enable SELinux enforcement"
        },
        {
            "location": "/os/selinux/#make-selinux-enforcement-permanent",
            "text": "To enable SELinux enforcement across reboots, replace the symbolic link  /etc/selinux/config  with the file it targets, so that the file can be written. You can use the  readlink  command to dereference the link, as shown in the following one-liner:  $ cp --remove-destination $(readlink -f /etc/selinux/config) /etc/selinux/config  Now, edit  /etc/selinux/config  to replace  SELINUX=permissive  with  SELINUX=enforcing .",
            "title": "Make SELinux enforcement permanent"
        },
        {
            "location": "/os/selinux/#limitations",
            "text": "SELinux enforcement is currently incompatible with Btrfs volumes and volumes that are shared between multiple containers.",
            "title": "Limitations"
        },
        {
            "location": "/os/disabling-smt/",
            "text": "Disabling SMT on Flatcar Container Linux\n\u00b6\n\n\nRecent Intel CPU vulnerabilities (\nL1TF\n and \nMDS\n) cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.\n\n\nIn addition, the Intel \nTAA\n vulnerability cannot be fully mitigated without disabling either of SMT or the Transactional Synchronization Extensions (TSX). Disabling TSX generally has less performance impact, so is the preferred approach on systems that don't otherwise need to disable SMT. For compatibility reasons, TSX is enabled by default.\n\n\nSMT and TSX should be disabled on affected Intel processors under the following circumstances:\n1. A bare-metal host runs untrusted virtual machines, and \nother arrangements\n have not been made for mitigation.\n2. A bare-metal host runs untrusted code outside a virtual machine.\n\n\nSMT can be conditionally disabled by passing \nmitigations=auto,nosmt\n on the kernel command line. This will disable SMT only if required for mitigating a vulnerability. This approach has two caveats:\n1. It does not protect against unknown vulnerabilities in SMT.\n2. It allows future Flatcar Container Linux updates to disable SMT if needed to mitigate new vulnerabilities.\n\n\nAlternatively, SMT can be unconditionally disabled by passing \nnosmt\n on the kernel command line. This provides the most protection and avoids possible behavior changes on upgrades, at the cost of a potentially unnecessary reduction in performance.\n\n\nTSX can be conditionally disabled on vulnerable CPUs by passing \ntsx=auto\n on the kernel command line, or unconditionally disabled by passing \ntsx=off\n. However, neither setting takes effect on systems affected by MDS, since MDS mitigation automatically protects against TAA as well.\n\n\nFor typical use cases, we recommend enabling the \nmitigations=auto,nosmt\n and \ntsx=auto\n command-line options.\n\n\nConfiguring new machines\n\u00b6\n\n\nThe following Container Linux Config performs two tasks:\n\n\n\n\nAdds \nmitigations=auto,nosmt tsx=auto\n to the kernel command line. This affects the second and subsequent boots of the machine, but not the first boot.\n\n\nOn the first boot, disables SMT at runtime if the system has an Intel processor. This is sufficient to protect against currently-known SMT vulnerabilities until the system is rebooted. After reboot, SMT will be re-enabled if the processor is not actually vulnerable.\n\n\n\n\n# Add kernel command-line arguments to automatically disable SMT or TSX\n# on CPUs where they are vulnerable.  This will affect the second and\n# subsequent boots of the machine, but not the first boot.\nstorage:\n  filesystems:\n    - name: OEM\n      mount:\n        device: /dev/disk/by-label/OEM\n        format: ext4\n  files:\n    - filesystem: OEM\n      path: /grub.cfg\n      append: true\n      mode: 0644\n      contents:\n        inline: |\n          # Disable SMT on CPUs affected by MDS or similar vulnerabilities.\n          # Disable TSX on CPUs affected by TAA but not by MDS.\n          set linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"\n\n# On the first boot only, disable SMT at runtime if it is enabled and\n# the system has an Intel CPU.  L1TF, MDS, and TAA vulnerabilities are\n# limited to Intel CPUs.\nsystemd:\n  units:\n    - name: disable-smt-firstboot.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Disable SMT on first boot on Intel CPUs to mitigate MDS\n        DefaultDependencies=no\n        Before=sysinit.target shutdown.target\n        Conflicts=shutdown.target\n        ConditionFirstBoot=true\n\n        [Service]\n        Type=oneshot\n        ExecStart=/bin/bash -c 'active=\"$(cat /sys/devices/system/cpu/smt/active)\" && if [[ \"$active\" != 0 ]] && grep -q \"vendor_id.*GenuineIntel\" /proc/cpuinfo; then echo \"Disabling SMT.\" && echo off > /sys/devices/system/cpu/smt/control; fi'\n\n        [Install]\n        WantedBy=sysinit.target\n\n\n\nConfiguring existing machines\n\u00b6\n\n\nTo add \nmitigations=auto,nosmt tsx=auto\n to the kernel command line on an existing system, add the following line to \n/usr/share/oem/grub.cfg\n:\n\n\nset linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"\n\n\n\nFor example, using SSH:\n\n\nssh core@node01 'sudo sh -c \"echo \\\"set linux_append=\\\\\\\"\\\\\\$linux_append mitigations=auto,nosmt tsx=auto\\\\\\\"\\\" >> /usr/share/oem/grub.cfg && systemctl reboot\"'\n\n\n\nIf you use locksmith for reboot coordination, replace \nsystemctl reboot\n with \nlocksmithctl send-need-reboot\n.",
            "title": "Disabling SMT"
        },
        {
            "location": "/os/disabling-smt/#disabling-smt-on-flatcar-container-linux",
            "text": "Recent Intel CPU vulnerabilities ( L1TF  and  MDS ) cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.  In addition, the Intel  TAA  vulnerability cannot be fully mitigated without disabling either of SMT or the Transactional Synchronization Extensions (TSX). Disabling TSX generally has less performance impact, so is the preferred approach on systems that don't otherwise need to disable SMT. For compatibility reasons, TSX is enabled by default.  SMT and TSX should be disabled on affected Intel processors under the following circumstances:\n1. A bare-metal host runs untrusted virtual machines, and  other arrangements  have not been made for mitigation.\n2. A bare-metal host runs untrusted code outside a virtual machine.  SMT can be conditionally disabled by passing  mitigations=auto,nosmt  on the kernel command line. This will disable SMT only if required for mitigating a vulnerability. This approach has two caveats:\n1. It does not protect against unknown vulnerabilities in SMT.\n2. It allows future Flatcar Container Linux updates to disable SMT if needed to mitigate new vulnerabilities.  Alternatively, SMT can be unconditionally disabled by passing  nosmt  on the kernel command line. This provides the most protection and avoids possible behavior changes on upgrades, at the cost of a potentially unnecessary reduction in performance.  TSX can be conditionally disabled on vulnerable CPUs by passing  tsx=auto  on the kernel command line, or unconditionally disabled by passing  tsx=off . However, neither setting takes effect on systems affected by MDS, since MDS mitigation automatically protects against TAA as well.  For typical use cases, we recommend enabling the  mitigations=auto,nosmt  and  tsx=auto  command-line options.",
            "title": "Disabling SMT on Flatcar Container Linux"
        },
        {
            "location": "/os/disabling-smt/#configuring-new-machines",
            "text": "The following Container Linux Config performs two tasks:   Adds  mitigations=auto,nosmt tsx=auto  to the kernel command line. This affects the second and subsequent boots of the machine, but not the first boot.  On the first boot, disables SMT at runtime if the system has an Intel processor. This is sufficient to protect against currently-known SMT vulnerabilities until the system is rebooted. After reboot, SMT will be re-enabled if the processor is not actually vulnerable.   # Add kernel command-line arguments to automatically disable SMT or TSX\n# on CPUs where they are vulnerable.  This will affect the second and\n# subsequent boots of the machine, but not the first boot.\nstorage:\n  filesystems:\n    - name: OEM\n      mount:\n        device: /dev/disk/by-label/OEM\n        format: ext4\n  files:\n    - filesystem: OEM\n      path: /grub.cfg\n      append: true\n      mode: 0644\n      contents:\n        inline: |\n          # Disable SMT on CPUs affected by MDS or similar vulnerabilities.\n          # Disable TSX on CPUs affected by TAA but not by MDS.\n          set linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"\n\n# On the first boot only, disable SMT at runtime if it is enabled and\n# the system has an Intel CPU.  L1TF, MDS, and TAA vulnerabilities are\n# limited to Intel CPUs.\nsystemd:\n  units:\n    - name: disable-smt-firstboot.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Disable SMT on first boot on Intel CPUs to mitigate MDS\n        DefaultDependencies=no\n        Before=sysinit.target shutdown.target\n        Conflicts=shutdown.target\n        ConditionFirstBoot=true\n\n        [Service]\n        Type=oneshot\n        ExecStart=/bin/bash -c 'active=\"$(cat /sys/devices/system/cpu/smt/active)\" && if [[ \"$active\" != 0 ]] && grep -q \"vendor_id.*GenuineIntel\" /proc/cpuinfo; then echo \"Disabling SMT.\" && echo off > /sys/devices/system/cpu/smt/control; fi'\n\n        [Install]\n        WantedBy=sysinit.target",
            "title": "Configuring new machines"
        },
        {
            "location": "/os/disabling-smt/#configuring-existing-machines",
            "text": "To add  mitigations=auto,nosmt tsx=auto  to the kernel command line on an existing system, add the following line to  /usr/share/oem/grub.cfg :  set linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"  For example, using SSH:  ssh core@node01 'sudo sh -c \"echo \\\"set linux_append=\\\\\\\"\\\\\\$linux_append mitigations=auto,nosmt tsx=auto\\\\\\\"\\\" >> /usr/share/oem/grub.cfg && systemctl reboot\"'  If you use locksmith for reboot coordination, replace  systemctl reboot  with  locksmithctl send-need-reboot .",
            "title": "Configuring existing machines"
        },
        {
            "location": "/os/install-debugging-tools/",
            "text": "Install debugging tools\n\u00b6\n\n\nYou can use common debugging tools like tcpdump or strace with Toolbox. Using the filesystem of a specified Docker container Toolbox will launch a container with full system privileges including access to system PIDs, network interfaces and other global information. Inside of the toolbox, the machine's filesystem is mounted to \n/media/root\n.\n\n\nQuick debugging\n\u00b6\n\n\nBy default, Toolbox uses the stock Fedora Docker container. To start using it, simply run:\n\n\n/usr/bin/toolbox\n\n\n\nYou're now in the namespace of Fedora and can install any software you'd like via \ndnf\n. For example, if you'd like to use \ntcpdump\n:\n\n\n[root@srv-3qy0p ~]# dnf -y install tcpdump\n[root@srv-3qy0p ~]# tcpdump -i ens3\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on ens3, link-type EN10MB (Ethernet), capture size 65535 bytes\n\n\n\nSpecify a custom Docker image\n\u00b6\n\n\nCreate a \n.toolboxrc\n in the user's home folder to use a specific Docker image:\n\n\n$ cat .toolboxrc\nTOOLBOX_DOCKER_IMAGE=index.example.com/debug\nTOOLBOX_USER=root\n$ /usr/bin/toolbox\nPulling repository index.example.com/debug\n...\n\n\n\nYou can also specify this in a Container Linux Config:\n\n\nstorage:\n  files:\n    - path: /home/core/.toolboxrc\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          TOOLBOX_DOCKER_IMAGE=index.example.com/debug\n          TOOLBOX_DOCKER_TAG=v1\n          TOOLBOX_USER=root\n\n\n\nUnder the hood\n\u00b6\n\n\nBehind the scenes, \ntoolbox\n downloads, prepares and exports the container\nimage you specify (or the default \nfedora\n image), then creates a container\nfrom that extracted image by calling \nsystemd-nspawn\n.  The exported\nimage is retained in\n\n/var/lib/toolbox/[username]-[image name]-[image tag]\n, e.g. the default\nimage run by the \ncore\n user is at \n/var/lib/toolbox/core-fedora-latest\n.  \n\n\nThis means two important things:\n\n\n\n\nChanges made inside the container will persist between sessions\n\n\nThe container filesystem will take up space on disk (a few hundred MiB\nfor the default \nfedora\n container)\n\n\n\n\nSSH directly into a toolbox\n\u00b6\n\n\nAdvanced users can SSH directly into a toolbox by setting up an \n/etc/passwd\n entry:\n\n\nuseradd bob -m -p '*' -s /usr/bin/toolbox -U -G sudo,docker,rkt\n\n\n\nTo test, SSH as bob:\n\n\nssh bob@hostname.example.com\n\n   ______                ____  _____\n  / ____/___  ________  / __ \\/ ___/\n / /   / __ \\/ ___/ _ \\/ / / /\\__ \\\n/ /___/ /_/ / /  /  __/ /_/ /___/ /\n\\____/\\____/_/   \\___/\\____//____/\n[root@srv-3qy0p ~]# dnf -y install emacs-nox\n[root@srv-3qy0p ~]# emacs /media/root/etc/systemd/system/newapp.service",
            "title": "Install debugging tools"
        },
        {
            "location": "/os/install-debugging-tools/#install-debugging-tools",
            "text": "You can use common debugging tools like tcpdump or strace with Toolbox. Using the filesystem of a specified Docker container Toolbox will launch a container with full system privileges including access to system PIDs, network interfaces and other global information. Inside of the toolbox, the machine's filesystem is mounted to  /media/root .",
            "title": "Install debugging tools"
        },
        {
            "location": "/os/install-debugging-tools/#quick-debugging",
            "text": "By default, Toolbox uses the stock Fedora Docker container. To start using it, simply run:  /usr/bin/toolbox  You're now in the namespace of Fedora and can install any software you'd like via  dnf . For example, if you'd like to use  tcpdump :  [root@srv-3qy0p ~]# dnf -y install tcpdump\n[root@srv-3qy0p ~]# tcpdump -i ens3\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on ens3, link-type EN10MB (Ethernet), capture size 65535 bytes",
            "title": "Quick debugging"
        },
        {
            "location": "/os/install-debugging-tools/#specify-a-custom-docker-image",
            "text": "Create a  .toolboxrc  in the user's home folder to use a specific Docker image:  $ cat .toolboxrc\nTOOLBOX_DOCKER_IMAGE=index.example.com/debug\nTOOLBOX_USER=root\n$ /usr/bin/toolbox\nPulling repository index.example.com/debug\n...  You can also specify this in a Container Linux Config:  storage:\n  files:\n    - path: /home/core/.toolboxrc\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          TOOLBOX_DOCKER_IMAGE=index.example.com/debug\n          TOOLBOX_DOCKER_TAG=v1\n          TOOLBOX_USER=root",
            "title": "Specify a custom Docker image"
        },
        {
            "location": "/os/install-debugging-tools/#under-the-hood",
            "text": "Behind the scenes,  toolbox  downloads, prepares and exports the container\nimage you specify (or the default  fedora  image), then creates a container\nfrom that extracted image by calling  systemd-nspawn .  The exported\nimage is retained in /var/lib/toolbox/[username]-[image name]-[image tag] , e.g. the default\nimage run by the  core  user is at  /var/lib/toolbox/core-fedora-latest .    This means two important things:   Changes made inside the container will persist between sessions  The container filesystem will take up space on disk (a few hundred MiB\nfor the default  fedora  container)",
            "title": "Under the hood"
        },
        {
            "location": "/os/install-debugging-tools/#ssh-directly-into-a-toolbox",
            "text": "Advanced users can SSH directly into a toolbox by setting up an  /etc/passwd  entry:  useradd bob -m -p '*' -s /usr/bin/toolbox -U -G sudo,docker,rkt  To test, SSH as bob:  ssh bob@hostname.example.com\n\n   ______                ____  _____\n  / ____/___  ________  / __ \\/ ___/\n / /   / __ \\/ ___/ _ \\/ / / /\\__ \\\n/ /___/ /_/ / /  /  __/ /_/ /___/ /\n\\____/\\____/_/   \\___/\\____//____/\n[root@srv-3qy0p ~]# dnf -y install emacs-nox\n[root@srv-3qy0p ~]# emacs /media/root/etc/systemd/system/newapp.service",
            "title": "SSH directly into a toolbox"
        },
        {
            "location": "/os/btrfs-troubleshooting/",
            "text": "Working with btrfs and common troubleshooting\n\u00b6\n\n\nbtrfs is a copy-on-write filesystem with full support in the upstream Linux kernel and several desirable features. In the past, Flatcar Container Linux shipped with a btrfs root filesystem to support Docker filesystem requirements at the time. As of version 561.0.0, Flatcar Container Linux ships with ext4 as the default root filesystem by default while still supporting Docker. Btrfs is still supported and works with the latest Flatcar Container Linux releases and Docker, but we recommend using ext4.\n\n\nbtrfs was marked as experimental for a long time, but it's now fully production-ready and supported by a number of Linux distributions.\n\n\nNotable Features of btrfs:\n\n\n\n\nAbility to add/remove block devices without interruption\n\n\nAbility to balance the filesystem without interruption\n\n\nRAID 0, RAID 1, RAID 5, RAID 6 and RAID 10\n\n\nSnapshots and file cloning\n\n\n\n\nThis guide won't cover these topics \u2014 it's mostly focused on troubleshooting.\n\n\nFor a more complete troubleshooting experience, let's explore how btrfs works under the hood.\n\n\nbtrfs stores data in chunks across all of the block devices on the system. The total storage across these devices is shown in the standard output of \ndf -h\n.\n\n\nRaw data and filesystem metadata are stored in one or many chunks, typically ~1GiB in size. When RAID is configured, these chunks are replicated instead of individual files.\n\n\nA copy-on-write filesystem maintains many changes of a single file, which is helpful for snapshotting and other advanced features, but can lead to fragmentation with some workloads.\n\n\nNo space left on device\n\u00b6\n\n\nWhen the filesystem is out of chunks to write data into, \nNo space left on device\n will be reported. This will prevent journal files from being recorded, containers from starting and so on.\n\n\nThe common reaction to this error is to run \ndf -h\n and you'll see that there is still some free space. That command isn't measuring the btrfs primitives (chunks, metadata, etc), which is what really matters.\n\n\nRunning \nsudo btrfs fi show\n will give you the btrfs view of how much free space you have. When starting/stopping many Docker containers or doing a large amount of random writes, chunks will become duplicated in an inefficient manner over time.\n\n\nRe-balancing the filesystem (\nofficial btrfs docs\n) will relocate data from empty or near-empty chunks to free up space. This operation can be done without downtime.\n\n\nFirst, let's see how much free space we have:\n\n\n$ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.44GiB\n  devid    1 size 32.68GiB used 32.68GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414\n\n\n\nThe answer: not a lot. We can re-balance to fix that.\n\n\nThe re-balance command can be configured to only relocate data in chunks up to a certain percentage used. This will prevent you from moving around a lot of data without a lot of benefit. If your disk is completely full, you may need to delete a few containers to create space for the re-balance operation to work with.\n\n\nLet's try to relocate chunks with less than 5% of usage:\n\n\n$ sudo btrfs fi balance start -dusage=5 /\nDone, had to relocate 5 out of 45 chunks\n$ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.39GiB\n  devid    1 size 32.68GiB used 28.93GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414\n\n\n\nThe operation took about a minute on a cloud server and gained us 4GiB of space on the filesystem. It's up to you to find out what percentage works best for your workload, the speed of your disks, etc.\n\n\nIf your balance operation is taking a long time, you can open a new shell and find the status:\n\n\n$ sudo btrfs balance status /\nBalance on '/' is running\n0 out of about 1 chunks balanced (1 considered), 100% left\n\n\n\nAdding a new physical disk\n\u00b6\n\n\nNew physical disks can be added to an existing btrfs filesystem. The first step is to have the new block device \nmounted on the machine\n. Afterwards, let btrfs know about the new device and re-balance the file system. The key step here is re-balancing, which will move the data and metadata across both block devices. Expect this process to take some time:\n\n\n$ btrfs device add /dev/sdc /\n$ btrfs filesystem balance /\n\n\n\nDisable copy-on-write\n\u00b6\n\n\nCopy-On-write isn't ideal for workloads that create or modify many small files, such as databases. Without disabling COW, you can heavily fragment the file system as explained above.\n\n\nThe best strategy for successfully running a database in a container is to disable COW on directory/volume that is mounted into the container.\n\n\nThe COW setting is stored as a file attribute and is modified with a utility called \nchattr\n. To disable COW for a MySQL container's volume, run:\n\n\n$ sudo mkdir /var/lib/mysql\n$ sudo chattr -R +C /var/lib/mysql\n\n\n\nThe directory \n/var/lib/mysql\n is now ready to be used by a Docker container without COW. Let's break down the command:\n\n\n-R\n indicates that want to recursively change the file attribute\n\n+C\n means we want to set the NOCOW attribute on the file/directory\n\n\nTo verify, we can run:\n\n\n$ sudo lsattr /var/lib/\n---------------- /var/lib/portage\n---------------- /var/lib/gentoo\n---------------- /var/lib/iptables\n---------------- /var/lib/ip6tables\n---------------- /var/lib/arpd\n---------------- /var/lib/ipset\n---------------- /var/lib/dbus\n---------------- /var/lib/systemd\n---------------- /var/lib/polkit-1\n---------------- /var/lib/dhcpcd\n---------------- /var/lib/ntp\n---------------- /var/lib/nfs\n---------------- /var/lib/etcd\n---------------- /var/lib/docker\n---------------- /var/lib/update_engine\n---------------C /var/lib/mysql\n\n\n\nDisable via a unit file\n\u00b6\n\n\nSetting the file attributes can be done via a systemd unit using two \nExecStartPre\n commands:\n\n\nExecStartPre=/usr/bin/mkdir -p /var/lib/mysql\nExecStartPre=/usr/bin/chattr -R +C /var/lib/mysql",
            "title": "Working with btrfs"
        },
        {
            "location": "/os/btrfs-troubleshooting/#working-with-btrfs-and-common-troubleshooting",
            "text": "btrfs is a copy-on-write filesystem with full support in the upstream Linux kernel and several desirable features. In the past, Flatcar Container Linux shipped with a btrfs root filesystem to support Docker filesystem requirements at the time. As of version 561.0.0, Flatcar Container Linux ships with ext4 as the default root filesystem by default while still supporting Docker. Btrfs is still supported and works with the latest Flatcar Container Linux releases and Docker, but we recommend using ext4.  btrfs was marked as experimental for a long time, but it's now fully production-ready and supported by a number of Linux distributions.  Notable Features of btrfs:   Ability to add/remove block devices without interruption  Ability to balance the filesystem without interruption  RAID 0, RAID 1, RAID 5, RAID 6 and RAID 10  Snapshots and file cloning   This guide won't cover these topics \u2014 it's mostly focused on troubleshooting.  For a more complete troubleshooting experience, let's explore how btrfs works under the hood.  btrfs stores data in chunks across all of the block devices on the system. The total storage across these devices is shown in the standard output of  df -h .  Raw data and filesystem metadata are stored in one or many chunks, typically ~1GiB in size. When RAID is configured, these chunks are replicated instead of individual files.  A copy-on-write filesystem maintains many changes of a single file, which is helpful for snapshotting and other advanced features, but can lead to fragmentation with some workloads.",
            "title": "Working with btrfs and common troubleshooting"
        },
        {
            "location": "/os/btrfs-troubleshooting/#no-space-left-on-device",
            "text": "When the filesystem is out of chunks to write data into,  No space left on device  will be reported. This will prevent journal files from being recorded, containers from starting and so on.  The common reaction to this error is to run  df -h  and you'll see that there is still some free space. That command isn't measuring the btrfs primitives (chunks, metadata, etc), which is what really matters.  Running  sudo btrfs fi show  will give you the btrfs view of how much free space you have. When starting/stopping many Docker containers or doing a large amount of random writes, chunks will become duplicated in an inefficient manner over time.  Re-balancing the filesystem ( official btrfs docs ) will relocate data from empty or near-empty chunks to free up space. This operation can be done without downtime.  First, let's see how much free space we have:  $ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.44GiB\n  devid    1 size 32.68GiB used 32.68GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414  The answer: not a lot. We can re-balance to fix that.  The re-balance command can be configured to only relocate data in chunks up to a certain percentage used. This will prevent you from moving around a lot of data without a lot of benefit. If your disk is completely full, you may need to delete a few containers to create space for the re-balance operation to work with.  Let's try to relocate chunks with less than 5% of usage:  $ sudo btrfs fi balance start -dusage=5 /\nDone, had to relocate 5 out of 45 chunks\n$ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.39GiB\n  devid    1 size 32.68GiB used 28.93GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414  The operation took about a minute on a cloud server and gained us 4GiB of space on the filesystem. It's up to you to find out what percentage works best for your workload, the speed of your disks, etc.  If your balance operation is taking a long time, you can open a new shell and find the status:  $ sudo btrfs balance status /\nBalance on '/' is running\n0 out of about 1 chunks balanced (1 considered), 100% left",
            "title": "No space left on device"
        },
        {
            "location": "/os/btrfs-troubleshooting/#adding-a-new-physical-disk",
            "text": "New physical disks can be added to an existing btrfs filesystem. The first step is to have the new block device  mounted on the machine . Afterwards, let btrfs know about the new device and re-balance the file system. The key step here is re-balancing, which will move the data and metadata across both block devices. Expect this process to take some time:  $ btrfs device add /dev/sdc /\n$ btrfs filesystem balance /",
            "title": "Adding a new physical disk"
        },
        {
            "location": "/os/btrfs-troubleshooting/#disable-copy-on-write",
            "text": "Copy-On-write isn't ideal for workloads that create or modify many small files, such as databases. Without disabling COW, you can heavily fragment the file system as explained above.  The best strategy for successfully running a database in a container is to disable COW on directory/volume that is mounted into the container.  The COW setting is stored as a file attribute and is modified with a utility called  chattr . To disable COW for a MySQL container's volume, run:  $ sudo mkdir /var/lib/mysql\n$ sudo chattr -R +C /var/lib/mysql  The directory  /var/lib/mysql  is now ready to be used by a Docker container without COW. Let's break down the command:  -R  indicates that want to recursively change the file attribute +C  means we want to set the NOCOW attribute on the file/directory  To verify, we can run:  $ sudo lsattr /var/lib/\n---------------- /var/lib/portage\n---------------- /var/lib/gentoo\n---------------- /var/lib/iptables\n---------------- /var/lib/ip6tables\n---------------- /var/lib/arpd\n---------------- /var/lib/ipset\n---------------- /var/lib/dbus\n---------------- /var/lib/systemd\n---------------- /var/lib/polkit-1\n---------------- /var/lib/dhcpcd\n---------------- /var/lib/ntp\n---------------- /var/lib/nfs\n---------------- /var/lib/etcd\n---------------- /var/lib/docker\n---------------- /var/lib/update_engine\n---------------C /var/lib/mysql",
            "title": "Disable copy-on-write"
        },
        {
            "location": "/os/btrfs-troubleshooting/#disable-via-a-unit-file",
            "text": "Setting the file attributes can be done via a systemd unit using two  ExecStartPre  commands:  ExecStartPre=/usr/bin/mkdir -p /var/lib/mysql\nExecStartPre=/usr/bin/chattr -R +C /var/lib/mysql",
            "title": "Disable via a unit file"
        },
        {
            "location": "/os/reading-the-system-log/",
            "text": "Reading the system log\n\u00b6\n\n\njournalctl\n is your interface into a single machine's journal/logging. All service files insert data into the systemd journal. There are a few helpful commands to read the journal:\n\n\nRead the entire journal\n\u00b6\n\n\n$ journalctl\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:28:45 UTC. --\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 184.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 188.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuset\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpu\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuacct\nDec 22 00:10:21 localhost kernel: Linux version 3.11.7+ (buildbot@10.10.10.10) (gcc version 4.6.3 (Gentoo Hardened 4.6.3 p1.13, pie-0.5.2)\n...\n1000s more lines\n\n\n\nRead entries for a specific service\n\u00b6\n\n\nRead entries generated by a specific unit:\n\n\n$ journalctl -u apache.service\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:32:52 UTC. --\nDec 22 12:32:39 localhost systemd[1]: Starting Apache Service...\nDec 22 12:32:39 localhost systemd[1]: Started Apache Service.\nDec 22 12:32:39 localhost docker[9772]: /usr/sbin/apache2ctl: 87: ulimit: error setting limit (Operation not permitted)\nDec 22 12:32:39 localhost docker[9772]: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.6 for ServerName\n\n\n\nRead entries since boot\n\u00b6\n\n\nReading just the entries since the last boot is an easy way to troubleshoot services that are failing to start properly:\n\n\njournalctl --boot\n\n\n\nTail the journal\n\u00b6\n\n\nYou can tail the entire journal or just a specific service:\n\n\njournalctl -f\n\n\n\njournalctl -u apache.service -f\n\n\n\nRead entries with line wrapping\n\u00b6\n\n\nBy default \njournalctl\n passes \nFRSXMK\n command line options to \nless\n. You can override these options by setting a custom \nSYSTEMD_LESS\n environment variable with omitted \nS\n option:\n\n\nSYSTEMD_LESS=FRXMK journalctl\n\n\n\nRead logs without pager:\n\n\njournalctl --no-pager\n\n\n\nDebugging journald\n\u00b6\n\n\nIf you've faced some problems with journald you can enable debug mode following the instructions below.\n\n\nEnable debugging manually\n\u00b6\n\n\nmkdir -p /etc/systemd/system/systemd-journald.service.d/\n\n\n\nCreate \nDrop-In\n \n/etc/systemd/system/systemd-journald.service.d/10-debug.conf\n with following content:\n\n\n[Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nAnd restart \nsystemd-journald\n service:\n\n\nsystemctl daemon-reload\nsystemctl restart systemd-journald\ndmesg | grep systemd-journald\n\n\n\nEnable debugging via a Container Linux Config\n\u00b6\n\n\nDefine a \nDrop-In\n in a \nContainer Linux Config\n:\n\n\nsystemd:\n  units:\n    - name: systemd-journald.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nMore information\n\u00b6\n\n\nGetting Started with systemd\n\n\nNetwork Configuration with networkd",
            "title": "Reading the system log"
        },
        {
            "location": "/os/reading-the-system-log/#reading-the-system-log",
            "text": "journalctl  is your interface into a single machine's journal/logging. All service files insert data into the systemd journal. There are a few helpful commands to read the journal:",
            "title": "Reading the system log"
        },
        {
            "location": "/os/reading-the-system-log/#read-the-entire-journal",
            "text": "$ journalctl\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:28:45 UTC. --\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 184.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 188.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuset\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpu\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuacct\nDec 22 00:10:21 localhost kernel: Linux version 3.11.7+ (buildbot@10.10.10.10) (gcc version 4.6.3 (Gentoo Hardened 4.6.3 p1.13, pie-0.5.2)\n...\n1000s more lines",
            "title": "Read the entire journal"
        },
        {
            "location": "/os/reading-the-system-log/#read-entries-for-a-specific-service",
            "text": "Read entries generated by a specific unit:  $ journalctl -u apache.service\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:32:52 UTC. --\nDec 22 12:32:39 localhost systemd[1]: Starting Apache Service...\nDec 22 12:32:39 localhost systemd[1]: Started Apache Service.\nDec 22 12:32:39 localhost docker[9772]: /usr/sbin/apache2ctl: 87: ulimit: error setting limit (Operation not permitted)\nDec 22 12:32:39 localhost docker[9772]: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.6 for ServerName",
            "title": "Read entries for a specific service"
        },
        {
            "location": "/os/reading-the-system-log/#read-entries-since-boot",
            "text": "Reading just the entries since the last boot is an easy way to troubleshoot services that are failing to start properly:  journalctl --boot",
            "title": "Read entries since boot"
        },
        {
            "location": "/os/reading-the-system-log/#tail-the-journal",
            "text": "You can tail the entire journal or just a specific service:  journalctl -f  journalctl -u apache.service -f",
            "title": "Tail the journal"
        },
        {
            "location": "/os/reading-the-system-log/#read-entries-with-line-wrapping",
            "text": "By default  journalctl  passes  FRSXMK  command line options to  less . You can override these options by setting a custom  SYSTEMD_LESS  environment variable with omitted  S  option:  SYSTEMD_LESS=FRXMK journalctl  Read logs without pager:  journalctl --no-pager",
            "title": "Read entries with line wrapping"
        },
        {
            "location": "/os/reading-the-system-log/#debugging-journald",
            "text": "If you've faced some problems with journald you can enable debug mode following the instructions below.",
            "title": "Debugging journald"
        },
        {
            "location": "/os/reading-the-system-log/#enable-debugging-manually",
            "text": "mkdir -p /etc/systemd/system/systemd-journald.service.d/  Create  Drop-In   /etc/systemd/system/systemd-journald.service.d/10-debug.conf  with following content:  [Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug  And restart  systemd-journald  service:  systemctl daemon-reload\nsystemctl restart systemd-journald\ndmesg | grep systemd-journald",
            "title": "Enable debugging manually"
        },
        {
            "location": "/os/reading-the-system-log/#enable-debugging-via-a-container-linux-config",
            "text": "Define a  Drop-In  in a  Container Linux Config :  systemd:\n  units:\n    - name: systemd-journald.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug",
            "title": "Enable debugging via a Container Linux Config"
        },
        {
            "location": "/os/reading-the-system-log/#more-information",
            "text": "Getting Started with systemd  Network Configuration with networkd",
            "title": "More information"
        },
        {
            "location": "/os/collecting-crash-logs/",
            "text": "Collecting crash logs\n\u00b6\n\n\nIn the unfortunate case that an OS crashes, it's often extremely helpful to gather information about the event. There are two popular tools used to accomplished this goal: kdump and pstore. Flatcar Container Linux relies on pstore, a persistent storage abstraction provided by the Linux kernel, to store logs in the event of a kernel panic. Since this mechanism is just an abstraction, it depends on hardware support to actually persist the data across reboots. If the hardware support is absent, the pstore will remain empty. On AMD64 machines, pstore is typically backed by the ACPI error record serialization table (ERST).\n\n\nUsing pstore\n\u00b6\n\n\nOn Flatcar Container Linux, the pstore is automatically mounted to \n/sys/fs/pstore\n. The contents of the store can be explored using standard filesystem tools:\n\n\n$ ls /sys/fs/pstore/\n\n\n\nOn this particular machine, there isn't anything in the pstore yet. In order to test the mechanism, a kernel panic can be triggered:\n\n\n$ echo c > /proc/sysrq-trigger\n\n\n\nOnce the machine boots, the pstore can again be inspected:\n\n\n$ ls /sys/fs/pstore/\ndmesg-erst-6319986351055831041  dmesg-erst-6319986351055831044\ndmesg-erst-6319986351055831042  dmesg-erst-6319986351055831045\ndmesg-erst-6319986351055831043\n\n\n\nNow there are a series of dmesg logs, stored in the ACPI ERST. Looking at the first file, the cause of the panic can be discovered:\n\n\n$ cat /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n...\n<6>[  201.650687] sysrq: SysRq : Trigger a crash\n<1>[  201.654822] BUG: unable to handle kernel NULL pointer dereference at           (null)\n<1>[  201.662670] IP: [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.668783] PGD 0 \n<4>[  201.670809] Oops: 0002 [#1] SMP\n<4>[  201.673948] Modules linked in: coretemp sb_edac edac_core x86_pkg_temp_thermal kvm_intel ipmi_ssif kvm mei_me irqbypass i2c_i801 mousedev evdev mei ipmi_si ipmi_msghandler tpm_tis button tpm sch_fq_codel ip_tables hid_generic usbhid hid sd_mod squashfs loop igb ahci xhci_pci ehci_pci i2c_algo_bit libahci xhci_hcd ehci_hcd i2c_core libata i40e hwmon usbcore ptp crc32c_intel scsi_mod usb_common pps_core dm_mirror dm_region_hash dm_log dm_mod autofs4\n<4>[  201.714354] CPU: 0 PID: 1899 Comm: bash Not tainted 4.7.0-coreos #1\n<4>[  201.720612] Hardware name: Supermicro SYS-F618R3-FT/X10DRFF, BIOS 1.0b 01/07/2015\n<4>[  201.728083] task: ffff881fdca79d40 ti: ffff881fd92d0000 task.ti: ffff881fd92d0000\n<4>[  201.735553] RIP: 0010:[<ffffffffbd3d1956>]  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.744083] RSP: 0018:ffff881fd92d3d98  EFLAGS: 00010286\n<4>[  201.749388] RAX: 000000000000000f RBX: 0000000000000063 RCX: 0000000000000000\n<4>[  201.756511] RDX: 0000000000000000 RSI: ffff881fff80dbc8 RDI: 0000000000000063\n<4>[  201.763635] RBP: ffff881fd92d3d98 R08: ffff88407ff57b80 R09: 00000000000000c2\n<4>[  201.770759] R10: ffff881fe4fab624 R11: 00000000000005dd R12: 0000000000000007\n<4>[  201.777885] R13: 0000000000000000 R14: ffffffffbdac37a0 R15: 0000000000000000\n<4>[  201.785009] FS:  00007fa68acee700(0000) GS:ffff881fff800000(0000) knlGS:0000000000000000\n<4>[  201.793085] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n<4>[  201.798825] CR2: 0000000000000000 CR3: 0000001fdcc97000 CR4: 00000000001406f0\n<4>[  201.805949] Stack:\n<4>[  201.807961]  ffff881fd92d3dc8 ffffffffbd3d2146 0000000000000002 fffffffffffffffb\n<4>[  201.815413]  00007fa68acf6000 ffff883fe2e46f00 ffff881fd92d3de0 ffffffffbd3d259f\n<4>[  201.822866]  ffff881fe4fab5c0 ffff881fd92d3e00 ffffffffbd24fda8 ffff883fe2e46f00\n<4>[  201.830320] Call Trace:\n<4>[  201.832769]  [<ffffffffbd3d2146>] __handle_sysrq+0xf6/0x150\n<4>[  201.838331]  [<ffffffffbd3d259f>] write_sysrq_trigger+0x2f/0x40\n<4>[  201.844244]  [<ffffffffbd24fda8>] proc_reg_write+0x48/0x70\n<4>[  201.849723]  [<ffffffffbd1e4697>] __vfs_write+0x37/0x140\n<4>[  201.855038]  [<ffffffffbd283e0d>] ? security_file_permission+0x3d/0xc0\n<4>[  201.861561]  [<ffffffffbd0c1062>] ? percpu_down_read+0x12/0x60\n<4>[  201.867383]  [<ffffffffbd1e55b8>] vfs_write+0xb8/0x1a0\n<4>[  201.872514]  [<ffffffffbd1e6a25>] SyS_write+0x55/0xc0\n<4>[  201.877562]  [<ffffffffbd003c6d>] do_syscall_64+0x5d/0x150\n<4>[  201.883047]  [<ffffffffbd58e161>] entry_SYSCALL64_slow_path+0x25/0x25\n<4>[  201.889474] Code: df ff 48 c7 c7 f3 a3 7d bd e8 47 c5 d3 ff e9 de fe ff ff 66 90 0f 1f 44 00 00 55 c7 05 48 b4 66 00 01 00 00 00 48 89 e5 0f ae f8 <c6> 04 25 00 00 00 00 01 5d c3 0f 1f 44 00 00 55 31 c0 c7 05 5e \n<1>[  201.909425] RIP  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.915615]  RSP <ffff881fd92d3d98>\n<4>[  201.919097] CR2: 0000000000000000\n<4>[  201.922450] ---[ end trace 8794939ba0598b91 ]---\n\n\n\nThe cause of the panic was a system request! The remaining files in the pstore contain more of the logs leading up to the panic as well as more context. Each of the files has a small, descriptive header describing the source of the logs. Looking at each of the headers shows the rough structure of the logs:\n\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831042\nOops#1 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831043\nPanic#2 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831044\nPanic#2 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831045\nPanic#2 Part3\n\n\n\nIt is important to note that the pstore typically has very limited storage space (on the order of kilobytes) and will not overwrite entries when out of space. The files in \n/sys/fs/pstore\n must be removed to free up space. The typical approach is to move the files from the pstore to a more permanent storage location on boot, but Flatcar Container Linux will not do this automatically for you.",
            "title": "Collecting crash logs"
        },
        {
            "location": "/os/collecting-crash-logs/#collecting-crash-logs",
            "text": "In the unfortunate case that an OS crashes, it's often extremely helpful to gather information about the event. There are two popular tools used to accomplished this goal: kdump and pstore. Flatcar Container Linux relies on pstore, a persistent storage abstraction provided by the Linux kernel, to store logs in the event of a kernel panic. Since this mechanism is just an abstraction, it depends on hardware support to actually persist the data across reboots. If the hardware support is absent, the pstore will remain empty. On AMD64 machines, pstore is typically backed by the ACPI error record serialization table (ERST).",
            "title": "Collecting crash logs"
        },
        {
            "location": "/os/collecting-crash-logs/#using-pstore",
            "text": "On Flatcar Container Linux, the pstore is automatically mounted to  /sys/fs/pstore . The contents of the store can be explored using standard filesystem tools:  $ ls /sys/fs/pstore/  On this particular machine, there isn't anything in the pstore yet. In order to test the mechanism, a kernel panic can be triggered:  $ echo c > /proc/sysrq-trigger  Once the machine boots, the pstore can again be inspected:  $ ls /sys/fs/pstore/\ndmesg-erst-6319986351055831041  dmesg-erst-6319986351055831044\ndmesg-erst-6319986351055831042  dmesg-erst-6319986351055831045\ndmesg-erst-6319986351055831043  Now there are a series of dmesg logs, stored in the ACPI ERST. Looking at the first file, the cause of the panic can be discovered:  $ cat /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n...\n<6>[  201.650687] sysrq: SysRq : Trigger a crash\n<1>[  201.654822] BUG: unable to handle kernel NULL pointer dereference at           (null)\n<1>[  201.662670] IP: [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.668783] PGD 0 \n<4>[  201.670809] Oops: 0002 [#1] SMP\n<4>[  201.673948] Modules linked in: coretemp sb_edac edac_core x86_pkg_temp_thermal kvm_intel ipmi_ssif kvm mei_me irqbypass i2c_i801 mousedev evdev mei ipmi_si ipmi_msghandler tpm_tis button tpm sch_fq_codel ip_tables hid_generic usbhid hid sd_mod squashfs loop igb ahci xhci_pci ehci_pci i2c_algo_bit libahci xhci_hcd ehci_hcd i2c_core libata i40e hwmon usbcore ptp crc32c_intel scsi_mod usb_common pps_core dm_mirror dm_region_hash dm_log dm_mod autofs4\n<4>[  201.714354] CPU: 0 PID: 1899 Comm: bash Not tainted 4.7.0-coreos #1\n<4>[  201.720612] Hardware name: Supermicro SYS-F618R3-FT/X10DRFF, BIOS 1.0b 01/07/2015\n<4>[  201.728083] task: ffff881fdca79d40 ti: ffff881fd92d0000 task.ti: ffff881fd92d0000\n<4>[  201.735553] RIP: 0010:[<ffffffffbd3d1956>]  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.744083] RSP: 0018:ffff881fd92d3d98  EFLAGS: 00010286\n<4>[  201.749388] RAX: 000000000000000f RBX: 0000000000000063 RCX: 0000000000000000\n<4>[  201.756511] RDX: 0000000000000000 RSI: ffff881fff80dbc8 RDI: 0000000000000063\n<4>[  201.763635] RBP: ffff881fd92d3d98 R08: ffff88407ff57b80 R09: 00000000000000c2\n<4>[  201.770759] R10: ffff881fe4fab624 R11: 00000000000005dd R12: 0000000000000007\n<4>[  201.777885] R13: 0000000000000000 R14: ffffffffbdac37a0 R15: 0000000000000000\n<4>[  201.785009] FS:  00007fa68acee700(0000) GS:ffff881fff800000(0000) knlGS:0000000000000000\n<4>[  201.793085] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n<4>[  201.798825] CR2: 0000000000000000 CR3: 0000001fdcc97000 CR4: 00000000001406f0\n<4>[  201.805949] Stack:\n<4>[  201.807961]  ffff881fd92d3dc8 ffffffffbd3d2146 0000000000000002 fffffffffffffffb\n<4>[  201.815413]  00007fa68acf6000 ffff883fe2e46f00 ffff881fd92d3de0 ffffffffbd3d259f\n<4>[  201.822866]  ffff881fe4fab5c0 ffff881fd92d3e00 ffffffffbd24fda8 ffff883fe2e46f00\n<4>[  201.830320] Call Trace:\n<4>[  201.832769]  [<ffffffffbd3d2146>] __handle_sysrq+0xf6/0x150\n<4>[  201.838331]  [<ffffffffbd3d259f>] write_sysrq_trigger+0x2f/0x40\n<4>[  201.844244]  [<ffffffffbd24fda8>] proc_reg_write+0x48/0x70\n<4>[  201.849723]  [<ffffffffbd1e4697>] __vfs_write+0x37/0x140\n<4>[  201.855038]  [<ffffffffbd283e0d>] ? security_file_permission+0x3d/0xc0\n<4>[  201.861561]  [<ffffffffbd0c1062>] ? percpu_down_read+0x12/0x60\n<4>[  201.867383]  [<ffffffffbd1e55b8>] vfs_write+0xb8/0x1a0\n<4>[  201.872514]  [<ffffffffbd1e6a25>] SyS_write+0x55/0xc0\n<4>[  201.877562]  [<ffffffffbd003c6d>] do_syscall_64+0x5d/0x150\n<4>[  201.883047]  [<ffffffffbd58e161>] entry_SYSCALL64_slow_path+0x25/0x25\n<4>[  201.889474] Code: df ff 48 c7 c7 f3 a3 7d bd e8 47 c5 d3 ff e9 de fe ff ff 66 90 0f 1f 44 00 00 55 c7 05 48 b4 66 00 01 00 00 00 48 89 e5 0f ae f8 <c6> 04 25 00 00 00 00 01 5d c3 0f 1f 44 00 00 55 31 c0 c7 05 5e \n<1>[  201.909425] RIP  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.915615]  RSP <ffff881fd92d3d98>\n<4>[  201.919097] CR2: 0000000000000000\n<4>[  201.922450] ---[ end trace 8794939ba0598b91 ]---  The cause of the panic was a system request! The remaining files in the pstore contain more of the logs leading up to the panic as well as more context. Each of the files has a small, descriptive header describing the source of the logs. Looking at each of the headers shows the rough structure of the logs:  $ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831042\nOops#1 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831043\nPanic#2 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831044\nPanic#2 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831045\nPanic#2 Part3  It is important to note that the pstore typically has very limited storage space (on the order of kilobytes) and will not overwrite entries when out of space. The files in  /sys/fs/pstore  must be removed to free up space. The typical approach is to move the files from the pstore to a more permanent storage location on boot, but Flatcar Container Linux will not do this automatically for you.",
            "title": "Using pstore"
        },
        {
            "location": "/os/manual-rollbacks/",
            "text": "Performing manual Flatcar Container Linux rollbacks\n\u00b6\n\n\nIn the event of an upgrade failure, Flatcar Container Linux will automatically boot with the version on the rollback partition. Immediately after an upgrade reboot, the active version of Flatcar Container Linux can be rolled back to the version installed on the rollback partition, or downgraded to the version current on any lower release channel. There is no method to downgrade to an arbitrary version number.\n\n\nThis section describes the automated upgrade process, performing a manual rollback, and forcing a channel downgrade.\n\n\nNote:\n Neither performing a manual rollback nor forcing a channel downgrade are recommended.\n\n\nHow do updates work?\n\u00b6\n\n\nThe system's GPT tables are used to encode which partition is currently active and which is passive. This can be seen using the \ncgpt\n command.\n\n\n$ cgpt show /dev/sda\n       start        size    part  contents\n           0           1          Hybrid MBR\n           1           1          Pri GPT header\n           2          32          Pri GPT table\n        4096      262144       1  Label: \"EFI-SYSTEM\"\n                                  Type: EFI System Partition\n                                  UUID: 596FF08E-5617-4497-B10B-27A23F658B73\n                                  Attr: Legacy BIOS Bootable\n      266240        4096       2  Label: \"BIOS-BOOT\"\n                                  Type: BIOS Boot Partition\n                                  UUID: EACCC3D5-E7E9-461D-A6E2-1DCDAE4671EC\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0\n     4464640      262144       6  Label: \"OEM\"\n                                  Type: Alias for linux-data\n                                  UUID: 726E33FA-DFE9-45B2-B215-FB35CD9C2388\n     4726784      131072       7  Label: \"OEM-CONFIG\"\n                                  Type: Flatcar Container Linux reserved\n                                  UUID: 8F39CE8B-1FB3-4E7E-A784-0C53C8F40442\n     4857856    37085151       9  Label: \"ROOT\"\n                                  Type: Flatcar Container Linux auto-resize\n                                  UUID: D9A972BB-8084-4AB5-BA55-F8A3AFFAD70D\n    41943007          32          Sec GPT table\n    41943039           1          Sec GPT header\n\n\n\nLooking specifically at \"USR-A\" and \"USR-B\", we see that \"USR-A\" is the active USR partition (this is what's actually mounted at /usr). Its priority is higher than that of \"USR-B\". When the system boots, GRUB (the bootloader) looks at the priorities, tries, and successful flags to determine which partition to use.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0\n\n\n\nYou'll notice that on this machine, \"USR-B\" hasn't actually successfully booted. Not to worry! This is a fresh machine that hasn't been through an update cycle yet. When the machine downloads an update, the partition table is updated to allow the newer image to boot.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=1 successful=0\n\n\n\nIn this case, we see that \"USR-B\" now has a higher priority and it has one try to successfully boot. Once the machine reboots, the partition table will again be updated.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=0\n\n\n\nNow we see that the number of tries for \"USR-B\" has been decremented to zero. The successful flag still hasn't been updated though. Once update-engine has had a chance to run, it marks the boot as being successful.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=1\n\n\n\nPerforming a manual rollback\n\u00b6\n\n\nSo, now that we understand what happens when the machine updates, we can tweak the process so that it boots an older image (assuming it's still intact on the passive partition). The first command we'll use is \ncgpt find -t flatcar-usr\n. This will give us a list of all of the USR partitions available on the disk.\n\n\n$ cgpt find -t flatcar-usr\n/dev/sda3\n/dev/sda4\n\n\n\nTo figure out which partition is currently active, we can use \nrootdev\n.\n\n\n$ rootdev -s /usr\n/dev/sda4\n\n\n\nSo now we know that \n/dev/sda3\n is the passive partition on our system. We can compose the previous two commands to dynamically figure out the passive partition.\n\n\n$ cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\"\n/dev/sda3\n\n\n\nIn order to rollback, we need to mark that partition as active using \ncgpt prioritize\n.\n\n\n$ cgpt prioritize /dev/sda3\n\n\n\nIf we take another look at the GPT tables, we'll see that the priorities have been updated.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=1\n\n\n\n\nComposing the previous two commands produces the following command to set the currently passive partition to be active on the next boot:\n\n\n$ cgpt prioritize \"$(cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\")\"\n\n\n\nForcing a Channel Downgrade\n\u00b6\n\n\nThe procedure above restores the last known good Flatcar Container Linux version from immediately before an upgrade reboot. The system remains on the same \nFlatcar Container Linux channel\n after rebooting with the previous USR partition. It is also possible, though not recommended, to switch a Flatcar Container Linux installation to an older release channel, for example to make a system running an Alpha release downgrade to the Stable channel. Root privileges are required for this procedure, noted by \nsudo\n in the commands below.\n\n\nFirst, edit \n/etc/coreos/update.conf\n to set \nGROUP\n to the name of the target channel, one of \nstable\n or \nbeta\n:\n\n\nGROUP=stable\n\n\n\nNext, clear the current version number from the \nrelease\n file so that the target channel will be certain to have a higher version number, triggering the \"upgrade,\" in this case a downgrade to the lower channel. Since \nrelease\n is on a read-only file system, it is convenient to temporarily override it with a bind mount. To do this, copy the original to a writable location, then bind the copy over the system \nrelease\n file:\n\n\n$ cp /usr/share/coreos/release /tmp\n$ sudo mount -o bind /tmp/release /usr/share/coreos/release\n\n\n\nThe file is now writable, but the bind mount will not survive the reboot, so that the default read-only system \nrelease\n file will be restored after this procedure is complete. Edit \n/usr/share/coreos/release\n to replace the value of \nCOREOS_RELEASE_VERSION\n with \n0.0.0\n:\n\n\nCOREOS_RELEASE_VERSION=0.0.0\n\n\n\nRestart the update service so that it rescans the edited configuration, then initiate an update. The system will reboot into the selected lower channel after downloading the release:\n\n\n$ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Manual Flatcar Container Linux rollbacks"
        },
        {
            "location": "/os/manual-rollbacks/#performing-manual-flatcar-container-linux-rollbacks",
            "text": "In the event of an upgrade failure, Flatcar Container Linux will automatically boot with the version on the rollback partition. Immediately after an upgrade reboot, the active version of Flatcar Container Linux can be rolled back to the version installed on the rollback partition, or downgraded to the version current on any lower release channel. There is no method to downgrade to an arbitrary version number.  This section describes the automated upgrade process, performing a manual rollback, and forcing a channel downgrade.  Note:  Neither performing a manual rollback nor forcing a channel downgrade are recommended.",
            "title": "Performing manual Flatcar Container Linux rollbacks"
        },
        {
            "location": "/os/manual-rollbacks/#how-do-updates-work",
            "text": "The system's GPT tables are used to encode which partition is currently active and which is passive. This can be seen using the  cgpt  command.  $ cgpt show /dev/sda\n       start        size    part  contents\n           0           1          Hybrid MBR\n           1           1          Pri GPT header\n           2          32          Pri GPT table\n        4096      262144       1  Label: \"EFI-SYSTEM\"\n                                  Type: EFI System Partition\n                                  UUID: 596FF08E-5617-4497-B10B-27A23F658B73\n                                  Attr: Legacy BIOS Bootable\n      266240        4096       2  Label: \"BIOS-BOOT\"\n                                  Type: BIOS Boot Partition\n                                  UUID: EACCC3D5-E7E9-461D-A6E2-1DCDAE4671EC\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0\n     4464640      262144       6  Label: \"OEM\"\n                                  Type: Alias for linux-data\n                                  UUID: 726E33FA-DFE9-45B2-B215-FB35CD9C2388\n     4726784      131072       7  Label: \"OEM-CONFIG\"\n                                  Type: Flatcar Container Linux reserved\n                                  UUID: 8F39CE8B-1FB3-4E7E-A784-0C53C8F40442\n     4857856    37085151       9  Label: \"ROOT\"\n                                  Type: Flatcar Container Linux auto-resize\n                                  UUID: D9A972BB-8084-4AB5-BA55-F8A3AFFAD70D\n    41943007          32          Sec GPT table\n    41943039           1          Sec GPT header  Looking specifically at \"USR-A\" and \"USR-B\", we see that \"USR-A\" is the active USR partition (this is what's actually mounted at /usr). Its priority is higher than that of \"USR-B\". When the system boots, GRUB (the bootloader) looks at the priorities, tries, and successful flags to determine which partition to use.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0  You'll notice that on this machine, \"USR-B\" hasn't actually successfully booted. Not to worry! This is a fresh machine that hasn't been through an update cycle yet. When the machine downloads an update, the partition table is updated to allow the newer image to boot.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=1 successful=0  In this case, we see that \"USR-B\" now has a higher priority and it has one try to successfully boot. Once the machine reboots, the partition table will again be updated.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=0  Now we see that the number of tries for \"USR-B\" has been decremented to zero. The successful flag still hasn't been updated though. Once update-engine has had a chance to run, it marks the boot as being successful.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=1",
            "title": "How do updates work?"
        },
        {
            "location": "/os/manual-rollbacks/#performing-a-manual-rollback",
            "text": "So, now that we understand what happens when the machine updates, we can tweak the process so that it boots an older image (assuming it's still intact on the passive partition). The first command we'll use is  cgpt find -t flatcar-usr . This will give us a list of all of the USR partitions available on the disk.  $ cgpt find -t flatcar-usr\n/dev/sda3\n/dev/sda4  To figure out which partition is currently active, we can use  rootdev .  $ rootdev -s /usr\n/dev/sda4  So now we know that  /dev/sda3  is the passive partition on our system. We can compose the previous two commands to dynamically figure out the passive partition.  $ cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\"\n/dev/sda3  In order to rollback, we need to mark that partition as active using  cgpt prioritize .  $ cgpt prioritize /dev/sda3  If we take another look at the GPT tables, we'll see that the priorities have been updated.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=1  Composing the previous two commands produces the following command to set the currently passive partition to be active on the next boot:  $ cgpt prioritize \"$(cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\")\"",
            "title": "Performing a manual rollback"
        },
        {
            "location": "/os/manual-rollbacks/#forcing-a-channel-downgrade",
            "text": "The procedure above restores the last known good Flatcar Container Linux version from immediately before an upgrade reboot. The system remains on the same  Flatcar Container Linux channel  after rebooting with the previous USR partition. It is also possible, though not recommended, to switch a Flatcar Container Linux installation to an older release channel, for example to make a system running an Alpha release downgrade to the Stable channel. Root privileges are required for this procedure, noted by  sudo  in the commands below.  First, edit  /etc/coreos/update.conf  to set  GROUP  to the name of the target channel, one of  stable  or  beta :  GROUP=stable  Next, clear the current version number from the  release  file so that the target channel will be certain to have a higher version number, triggering the \"upgrade,\" in this case a downgrade to the lower channel. Since  release  is on a read-only file system, it is convenient to temporarily override it with a bind mount. To do this, copy the original to a writable location, then bind the copy over the system  release  file:  $ cp /usr/share/coreos/release /tmp\n$ sudo mount -o bind /tmp/release /usr/share/coreos/release  The file is now writable, but the bind mount will not survive the reboot, so that the default read-only system  release  file will be restored after this procedure is complete. Edit  /usr/share/coreos/release  to replace the value of  COREOS_RELEASE_VERSION  with  0.0.0 :  COREOS_RELEASE_VERSION=0.0.0  Restart the update service so that it rescans the edited configuration, then initiate an update. The system will reboot into the selected lower channel after downloading the release:  $ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Forcing a Channel Downgrade"
        },
        {
            "location": "/os/getting-started-with-docker/",
            "text": "Getting started with Docker\n\u00b6\n\n\nDocker is an open-source project that makes creating and managing Linux containers really easy. Containers are like extremely lightweight VMs \u2013 they allow code to run in isolation from other containers but safely share the machine\u2019s resources, all without the overhead of a hypervisor.\n\n\nDocker containers can boot extremely fast (in milliseconds!) which gives you unprecedented flexibility in managing load across your cluster. For example, instead of running chef on each of your VMs, it\u2019s faster and more reliable to have your build system create a container and launch it on the appropriate number of Flatcar Container Linux hosts. This guide will show you how to launch a container, install some software on it, commit that container, and optionally launch it on another Flatcar Container Linux machine. Before starting, make sure you've got at least one Flatcar Container Linux machine up and running \u2014 try it on \nAmazon EC2\n or locally with \nVagrant\n.\n\n\nDocker CLI basics\n\u00b6\n\n\nDocker has a \nstraightforward CLI\n that allows you to do almost everything you could want to a container. All of these commands use the image id (ex. be29975e0098), the image name (ex. myusername/webapp) and the container id (ex. 72d468f455ea) interchangeably depending on the operation you are trying to do. This is confusing at first, so pay special attention to what you're using.\n\n\nLaunching a container\n\u00b6\n\n\nLaunching a container is simple as \ndocker run\n + the image name you would like to run + the command to run within the container. If the image doesn't exist on your local machine, Docker will attempt to fetch it from the public image registry. Later we'll explore how to use Docker with a private registry. It's important to note that containers are designed to stop once the command executed within them has exited. For example, if you ran \n/bin/echo hello world\n as your command, the container will start, print hello world and then stop:\n\n\ndocker run ubuntu /bin/echo hello world\n\n\n\nLet's launch an Ubuntu container and install Apache inside of it using the bash prompt:\n\n\ndocker run -t -i ubuntu /bin/bash\n\n\n\nThe \n-t\n and \n-i\n flags allocate a pseudo-tty and keep stdin open even if not attached. This will allow you to use the container like a traditional VM as long as the bash prompt is running. Install Apache with \napt-get update && apt-get install apache2\n. You're probably wondering what address you can connect to in order to test that Apache was correctly installed...we'll get to that after we commit the container.\n\n\nCommitting a container\n\u00b6\n\n\nAfter that completes, we need to \ncommit\n these changes to our container with the container ID and the image name.\n\n\nTo find the container ID, open another shell (so the container is still running) and read the ID using \ndocker ps\n.\n\n\nThe image name is in the format of \nusername/name\n. We're going to use \ncoreos\n as our username in this example but you should \nsign up for a Docker.IO user account\n and use that instead.\n\n\nIt's important to note that you can commit using any username and image name locally, but to push an image to the public registry, the username must be a valid \nDocker.IO user account\n.\n\n\nCommit the container with the container ID, your username, and the name \napache\n:\n\n\ndocker commit 72d468f455ea coreos/apache\n\n\n\nThe overlay filesystem works similar to git: our image now builds off of the \nubuntu\n base and adds another layer with Apache on top. These layers get cached separately so that you won't have to pull down the ubuntu base more than once.\n\n\nKeeping the Apache container running\n\u00b6\n\n\nNow we have our Ubuntu container with Apache running in one shell and an image of that container sitting on disk. Let's launch a new container based on that image but set it up to keep running indefinitely. The basic syntax looks like this, but we need to configure a few additional options that we'll fill in as we go:\n\n\ndocker run [options] [image] [process]\n\n\n\nThe first step is to tell Docker that we want to run our \ncoreos/apache\n image:\n\n\ndocker run [options] coreos/apache [process]\n\n\n\nRun container detached\n\u00b6\n\n\nWhen running Docker containers manually, the most important option is to run the container in detached mode with the \n-d\n flag. This will output the container ID to show that the command was successful, but nothing else. At any time you can run \ndocker ps\n in the other shell to view a list of the running containers. Our command now looks like:\n\n\ndocker run -d coreos/apache [process]\n\n\n\nAfter you are comfortable with the mechanics of running containers by hand, it's recommended to use \nsystemd units\n to run your containers on a cluster of Flatcar Container Linux machines.\n\n\nDo not run containers with detached mode inside of systemd unit files. Detached mode prevents your init system, in our case systemd, from monitoring the process that owns the container because detached mode forks it into the background. To prevent this issue, just omit the \n-d\n flag if you aren't running something manually.\n\n\nRun Apache in foreground\n\u00b6\n\n\nWe need to run the apache process in the foreground, since our container will stop when the process specified in the \ndocker run\n command stops. We can do this with a flag \n-D\n when starting the apache2 process:\n\n\n/usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nLet's add that to our command:\n\n\ndocker run -d coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nPermanently running a container\n\u00b6\n\n\nWhile the sections above explained how to run a container when configuring it, for a production setup, you should not manually start and babysit containers.\n\n\nInstead, create a systemd unit file to make systemd keep that container running. See the \nGetting Started with systemd\n for details.\n\n\nNetwork access to 80\n\u00b6\n\n\nThe default apache install will be running on port 80. To give our container access to traffic over port 80, we use the \n-p\n flag and specify the port on the host that maps to the port inside the container. In our case we want 80 for each, so we include \n-p 80:80\n in our command:\n\n\ndocker run -d -p 80:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nYou can now run this command on your Flatcar Container Linux host to create the container. You should see the default apache webpage when you load either \nlocalhost:80\n or the IP of your remote server. Be sure that any firewall or EC2 Security Group allows traffic to port 80.\n\n\nUsing the Docker registry\n\u00b6\n\n\nEarlier we downloaded the ubuntu image remotely from the Docker public registry because it didn't exist on our local machine. We can also push local images to the public registry (or a private registry) very easily with the \npush\n command:\n\n\ndocker push coreos/apache\n\n\n\nTo push to a private repository the syntax is very similar. First, we must prefix our image with the host running our private registry instead of our username. List images by running \ndocker images\n and insert the correct ID into the \ntag\n command:\n\n\ndocker tag f455ea72d468 registry.example.com:5000/apache\n\n\n\nAfter tagging, the image needs to be pushed to the registry:\n\n\ndocker push registry.example.com:5000/apache\n\n\n\nOnce the image is done uploading, you should be able to start the exact same container on a different Flatcar Container Linux host by running:\n\n\ndocker run -d -p 80:80 registry.example.com:5000/apache /usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nMore information\n\u00b6\n\n\nDocker Overview\n\n\nDocker Website\n\n\ndocker's Getting Started Guide",
            "title": "Getting started with Docker"
        },
        {
            "location": "/os/getting-started-with-docker/#getting-started-with-docker",
            "text": "Docker is an open-source project that makes creating and managing Linux containers really easy. Containers are like extremely lightweight VMs \u2013 they allow code to run in isolation from other containers but safely share the machine\u2019s resources, all without the overhead of a hypervisor.  Docker containers can boot extremely fast (in milliseconds!) which gives you unprecedented flexibility in managing load across your cluster. For example, instead of running chef on each of your VMs, it\u2019s faster and more reliable to have your build system create a container and launch it on the appropriate number of Flatcar Container Linux hosts. This guide will show you how to launch a container, install some software on it, commit that container, and optionally launch it on another Flatcar Container Linux machine. Before starting, make sure you've got at least one Flatcar Container Linux machine up and running \u2014 try it on  Amazon EC2  or locally with  Vagrant .",
            "title": "Getting started with Docker"
        },
        {
            "location": "/os/getting-started-with-docker/#docker-cli-basics",
            "text": "Docker has a  straightforward CLI  that allows you to do almost everything you could want to a container. All of these commands use the image id (ex. be29975e0098), the image name (ex. myusername/webapp) and the container id (ex. 72d468f455ea) interchangeably depending on the operation you are trying to do. This is confusing at first, so pay special attention to what you're using.",
            "title": "Docker CLI basics"
        },
        {
            "location": "/os/getting-started-with-docker/#launching-a-container",
            "text": "Launching a container is simple as  docker run  + the image name you would like to run + the command to run within the container. If the image doesn't exist on your local machine, Docker will attempt to fetch it from the public image registry. Later we'll explore how to use Docker with a private registry. It's important to note that containers are designed to stop once the command executed within them has exited. For example, if you ran  /bin/echo hello world  as your command, the container will start, print hello world and then stop:  docker run ubuntu /bin/echo hello world  Let's launch an Ubuntu container and install Apache inside of it using the bash prompt:  docker run -t -i ubuntu /bin/bash  The  -t  and  -i  flags allocate a pseudo-tty and keep stdin open even if not attached. This will allow you to use the container like a traditional VM as long as the bash prompt is running. Install Apache with  apt-get update && apt-get install apache2 . You're probably wondering what address you can connect to in order to test that Apache was correctly installed...we'll get to that after we commit the container.",
            "title": "Launching a container"
        },
        {
            "location": "/os/getting-started-with-docker/#committing-a-container",
            "text": "After that completes, we need to  commit  these changes to our container with the container ID and the image name.  To find the container ID, open another shell (so the container is still running) and read the ID using  docker ps .  The image name is in the format of  username/name . We're going to use  coreos  as our username in this example but you should  sign up for a Docker.IO user account  and use that instead.  It's important to note that you can commit using any username and image name locally, but to push an image to the public registry, the username must be a valid  Docker.IO user account .  Commit the container with the container ID, your username, and the name  apache :  docker commit 72d468f455ea coreos/apache  The overlay filesystem works similar to git: our image now builds off of the  ubuntu  base and adds another layer with Apache on top. These layers get cached separately so that you won't have to pull down the ubuntu base more than once.",
            "title": "Committing a container"
        },
        {
            "location": "/os/getting-started-with-docker/#keeping-the-apache-container-running",
            "text": "Now we have our Ubuntu container with Apache running in one shell and an image of that container sitting on disk. Let's launch a new container based on that image but set it up to keep running indefinitely. The basic syntax looks like this, but we need to configure a few additional options that we'll fill in as we go:  docker run [options] [image] [process]  The first step is to tell Docker that we want to run our  coreos/apache  image:  docker run [options] coreos/apache [process]",
            "title": "Keeping the Apache container running"
        },
        {
            "location": "/os/getting-started-with-docker/#run-container-detached",
            "text": "When running Docker containers manually, the most important option is to run the container in detached mode with the  -d  flag. This will output the container ID to show that the command was successful, but nothing else. At any time you can run  docker ps  in the other shell to view a list of the running containers. Our command now looks like:  docker run -d coreos/apache [process]  After you are comfortable with the mechanics of running containers by hand, it's recommended to use  systemd units  to run your containers on a cluster of Flatcar Container Linux machines.  Do not run containers with detached mode inside of systemd unit files. Detached mode prevents your init system, in our case systemd, from monitoring the process that owns the container because detached mode forks it into the background. To prevent this issue, just omit the  -d  flag if you aren't running something manually.",
            "title": "Run container detached"
        },
        {
            "location": "/os/getting-started-with-docker/#run-apache-in-foreground",
            "text": "We need to run the apache process in the foreground, since our container will stop when the process specified in the  docker run  command stops. We can do this with a flag  -D  when starting the apache2 process:  /usr/sbin/apache2ctl -D FOREGROUND  Let's add that to our command:  docker run -d coreos/apache /usr/sbin/apache2ctl -D FOREGROUND",
            "title": "Run Apache in foreground"
        },
        {
            "location": "/os/getting-started-with-docker/#permanently-running-a-container",
            "text": "While the sections above explained how to run a container when configuring it, for a production setup, you should not manually start and babysit containers.  Instead, create a systemd unit file to make systemd keep that container running. See the  Getting Started with systemd  for details.",
            "title": "Permanently running a container"
        },
        {
            "location": "/os/getting-started-with-docker/#network-access-to-80",
            "text": "The default apache install will be running on port 80. To give our container access to traffic over port 80, we use the  -p  flag and specify the port on the host that maps to the port inside the container. In our case we want 80 for each, so we include  -p 80:80  in our command:  docker run -d -p 80:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND  You can now run this command on your Flatcar Container Linux host to create the container. You should see the default apache webpage when you load either  localhost:80  or the IP of your remote server. Be sure that any firewall or EC2 Security Group allows traffic to port 80.",
            "title": "Network access to 80"
        },
        {
            "location": "/os/getting-started-with-docker/#using-the-docker-registry",
            "text": "Earlier we downloaded the ubuntu image remotely from the Docker public registry because it didn't exist on our local machine. We can also push local images to the public registry (or a private registry) very easily with the  push  command:  docker push coreos/apache  To push to a private repository the syntax is very similar. First, we must prefix our image with the host running our private registry instead of our username. List images by running  docker images  and insert the correct ID into the  tag  command:  docker tag f455ea72d468 registry.example.com:5000/apache  After tagging, the image needs to be pushed to the registry:  docker push registry.example.com:5000/apache  Once the image is done uploading, you should be able to start the exact same container on a different Flatcar Container Linux host by running:  docker run -d -p 80:80 registry.example.com:5000/apache /usr/sbin/apache2ctl -D FOREGROUND",
            "title": "Using the Docker registry"
        },
        {
            "location": "/os/getting-started-with-docker/#more-information",
            "text": "Docker Overview  Docker Website  docker's Getting Started Guide",
            "title": "More information"
        },
        {
            "location": "/os/customizing-docker/",
            "text": "Customizing docker\n\u00b6\n\n\nThe Docker systemd unit can be customized by overriding the unit that ships with the default Flatcar Container Linux settings. Common use-cases for doing this are covered below.\n\n\nEnable the remote API on a new socket\n\u00b6\n\n\nCreate a file called \n/etc/systemd/system/docker-tcp.socket\n to make Docker available on a TCP socket on port 2375.\n\n\n[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=2375\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target\n\n\n\nThen enable this new socket:\n\n\nsystemctl enable docker-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tcp.socket\nsystemctl start docker\n\n\n\nTest that it's working:\n\n\ndocker -H tcp://127.0.0.1:2375 ps\n\n\n\nContainer Linux Config\n\u00b6\n\n\nTo enable the remote API on every Flatcar Container Linux machine in a cluster, use a \nContainer Linux Config\n. We need to provide the new socket file and Docker's socket activation support will automatically start using the socket:\n\n\nsystemd:\n  units:\n    - name: docker-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\n\n\n\nTo keep access to the port local, replace the \nListenStream\n configuration above with:\n\n\n        [Socket]\n        ListenStream=127.0.0.1:2375\n\n\n\nEnable the remote API with TLS authentication\n\u00b6\n\n\nDocker TLS configuration consists of three parts: keys creation, configuring new \nsystemd socket\n unit and systemd \ndrop-in\n configuration.\n\n\nTLS keys creation\n\u00b6\n\n\nPlease follow the \ninstruction\n to know how to create self-signed certificates and private keys. Then copy with following files into \n/etc/docker\n Flatcar Container Linux's directory and fix their permissions:\n\n\nscp ~/cfssl/{server.pem,server-key.pem,ca.pem} coreos.example.com:\nssh core@coreos.example.com\nsudo mv {server.pem,server-key.pem,ca.pem} /etc/docker/\nsudo chown root:root /etc/docker/{server-key.pem,server.pem,ca.pem}\nsudo chmod 0600 /etc/docker/server-key.pem\n\n\n\nOn your local host copy certificates into \n~/.docker\n:\n\n\nmkdir ~/.docker\nchmod 700 ~/.docker\ncd ~/.docker\ncp -p ~/cfssl/ca.pem ca.pem\ncp -p ~/cfssl/client.pem cert.pem\ncp -p ~/cfssl/client-key.pem key.pem\n\n\n\nEnable the secure remote API on a new socket\n\u00b6\n\n\nCreate a file called \n/etc/systemd/system/docker-tls-tcp.socket\n to make Docker available on a secured TCP socket on port 2376.\n\n\n[Unit]\nDescription=Docker Secured Socket for the API\n\n[Socket]\nListenStream=2376\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target\n\n\n\nThen enable this new socket:\n\n\nsystemctl enable docker-tls-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tls-tcp.socket\n\n\n\nDrop-in configuration\n\u00b6\n\n\nCreate \n/etc/systemd/system/docker.service.d/10-tls-verify.conf\n \ndrop-in\n for systemd Docker service:\n\n\n[Service]\nEnvironment=\"DOCKER_OPTS=--tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server.pem --tlskey=/etc/docker/server-key.pem\"\n\n\n\nReload systemd config files and restart docker service:\n\n\nsudo systemctl daemon-reload\nsudo systemctl restart docker.service\n\n\n\nNow you can access your Docker's API through TLS secured connection:\n\n\ndocker --tlsverify -H tcp://server:2376 images\n# or\ndocker --tlsverify -H tcp://server.example.com:2376 images\n\n\n\nIf you've experienceed problems connection to remote Docker API using TLS connection, you can debug it with \ncurl\n:\n\n\ncurl -v --cacert ~/.docker/ca.pem --cert ~/.docker/cert.pem --key ~/.docker/key.pem https://server:2376\n\n\n\nOr on your Flatcar Container Linux host:\n\n\njournalctl -f -u docker.service\n\n\n\nIn addition you can export environment variables and use docker client without additional options:\n\n\nexport DOCKER_HOST=tcp://server.example.com:2376 DOCKER_TLS_VERIFY=1\ndocker images\n\n\n\nContainer Linux Config\n\u00b6\n\n\nA Container Linux Config for Docker TLS authentication will look like:\n\n\nstorage:\n  files:\n    - path: /etc/docker/ca.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFNDCCAx6gAwIBAgIBATALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDExMDhaFw0y\n          NTA5MDIxMDExMThaMC0xDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEL\n          ... ... ...\n    - path: /etc/docker/server.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFajCCA1SgAwIBAgIBBTALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDM3MDFaFw0y\n          NTA5MDIxMDM3MDNaMEQxDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEQ\n          ... ... ...\n    - path: /etc/docker/server-key.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          MIIJKAIBAAKCAgEA23Q4yELhNEywScrHl6+MUtbonCu59LIjpxDMAGxAHvWhWpEY\n          P5vfas8KgxxNyR+U8VpIjEXvwnhwCx/CSCJc3/VtU9v011Ir0WtTrNDocb90fIr3\n          YeRWq744UJpBeDHPV9opf8xFE7F74zWeTVMwtiMPKcQDzZ7XoNyJMxg1wmiMbdCj\n          ... ... ...\nsystemd:\n  units:\n    - name: docker-tls-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Secured Socket for the API\n\n        [Socket]\n        ListenStream=2376\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\ndocker:\n  flags:\n    - --tlsverify\n    - --tlscacert=/etc/docker/ca.pem\n    - --tlscert=/etc/docker/server.pem\n    - --tlskey=/etc/docker/server-key.pem\n\n\n\nUse attached storage for Docker images\n\u00b6\n\n\nDocker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Check out the guide to \nmounting storage to your Flatcar Container Linux machine\n for an example of how to bind mount storage into \n/var/lib/docker\n.\n\n\nEnabling the Docker debug flag\n\u00b6\n\n\nSet the \n--debug\n (\n-D\n) flag in the \nDOCKER_OPTS\n environment variable by using a drop-in file. For example, the following could be written to \n/etc/systemd/system/docker.service.d/10-debug.conf\n:\n\n\n[Service]\nEnvironment=DOCKER_OPTS=--debug\n\n\n\nNow tell systemd about the new configuration and restart Docker:\n\n\nsystemctl daemon-reload\nsystemctl restart docker\n\n\n\nTo test our debugging stream, run a Docker command and then read the systemd journal, which should contain the output:\n\n\ndocker ps\njournalctl -u docker\n\n\n\nContainer Linux Config\n\u00b6\n\n\nIf you need to modify a flag across many machines, you can add the flag with a Container Linux Config:\n\n\ndocker:\n  flags:\n    - --debug\n\n\n\nUse an HTTP proxy\n\u00b6\n\n\nIf you're operating in a locked down networking environment, you can specify an HTTP proxy for Docker to use via an environment variable. First, create a directory for drop-in configuration for Docker:\n\n\nmkdir /etc/systemd/system/docker.service.d\n\n\n\nNow, create a file called \n/etc/systemd/system/docker.service.d/http-proxy.conf\n that adds the environment variable:\n\n\n[Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:8080\"\n\n\n\nTo apply the change, reload the unit and restart Docker:\n\n\nsystemctl daemon-reload\nsystemctl restart docker\n\n\n\nProxy environment variables can also be set \nsystem-wide\n.\n\n\nContainer Linux Config\n\u00b6\n\n\nThe easiest way to use this proxy on all of your machines is via a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 20-http-proxy.conf\n          contents: |\n            [Service]\n            Environment=\"HTTP_PROXY=http://proxy.example.com:8080\"\n\n\n\nIncrease ulimits\n\u00b6\n\n\nIf you need to increase certain ulimits that are too low for your application by default, like memlock, you will need to modify the Docker service to increase the limit. First, create a directory for drop-in configuration for Docker:\n\n\nmkdir /etc/systemd/system/docker.service.d\n\n\n\nNow, create a file called \n/etc/systemd/system/docker.service.d/increase-ulimit.conf\n that adds increased limit:\n\n\n[Service]\nLimitMEMLOCK=infinity\n\n\n\nTo apply the change, reload the unit and restart Docker:\n\n\nsystemctl daemon-reload\nsystemctl restart docker\n\n\n\nContainer Linux Config\n\u00b6\n\n\nThe easiest way to use these new ulimits on all of your machines is via a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 30-increase-ulimit.conf\n          contents: |\n            [Service]\n            LimitMEMLOCK=infinity\n\n\n\nUsing a dockercfg file for authentication\n\u00b6\n\n\nA json file \n.dockercfg\n can be created in your home directory that holds authentication information for a public or private Docker registry.",
            "title": "Customizing Docker"
        },
        {
            "location": "/os/customizing-docker/#customizing-docker",
            "text": "The Docker systemd unit can be customized by overriding the unit that ships with the default Flatcar Container Linux settings. Common use-cases for doing this are covered below.",
            "title": "Customizing docker"
        },
        {
            "location": "/os/customizing-docker/#enable-the-remote-api-on-a-new-socket",
            "text": "Create a file called  /etc/systemd/system/docker-tcp.socket  to make Docker available on a TCP socket on port 2375.  [Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=2375\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target  Then enable this new socket:  systemctl enable docker-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tcp.socket\nsystemctl start docker  Test that it's working:  docker -H tcp://127.0.0.1:2375 ps",
            "title": "Enable the remote API on a new socket"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config",
            "text": "To enable the remote API on every Flatcar Container Linux machine in a cluster, use a  Container Linux Config . We need to provide the new socket file and Docker's socket activation support will automatically start using the socket:  systemd:\n  units:\n    - name: docker-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target  To keep access to the port local, replace the  ListenStream  configuration above with:          [Socket]\n        ListenStream=127.0.0.1:2375",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#enable-the-remote-api-with-tls-authentication",
            "text": "Docker TLS configuration consists of three parts: keys creation, configuring new  systemd socket  unit and systemd  drop-in  configuration.",
            "title": "Enable the remote API with TLS authentication"
        },
        {
            "location": "/os/customizing-docker/#tls-keys-creation",
            "text": "Please follow the  instruction  to know how to create self-signed certificates and private keys. Then copy with following files into  /etc/docker  Flatcar Container Linux's directory and fix their permissions:  scp ~/cfssl/{server.pem,server-key.pem,ca.pem} coreos.example.com:\nssh core@coreos.example.com\nsudo mv {server.pem,server-key.pem,ca.pem} /etc/docker/\nsudo chown root:root /etc/docker/{server-key.pem,server.pem,ca.pem}\nsudo chmod 0600 /etc/docker/server-key.pem  On your local host copy certificates into  ~/.docker :  mkdir ~/.docker\nchmod 700 ~/.docker\ncd ~/.docker\ncp -p ~/cfssl/ca.pem ca.pem\ncp -p ~/cfssl/client.pem cert.pem\ncp -p ~/cfssl/client-key.pem key.pem",
            "title": "TLS keys creation"
        },
        {
            "location": "/os/customizing-docker/#enable-the-secure-remote-api-on-a-new-socket",
            "text": "Create a file called  /etc/systemd/system/docker-tls-tcp.socket  to make Docker available on a secured TCP socket on port 2376.  [Unit]\nDescription=Docker Secured Socket for the API\n\n[Socket]\nListenStream=2376\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target  Then enable this new socket:  systemctl enable docker-tls-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tls-tcp.socket",
            "title": "Enable the secure remote API on a new socket"
        },
        {
            "location": "/os/customizing-docker/#drop-in-configuration",
            "text": "Create  /etc/systemd/system/docker.service.d/10-tls-verify.conf   drop-in  for systemd Docker service:  [Service]\nEnvironment=\"DOCKER_OPTS=--tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server.pem --tlskey=/etc/docker/server-key.pem\"  Reload systemd config files and restart docker service:  sudo systemctl daemon-reload\nsudo systemctl restart docker.service  Now you can access your Docker's API through TLS secured connection:  docker --tlsverify -H tcp://server:2376 images\n# or\ndocker --tlsverify -H tcp://server.example.com:2376 images  If you've experienceed problems connection to remote Docker API using TLS connection, you can debug it with  curl :  curl -v --cacert ~/.docker/ca.pem --cert ~/.docker/cert.pem --key ~/.docker/key.pem https://server:2376  Or on your Flatcar Container Linux host:  journalctl -f -u docker.service  In addition you can export environment variables and use docker client without additional options:  export DOCKER_HOST=tcp://server.example.com:2376 DOCKER_TLS_VERIFY=1\ndocker images",
            "title": "Drop-in configuration"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_1",
            "text": "A Container Linux Config for Docker TLS authentication will look like:  storage:\n  files:\n    - path: /etc/docker/ca.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFNDCCAx6gAwIBAgIBATALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDExMDhaFw0y\n          NTA5MDIxMDExMThaMC0xDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEL\n          ... ... ...\n    - path: /etc/docker/server.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFajCCA1SgAwIBAgIBBTALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDM3MDFaFw0y\n          NTA5MDIxMDM3MDNaMEQxDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEQ\n          ... ... ...\n    - path: /etc/docker/server-key.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          MIIJKAIBAAKCAgEA23Q4yELhNEywScrHl6+MUtbonCu59LIjpxDMAGxAHvWhWpEY\n          P5vfas8KgxxNyR+U8VpIjEXvwnhwCx/CSCJc3/VtU9v011Ir0WtTrNDocb90fIr3\n          YeRWq744UJpBeDHPV9opf8xFE7F74zWeTVMwtiMPKcQDzZ7XoNyJMxg1wmiMbdCj\n          ... ... ...\nsystemd:\n  units:\n    - name: docker-tls-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Secured Socket for the API\n\n        [Socket]\n        ListenStream=2376\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\ndocker:\n  flags:\n    - --tlsverify\n    - --tlscacert=/etc/docker/ca.pem\n    - --tlscert=/etc/docker/server.pem\n    - --tlskey=/etc/docker/server-key.pem",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#use-attached-storage-for-docker-images",
            "text": "Docker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Check out the guide to  mounting storage to your Flatcar Container Linux machine  for an example of how to bind mount storage into  /var/lib/docker .",
            "title": "Use attached storage for Docker images"
        },
        {
            "location": "/os/customizing-docker/#enabling-the-docker-debug-flag",
            "text": "Set the  --debug  ( -D ) flag in the  DOCKER_OPTS  environment variable by using a drop-in file. For example, the following could be written to  /etc/systemd/system/docker.service.d/10-debug.conf :  [Service]\nEnvironment=DOCKER_OPTS=--debug  Now tell systemd about the new configuration and restart Docker:  systemctl daemon-reload\nsystemctl restart docker  To test our debugging stream, run a Docker command and then read the systemd journal, which should contain the output:  docker ps\njournalctl -u docker",
            "title": "Enabling the Docker debug flag"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_2",
            "text": "If you need to modify a flag across many machines, you can add the flag with a Container Linux Config:  docker:\n  flags:\n    - --debug",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#use-an-http-proxy",
            "text": "If you're operating in a locked down networking environment, you can specify an HTTP proxy for Docker to use via an environment variable. First, create a directory for drop-in configuration for Docker:  mkdir /etc/systemd/system/docker.service.d  Now, create a file called  /etc/systemd/system/docker.service.d/http-proxy.conf  that adds the environment variable:  [Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:8080\"  To apply the change, reload the unit and restart Docker:  systemctl daemon-reload\nsystemctl restart docker  Proxy environment variables can also be set  system-wide .",
            "title": "Use an HTTP proxy"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_3",
            "text": "The easiest way to use this proxy on all of your machines is via a Container Linux Config:  systemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 20-http-proxy.conf\n          contents: |\n            [Service]\n            Environment=\"HTTP_PROXY=http://proxy.example.com:8080\"",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#increase-ulimits",
            "text": "If you need to increase certain ulimits that are too low for your application by default, like memlock, you will need to modify the Docker service to increase the limit. First, create a directory for drop-in configuration for Docker:  mkdir /etc/systemd/system/docker.service.d  Now, create a file called  /etc/systemd/system/docker.service.d/increase-ulimit.conf  that adds increased limit:  [Service]\nLimitMEMLOCK=infinity  To apply the change, reload the unit and restart Docker:  systemctl daemon-reload\nsystemctl restart docker",
            "title": "Increase ulimits"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_4",
            "text": "The easiest way to use these new ulimits on all of your machines is via a Container Linux Config:  systemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 30-increase-ulimit.conf\n          contents: |\n            [Service]\n            LimitMEMLOCK=infinity",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#using-a-dockercfg-file-for-authentication",
            "text": "A json file  .dockercfg  can be created in your home directory that holds authentication information for a public or private Docker registry.",
            "title": "Using a dockercfg file for authentication"
        },
        {
            "location": "/container-linux-config-transpiler/code-of-conduct/",
            "text": "CoreOS Community Code of Conduct\n\u00b6\n\n\nContributor Code of Conduct\n\u00b6\n\n\nAs contributors and maintainers of this project, and in the interest of\nfostering an open and welcoming community, we pledge to respect all people who\ncontribute through reporting issues, posting feature requests, updating\ndocumentation, submitting pull requests or patches, and other activities.\n\n\nWe are committed to making participation in this project a harassment-free\nexperience for everyone, regardless of level of experience, gender, gender\nidentity and expression, sexual orientation, disability, personal appearance,\nbody size, race, ethnicity, age, religion, or nationality.\n\n\nExamples of unacceptable behavior by participants include:\n\n\n\n\nThe use of sexualized language or imagery\n\n\nPersonal attacks\n\n\nTrolling or insulting/derogatory comments\n\n\nPublic or private harassment\n\n\nPublishing others' private information, such as physical or electronic addresses, without explicit permission\n\n\nOther unethical or unprofessional conduct.\n\n\n\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct. By adopting this Code of Conduct,\nproject maintainers commit themselves to fairly and consistently applying these\nprinciples to every aspect of managing this project. Project maintainers who do\nnot follow or enforce the Code of Conduct may be permanently removed from the\nproject team.\n\n\nThis code of conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community.\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting a project maintainer, Brandon Philips\n\nbrandon.philips@coreos.com\n, and/or Rithu John \nrithu.john@coreos.com\n.\n\n\nThis Code of Conduct is adapted from the Contributor Covenant\n(\nhttp://contributor-covenant.org\n), version 1.2.0, available at\n\nhttp://contributor-covenant.org/version/1/2/0/\n\n\nCoreOS Events Code of Conduct\n\u00b6\n\n\nCoreOS events are working conferences intended for professional networking and\ncollaboration in the CoreOS community. Attendees are expected to behave\naccording to professional standards and in accordance with their employer\u2019s\npolicies on appropriate workplace behavior.\n\n\nWhile at CoreOS events or related social networking opportunities, attendees\nshould not engage in discriminatory or offensive speech or actions including\nbut not limited to gender, sexuality, race, age, disability, or religion.\nSpeakers should be especially aware of these concerns.\n\n\nCoreOS does not condone any statements by speakers contrary to these standards.\nCoreOS reserves the right to deny entrance and/or eject from an event (without\nrefund) any individual found to be engaging in discriminatory or offensive\nspeech or actions.\n\n\nPlease bring any concerns to the immediate attention of designated on-site\nstaff, Brandon Philips \nbrandon.philips@coreos.com\n, and/or Rithu John \nrithu.john@coreos.com\n.",
            "title": "Code of conduct"
        },
        {
            "location": "/container-linux-config-transpiler/code-of-conduct/#coreos-community-code-of-conduct",
            "text": "",
            "title": "CoreOS Community Code of Conduct"
        },
        {
            "location": "/container-linux-config-transpiler/code-of-conduct/#contributor-code-of-conduct",
            "text": "As contributors and maintainers of this project, and in the interest of\nfostering an open and welcoming community, we pledge to respect all people who\ncontribute through reporting issues, posting feature requests, updating\ndocumentation, submitting pull requests or patches, and other activities.  We are committed to making participation in this project a harassment-free\nexperience for everyone, regardless of level of experience, gender, gender\nidentity and expression, sexual orientation, disability, personal appearance,\nbody size, race, ethnicity, age, religion, or nationality.  Examples of unacceptable behavior by participants include:   The use of sexualized language or imagery  Personal attacks  Trolling or insulting/derogatory comments  Public or private harassment  Publishing others' private information, such as physical or electronic addresses, without explicit permission  Other unethical or unprofessional conduct.   Project maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct. By adopting this Code of Conduct,\nproject maintainers commit themselves to fairly and consistently applying these\nprinciples to every aspect of managing this project. Project maintainers who do\nnot follow or enforce the Code of Conduct may be permanently removed from the\nproject team.  This code of conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community.  Instances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting a project maintainer, Brandon Philips brandon.philips@coreos.com , and/or Rithu John  rithu.john@coreos.com .  This Code of Conduct is adapted from the Contributor Covenant\n( http://contributor-covenant.org ), version 1.2.0, available at http://contributor-covenant.org/version/1/2/0/",
            "title": "Contributor Code of Conduct"
        },
        {
            "location": "/container-linux-config-transpiler/code-of-conduct/#coreos-events-code-of-conduct",
            "text": "CoreOS events are working conferences intended for professional networking and\ncollaboration in the CoreOS community. Attendees are expected to behave\naccording to professional standards and in accordance with their employer\u2019s\npolicies on appropriate workplace behavior.  While at CoreOS events or related social networking opportunities, attendees\nshould not engage in discriminatory or offensive speech or actions including\nbut not limited to gender, sexuality, race, age, disability, or religion.\nSpeakers should be especially aware of these concerns.  CoreOS does not condone any statements by speakers contrary to these standards.\nCoreOS reserves the right to deny entrance and/or eject from an event (without\nrefund) any individual found to be engaging in discriminatory or offensive\nspeech or actions.  Please bring any concerns to the immediate attention of designated on-site\nstaff, Brandon Philips  brandon.philips@coreos.com , and/or Rithu John  rithu.john@coreos.com .",
            "title": "CoreOS Events Code of Conduct"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/",
            "text": "How to Contribute\n\u00b6\n\n\nCoreOS projects are \nApache 2.0 licensed\n and accept contributions via\nGitHub pull requests.  This document outlines some of the conventions on\ndevelopment workflow, commit message formatting, contact points and other\nresources to make it easier to get your contribution accepted.\n\n\nCertificate of Origin\n\u00b6\n\n\nBy contributing to this project you agree to the Developer Certificate of\nOrigin (DCO). This document was created by the Linux Kernel community and is a\nsimple statement that you, as a contributor, have the legal right to make the\ncontribution. See the \nDCO\n file for details.\n\n\nEmail and Chat\n\u00b6\n\n\nThe project currently uses the general CoreOS email list and IRC channel:\n- Email: \ncoreos-dev\n\n- IRC: #\ncoreos\n IRC channel on freenode.org\n\n\nPlease avoid emailing maintainers found in the MAINTAINERS file directly. They\nare very busy and read the mailing lists.\n\n\nGetting Started\n\u00b6\n\n\n\n\nFork the repository on GitHub\n\n\nRead the \nREADME\n for build and test instructions\n\n\nPlay with the project, submit bugs, submit patches!\n\n\n\n\nContribution Flow\n\u00b6\n\n\nThis is a rough outline of what a contributor's workflow looks like:\n\n\n\n\nCreate a topic branch from where you want to base your work (usually master).\n\n\nMake commits of logical units.\n\n\nMake sure your commit messages are in the proper format (see below).\n\n\nPush your changes to a topic branch in your fork of the repository.\n\n\nMake sure the tests pass, and add any new tests as appropriate.\n\n\nSubmit a pull request to the original repository.\n\n\n\n\nThanks for your contributions!\n\n\nFormat of the Commit Message\n\u00b6\n\n\nWe follow a rough convention for commit messages that is designed to answer two\nquestions: what changed and why. The subject line should feature the what and\nthe body of the commit should describe the why.\n\n\nscripts: add the test-cluster command\n\nthis uses tmux to setup a test cluster that you can easily kill and\nstart for debugging.\n\nFixes #38\n\n\n\nThe format can be described more formally as follows:\n\n\n<subsystem>: <what changed>\n<BLANK LINE>\n<why this change was made>\n<BLANK LINE>\n<footer>\n\n\n\nThe first line is the subject and should be no longer than 70 characters, the\nsecond line is always blank, and other lines should be wrapped at 80 characters.\nThis allows the message to be easier to read on GitHub as well as in various\ngit tools.",
            "title": "How to Contribute"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/#how-to-contribute",
            "text": "CoreOS projects are  Apache 2.0 licensed  and accept contributions via\nGitHub pull requests.  This document outlines some of the conventions on\ndevelopment workflow, commit message formatting, contact points and other\nresources to make it easier to get your contribution accepted.",
            "title": "How to Contribute"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/#certificate-of-origin",
            "text": "By contributing to this project you agree to the Developer Certificate of\nOrigin (DCO). This document was created by the Linux Kernel community and is a\nsimple statement that you, as a contributor, have the legal right to make the\ncontribution. See the  DCO  file for details.",
            "title": "Certificate of Origin"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/#email-and-chat",
            "text": "The project currently uses the general CoreOS email list and IRC channel:\n- Email:  coreos-dev \n- IRC: # coreos  IRC channel on freenode.org  Please avoid emailing maintainers found in the MAINTAINERS file directly. They\nare very busy and read the mailing lists.",
            "title": "Email and Chat"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/#getting-started",
            "text": "Fork the repository on GitHub  Read the  README  for build and test instructions  Play with the project, submit bugs, submit patches!",
            "title": "Getting Started"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/#contribution-flow",
            "text": "This is a rough outline of what a contributor's workflow looks like:   Create a topic branch from where you want to base your work (usually master).  Make commits of logical units.  Make sure your commit messages are in the proper format (see below).  Push your changes to a topic branch in your fork of the repository.  Make sure the tests pass, and add any new tests as appropriate.  Submit a pull request to the original repository.   Thanks for your contributions!",
            "title": "Contribution Flow"
        },
        {
            "location": "/container-linux-config-transpiler/CONTRIBUTING/#format-of-the-commit-message",
            "text": "We follow a rough convention for commit messages that is designed to answer two\nquestions: what changed and why. The subject line should feature the what and\nthe body of the commit should describe the why.  scripts: add the test-cluster command\n\nthis uses tmux to setup a test cluster that you can easily kill and\nstart for debugging.\n\nFixes #38  The format can be described more formally as follows:  <subsystem>: <what changed>\n<BLANK LINE>\n<why this change was made>\n<BLANK LINE>\n<footer>  The first line is the subject and should be no longer than 70 characters, the\nsecond line is always blank, and other lines should be wrapped at 80 characters.\nThis allows the message to be easier to read on GitHub as well as in various\ngit tools.",
            "title": "Format of the Commit Message"
        },
        {
            "location": "/container-linux-config-transpiler/README/",
            "text": "Container Linux Config Transpiler\n\u00b6\n\n\nThe Config Transpiler (\"ct\" for short) is the utility responsible for transforming a human-friendly Container Linux Config into a JSON file. This resulting file can be provided to a Container Linux machine when it first boots to provision the machine.\n\n\nDocumentation\n\u00b6\n\n\nIf you're looking to begin writing configs for your Container Linux machines, check out the \ngetting started\n documentation.\n\n\nThe \nconfiguration\n documentation is a comprehensive resource specifying what options can be in a Container Linux Config.\n\n\nFor a more in-depth view of ct and why it exists, take a look at the \nOverview\n document.\n\n\nPlease use the \nbug tracker\n to report bugs.\n\n\nExamples\n\u00b6\n\n\nThere are plenty of small, self-contained examples \nin the documentation\n.\n\n\nInstallation\n\u00b6\n\n\nmacOS homebrew\n\u00b6\n\n\nbrew install coreos-ct\n\n\n\nPrebuilt binaries\n\u00b6\n\n\nThe easiest way to get started using ct is to download one of the binaries from the \nreleases page on GitHub\n.\n\n\nOne can use the following script to download and verify the signature of Config Transpiler:\n\n\n# Specify Config Transpiler version\nCT_VER=v0.6.1\n\n# Specify Architecture\n# ARCH=aarch64 # ARM's 64-bit architecture\nARCH=x86_64\n\n# Specify OS\n# OS=apple-darwin # MacOS\n# OS=pc-windows-gnu.exe # Windows\nOS=unknown-linux-gnu # Linux\n\n# Specify download URL\nDOWNLOAD_URL=https://github.com/coreos/container-linux-config-transpiler/releases/download\n\n# Remove previous downloads\nrm -f /tmp/ct-${CT_VER}-${ARCH}-${OS} /tmp/ct-${CT_VER}-${ARCH}-${OS}.asc /tmp/coreos-app-signing-pubkey.gpg\n\n# Download Config Transpiler binary\ncurl -L ${DOWNLOAD_URL}/${CT_VER}/ct-${CT_VER}-${ARCH}-${OS} -o /tmp/ct-${CT_VER}-${ARCH}-${OS}\nchmod u+x /tmp/ct-${CT_VER}-${ARCH}-${OS}\n\n# Download and import CoreOS application signing GPG key\ncurl https://coreos.com/dist/pubkeys/app-signing-pubkey.gpg -o /tmp/coreos-app-signing-pubkey.gpg\ngpg2 --import --keyid-format LONG /tmp/coreos-app-signing-pubkey.gpg\n\n# Download and import CoreOS application signing GPG key if it has not already been imported\ncurl -L ${DOWNLOAD_URL}/${CT_VER}/ct-${CT_VER}-${ARCH}-${OS}.asc -o /tmp/ct-${CT_VER}-${ARCH}-${OS}.asc\ngpg2 --verify /tmp/ct-${CT_VER}-${ARCH}-${OS}.asc /tmp/ct-${CT_VER}-${ARCH}-${OS}\n\n\n\nBuilding from source\n\u00b6\n\n\nTo build from source you'll need to have the go compiler installed on your system.\n\n\ngit clone --branch v0.8.0 https://github.com/coreos/container-linux-config-transpiler\ncd container-linux-config-transpiler\nmake\n\n\n\nThe \nct\n binary will be placed in \n./bin/\n.\n\n\nNote: Review releases for new branch versions.\n\n\nRelated projects\n\u00b6\n\n\n\n\nhttps://github.com/coreos/ignition\n\n\nhttps://github.com/coreos/coreos-metadata/\n\n\nhttps://github.com/coreos/matchbox",
            "title": "Container Linux Config Transpiler"
        },
        {
            "location": "/container-linux-config-transpiler/README/#container-linux-config-transpiler",
            "text": "The Config Transpiler (\"ct\" for short) is the utility responsible for transforming a human-friendly Container Linux Config into a JSON file. This resulting file can be provided to a Container Linux machine when it first boots to provision the machine.",
            "title": "Container Linux Config Transpiler"
        },
        {
            "location": "/container-linux-config-transpiler/README/#documentation",
            "text": "If you're looking to begin writing configs for your Container Linux machines, check out the  getting started  documentation.  The  configuration  documentation is a comprehensive resource specifying what options can be in a Container Linux Config.  For a more in-depth view of ct and why it exists, take a look at the  Overview  document.  Please use the  bug tracker  to report bugs.",
            "title": "Documentation"
        },
        {
            "location": "/container-linux-config-transpiler/README/#examples",
            "text": "There are plenty of small, self-contained examples  in the documentation .",
            "title": "Examples"
        },
        {
            "location": "/container-linux-config-transpiler/README/#installation",
            "text": "",
            "title": "Installation"
        },
        {
            "location": "/container-linux-config-transpiler/README/#macos-homebrew",
            "text": "brew install coreos-ct",
            "title": "macOS homebrew"
        },
        {
            "location": "/container-linux-config-transpiler/README/#prebuilt-binaries",
            "text": "The easiest way to get started using ct is to download one of the binaries from the  releases page on GitHub .  One can use the following script to download and verify the signature of Config Transpiler:  # Specify Config Transpiler version\nCT_VER=v0.6.1\n\n# Specify Architecture\n# ARCH=aarch64 # ARM's 64-bit architecture\nARCH=x86_64\n\n# Specify OS\n# OS=apple-darwin # MacOS\n# OS=pc-windows-gnu.exe # Windows\nOS=unknown-linux-gnu # Linux\n\n# Specify download URL\nDOWNLOAD_URL=https://github.com/coreos/container-linux-config-transpiler/releases/download\n\n# Remove previous downloads\nrm -f /tmp/ct-${CT_VER}-${ARCH}-${OS} /tmp/ct-${CT_VER}-${ARCH}-${OS}.asc /tmp/coreos-app-signing-pubkey.gpg\n\n# Download Config Transpiler binary\ncurl -L ${DOWNLOAD_URL}/${CT_VER}/ct-${CT_VER}-${ARCH}-${OS} -o /tmp/ct-${CT_VER}-${ARCH}-${OS}\nchmod u+x /tmp/ct-${CT_VER}-${ARCH}-${OS}\n\n# Download and import CoreOS application signing GPG key\ncurl https://coreos.com/dist/pubkeys/app-signing-pubkey.gpg -o /tmp/coreos-app-signing-pubkey.gpg\ngpg2 --import --keyid-format LONG /tmp/coreos-app-signing-pubkey.gpg\n\n# Download and import CoreOS application signing GPG key if it has not already been imported\ncurl -L ${DOWNLOAD_URL}/${CT_VER}/ct-${CT_VER}-${ARCH}-${OS}.asc -o /tmp/ct-${CT_VER}-${ARCH}-${OS}.asc\ngpg2 --verify /tmp/ct-${CT_VER}-${ARCH}-${OS}.asc /tmp/ct-${CT_VER}-${ARCH}-${OS}",
            "title": "Prebuilt binaries"
        },
        {
            "location": "/container-linux-config-transpiler/README/#building-from-source",
            "text": "To build from source you'll need to have the go compiler installed on your system.  git clone --branch v0.8.0 https://github.com/coreos/container-linux-config-transpiler\ncd container-linux-config-transpiler\nmake  The  ct  binary will be placed in  ./bin/ .  Note: Review releases for new branch versions.",
            "title": "Building from source"
        },
        {
            "location": "/container-linux-config-transpiler/README/#related-projects",
            "text": "https://github.com/coreos/ignition  https://github.com/coreos/coreos-metadata/  https://github.com/coreos/matchbox",
            "title": "Related projects"
        },
        {
            "location": "/container-linux-config-transpiler/doc/configuration/",
            "text": "Configuration Specification\n\u00b6\n\n\nA Container Linux Configuration, to be processed by ct, is a YAML document conforming to the following specification:\n\n\nNote: all fields are optional unless otherwise marked\n\n\n\n\nignition\n (object): metadata about the configuration itself.\n\n\nconfig\n (objects): options related to the configuration.\n\n\nappend\n (list of objects): a list of the configs to be appended to the current config.\n\n\nsource\n (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and \ndata\n. Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the config.\n\n\nhash\n (object): the hash of the config\n\n\nfunction\n (string): the function used to hash the config. Supported functions are sha512.\n\n\nsum\n (string): the resulting sum of the hash applied to the contents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreplace\n (object): the config that will replace the current.\n\n\nsource\n (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and \ndata\n. Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the config.\n\n\nhash\n (object): the hash of the config\n\n\nfunction\n (string): the function used to hash the config. Supported functions are sha512.\n\n\nsum\n (string): the resulting sum of the hash applied to the contents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntimeouts\n (object): options relating to http timeouts when fetching files over http or https.\n\n\nhttp_response_headers\n (integer): the time to wait (in seconds) for the server's response headers (but not the body) after making a request. 0 indicates no timeout. Default is 10 seconds.\n\n\nhttp_total\n (integer): the time limit (in seconds) for the operation (connection, request, and response), including retries. 0 indicates no timeout. Default is 0.\n\n\n\n\n\n\nsecurity\n (object): options relating to network security.\n\n\ntls\n (object): options relating to TLS when fetching resources over \nhttps\n.\n\n\ncertificate_authorities\n (object): the list of additional certificate authorities (in addition to the system authorities) to be used for TLS verification when fetching over \nhttps\n.\n\n\nsource\n (string, required): the URL of the certificate (in PEM format). Supported schemes are \nhttp\n, \nhttps\n, \ns3\n, \ntftp\n, and \ndata\n. Note: When using \nhttp\n, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the certificate.\n\n\nhash\n (string): the hash of the certificate, in the form \n<type>-<value>\n where type is sha512.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstorage\n (object): describes the desired state of the system's storage devices.\n\n\ndisks\n (list of objects): the list of disks to be configured and their options.\n\n\ndevice\n (string, required): the absolute path to the device. Devices are typically referenced by the \n/dev/disk/by-*\n symlinks.\n\n\nwipe_table\n (boolean): whether or not the partition tables shall be wiped. When true, the partition tables are erased before any further manipulation. Otherwise, the existing entries are left intact.\n\n\npartitions\n (list of objects): the list of partitions and their configuration for this particular disk.\n\n\nlabel\n (string): the PARTLABEL for the partition.\n\n\nnumber\n (integer): the partition number, which dictates it's position in the partition table (one-indexed). If zero, use the next available partition slot.\n\n\nsize\n (string): the size of the partition with a unit (KiB, MiB, GiB). If zero, the partition will fill the remainder of the disk.\n\n\nstart\n (string): the start of the partition with a unit (KiB, MiB, GiB). If zero, the partition will be positioned at the earliest available part of the disk.\n\n\ntype_guid\n (string): the GPT \npartition type GUID\n. If omitted, the default will be 0FC63DAF-8483-4772-8E79-3D69D8477DE4 (Linux filesystem data). The keywords \nlinux_filesystem_data\n, \nraid_partition\n, \nswap_partition\n, and \nraid_containing_root\n can also be used.\n\n\nguid\n (string): the GPT unique partition GUID.\n\n\n\n\n\n\n\n\n\n\nraid\n (list of objects): the list of RAID arrays to be configured.\n\n\nname\n (string, required): the name to use for the resulting md device.\n\n\nlevel\n (string, required): the redundancy level of the array (e.g. linear, raid1, raid5, etc.).\n\n\ndevices\n (list of strings, required): the list of devices (referenced by their absolute path) in the array.\n\n\nspares\n (integer): the number of spares (if applicable) in the array.\n\n\noptions\n (list of strings): any additional options to be passed to mdadm.\n\n\n\n\n\n\nfilesystems\n (list of objects): the list of filesystems to be configured and/or used in the \"files\" section. Either \"mount\" or \"path\" needs to be specified.\n\n\nname\n (string): the identifier for the filesystem, internal to Ignition. This is only required if the filesystem needs to be referenced in the \"files\" section.\n\n\nmount\n (object): contains the set of mount and formatting options for the filesystem. A non-null entry indicates that the filesystem should be mounted before it is used by Ignition.\n\n\ndevice\n (string, required): the absolute path to the device. Devices are typically referenced by the \n/dev/disk/by-*\n symlinks.\n\n\nformat\n (string, required): the filesystem format (ext4, btrfs, or xfs).\n\n\nwipe_filesystem\n (boolean): whether or not to wipe the device before filesystem creation, see \nIgnition's documentation on filesystems\n for more information.\n\n\nlabel\n (string): the label of the filesystem.\n\n\nuuid\n (string): the uuid of the filesystem.\n\n\noptions\n (list of strings): any additional options to be passed to the format-specific mkfs utility.\n\n\ncreate\n (object, DEPRECATED): contains the set of options to be used when creating the filesystem. A non-null entry indicates that the filesystem shall be created.\n\n\nforce\n (boolean, DEPRECATED): whether or not the create operation shall overwrite an existing filesystem.\n\n\noptions\n (list of strings, DEPRECATED): any additional options to be passed to the format-specific mkfs utility.\n\n\n\n\n\n\n\n\n\n\npath\n (string): the mount-point of the filesystem. A non-null entry indicates that the filesystem has already been mounted by the system at the specified path. This is really only useful for \"/sysroot\".\n\n\n\n\n\n\nfiles\n (list of objects): the list of files, rooted in this particular filesystem, to be written.\n\n\nfilesystem\n (string, required): the internal identifier of the filesystem. This matches the last filesystem with the given identifier.\n\n\npath\n (string, required): the absolute path to the file.\n\n\noverwrite\n (boolean): whether to delete preexisting nodes at the path. Defaults to true.\n\n\nappend\n (boolean): whether to append to the specified file. Creates a new file if nothing exists at the path. Cannot be set if overwrite is set to true.\n\n\ncontents\n (object): options related to the contents of the file.\n\n\ninline\n (string): the contents of the file.\n\n\nlocal\n (string): the path to a local file, relative to the \n--files-dir\n directory. When using local files, the \n--files-dir\n flag must be passed to \nct\n. The file contents are included in the generated config.\n\n\nremote\n (object): options related to the fetching of remote file contents. Remote files are fetched by Ignition when Ignition runs, the contents are not included in the generated config.\n\n\ncompression\n (string): the type of compression used on the contents (null or gzip)\n\n\nurl\n (string): the URL of the file contents. Supported schemes are http, https, tftp, s3, and \ndata\n. Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.\n\n\nverification\n (object): options related to the verification of the file contents.\n\n\nhash\n (object): the hash of the config\n\n\nfunction\n (string): the function used to hash the config. Supported functions are sha512.\n\n\nsum\n (string): the resulting sum of the hash applied to the contents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmode\n (integer): the file's permission mode.\n\n\nuser\n (object): specifies the file's owner.\n\n\nid\n (integer): the user ID of the owner.\n\n\nname\n (string): the user name of the owner.\n\n\n\n\n\n\ngroup\n (object): specifies the group of the owner.\n\n\nid\n (integer): the group ID of the owner.\n\n\nname\n (string): the group name of the owner.\n\n\n\n\n\n\n\n\n\n\ndirectories\n (list of objects): the list of directories to be created.\n\n\nfilesystem\n (string, required): the internal identifier of the filesystem in which to create the directory. This matches the last filesystem with the given identifier.\n\n\npath\n (string, required): the absolute path to the directory.\n\n\noverwrite\n (boolean): whether to delete preexisting nodes at the path.\n\n\nmode\n (integer): the directory's permission mode.\n\n\nuser\n (object): specifies the directory's owner.\n\n\nid\n (integer): the user ID of the owner.\n\n\nname\n (string): the user name of the owner.\n\n\n\n\n\n\ngroup\n (object): specifies the group of the owner.\n\n\nid\n (integer): the group ID of the owner.\n\n\nname\n (string): the group name of the owner.\n\n\n\n\n\n\n\n\n\n\nlinks\n (list of objects): the list of links to be created\n\n\nfilesystem\n (string, required): the internal identifier of the filesystem in which to write the link. This matches the last filesystem with the given identifier.\n\n\npath\n (string, required): the absolute path to the link\n\n\noverwrite\n (boolean): whether to delete preexisting nodes at the path.\n\n\nuser\n (object): specifies the symbolic link's owner.\n\n\nid\n (integer): the user ID of the owner.\n\n\nname\n (string): the user name of the owner.\n\n\n\n\n\n\ngroup\n (object): specifies the group of the owner.\n\n\nid\n (integer): the group ID of the owner.\n\n\nname\n (string): the group name of the owner.\n\n\n\n\n\n\ntarget\n (string, required): the target path of the link\n\n\nhard\n (boolean): a symbolic link is created if this is false, a hard one if this is true.\n\n\n\n\n\n\n\n\n\n\nsystemd\n (object): describes the desired state of the systemd units.\n\n\nunits\n (list of objects): the list of systemd units.\n\n\nname\n (string, required): the name of the unit. This must be suffixed with a valid unit type (e.g. \"thing.service\").\n\n\nenable\n (boolean, DEPRECATED): whether or not the service shall be enabled. When true, the service is enabled. In order for this to have any effect, the unit must have an install section.\n\n\nenabled\n (boolean): whether or not the service shall be enabled. When true, the service is enabled. When false, the service is disabled. When omitted, the service is unmodified. In order for this to have any effect, the unit must have an install section.\n\n\nmask\n (boolean): whether or not the service shall be masked. When true, the service is masked by symlinking it to \n/dev/null\n.\n\n\ncontents\n (string): the contents of the unit.\n\n\ndropins\n (list of objects): the list of drop-ins for the unit.\n\n\nname\n (string, required): the name of the drop-in. This must be suffixed with \".conf\".\n\n\ncontents\n (string): the contents of the drop-in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnetworkd\n (object): describes the desired state of the networkd files.\n\n\nunits\n (list of objects): the list of networkd files.\n\n\nname\n (string, required): the name of the file. This must be suffixed with a valid unit type (e.g. \"00-eth0.network\").\n\n\ncontents\n (string): the contents of the networkd file.\n\n\ndropins\n (list of objects): the list of drop-ins for the unit.\n\n\nname\n (string, required): the name of the drop-in. This must be suffixed with \".conf\".\n\n\ncontents\n (string): the contents of the drop-in.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npasswd\n (object): describes the desired additions to the passwd database.\n\n\nusers\n (list of objects): the list of accounts that shall exist.\n\n\nname\n (string, required): the username for the account.\n\n\npassword_hash\n (string): the encrypted password for the account.\n\n\nssh_authorized_keys\n (list of strings): a list of SSH keys to be added to the user's authorized_keys.\n\n\nuid\n (integer): the user ID of the account.\n\n\ngecos\n (string): the GECOS field of the account.\n\n\nhome_dir\n (string): the home directory of the account.\n\n\nno_create_home\n (boolean): whether or not to create the user's home directory. This only has an effect if the account doesn't exist yet.\n\n\nprimary_group\n (string): the name of the primary group of the account.\n\n\ngroups\n (list of strings): the list of supplementary groups of the account.\n\n\nno_user_group\n (boolean): whether or not to create a group with the same name as the user. This only has an effect if the account doesn't exist yet.\n\n\nno_log_init\n (boolean): whether or not to add the user to the lastlog and faillog databases. This only has an effect if the account doesn't exist yet.\n\n\nshell\n (string): the login shell of the new account.\n\n\nsystem\n (bool): whether or not to make the account a system account. This only has an effect if the account doesn't exist yet.\n\n\ncreate\n (object, DEPRECATED): contains the set of options to be used when creating the user. A non-null entry indicates that the user account shall be created.\n\n\nuid\n (integer, DEPRECATED): the user ID of the new account.\n\n\ngecos\n (string, DEPRECATED): the GECOS field of the new account.\n\n\nhome_dir\n (string, DEPRECATED): the home directory of the new account.\n\n\nno_create_home\n (boolean, DEPRECATED): whether or not to create the user's home directory.\n\n\nprimary_group\n (string, DEPRECATED): the name or ID of the primary group of the new account.\n\n\ngroups\n (list of strings, DEPRECATED): the list of supplementary groups of the new account.\n\n\nno_user_group\n (boolean, DEPRECATED): whether or not to create a group with the same name as the user.\n\n\nno_log_init\n (boolean, DEPRECATED): whether or not to add the user to the lastlog and faillog databases.\n\n\nshell\n (string, DEPRECATED): the login shell of the new account.\n\n\n\n\n\n\n\n\n\n\ngroups\n (list of objects): the list of groups to be added.\n\n\nname\n (string, required): the name of the group.\n\n\ngid\n (integer): the group ID of the new group.\n\n\npassword_hash\n (string): the encrypted password of the new group.\n\n\n\n\n\n\n\n\n\n\netcd\n\n\nversion\n (string): the version of etcd to be run\n\n\nother options\n (string): this section accepts any valid etcd options for the version of etcd specified. For a comprehensive list, please consult etcd's documentation. Note all options here should be in snake_case, not spine-case.\n\n\n\n\n\n\nflannel\n\n\nversion\n (string): the version of flannel to be run\n\n\nnetwork_config\n (string): the flannel configuration to be written into etcd before flannel starts.\n\n\nother options\n (string): this section accepts any valid flannel options for the version of flannel specified. For a comprehensive list, please consult flannel's documentation. Note all options here should be in snake_case, not spine-case.\n\n\n\n\n\n\ndocker\n\n\nflags\n (list of strings): additional flags to pass to the docker daemon when it is started\n\n\n\n\n\n\nupdate\n\n\ngroup\n (string): the update group to follow. Most users will want one of: stable, beta, alpha.\n\n\nserver\n (string): the server to fetch updates from.\n\n\n\n\n\n\nlocksmith\n\n\nreboot_strategy\n (string): the reboot strategy for locksmithd to follow. Must be one of: reboot, etcd-lock, off.\n\n\nwindow_start\n (string, required if window-length isn't empty): the start of the window that locksmithd can reboot the machine during\n\n\nwindow_length\n (string, required if window-start isn't empty): the duration of the window that locksmithd can reboot the machine during\n\n\ngroup\n (string): the locksmith etcd group to be part of for reboot control\n\n\netcd_endpoints\n (string): the endpoints of etcd locksmith should use\n\n\netcd_cafile\n (string): the tls CA file to use when communicating with etcd\n\n\netcd_certfile\n (string): the tls cert file to use when communicating with etcd\n\n\netcd_keyfile\n (string): the tls key file to use when communicating with etcd",
            "title": "Configuration Specification #"
        },
        {
            "location": "/container-linux-config-transpiler/doc/configuration/#configuration-specification",
            "text": "A Container Linux Configuration, to be processed by ct, is a YAML document conforming to the following specification:  Note: all fields are optional unless otherwise marked   ignition  (object): metadata about the configuration itself.  config  (objects): options related to the configuration.  append  (list of objects): a list of the configs to be appended to the current config.  source  (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and  data . Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the config.  hash  (object): the hash of the config  function  (string): the function used to hash the config. Supported functions are sha512.  sum  (string): the resulting sum of the hash applied to the contents.        replace  (object): the config that will replace the current.  source  (string, required): the URL of the config. Supported schemes are http, https, s3, tftp, and  data . Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the config.  hash  (object): the hash of the config  function  (string): the function used to hash the config. Supported functions are sha512.  sum  (string): the resulting sum of the hash applied to the contents.          timeouts  (object): options relating to http timeouts when fetching files over http or https.  http_response_headers  (integer): the time to wait (in seconds) for the server's response headers (but not the body) after making a request. 0 indicates no timeout. Default is 10 seconds.  http_total  (integer): the time limit (in seconds) for the operation (connection, request, and response), including retries. 0 indicates no timeout. Default is 0.    security  (object): options relating to network security.  tls  (object): options relating to TLS when fetching resources over  https .  certificate_authorities  (object): the list of additional certificate authorities (in addition to the system authorities) to be used for TLS verification when fetching over  https .  source  (string, required): the URL of the certificate (in PEM format). Supported schemes are  http ,  https ,  s3 ,  tftp , and  data . Note: When using  http , it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the certificate.  hash  (string): the hash of the certificate, in the form  <type>-<value>  where type is sha512.            storage  (object): describes the desired state of the system's storage devices.  disks  (list of objects): the list of disks to be configured and their options.  device  (string, required): the absolute path to the device. Devices are typically referenced by the  /dev/disk/by-*  symlinks.  wipe_table  (boolean): whether or not the partition tables shall be wiped. When true, the partition tables are erased before any further manipulation. Otherwise, the existing entries are left intact.  partitions  (list of objects): the list of partitions and their configuration for this particular disk.  label  (string): the PARTLABEL for the partition.  number  (integer): the partition number, which dictates it's position in the partition table (one-indexed). If zero, use the next available partition slot.  size  (string): the size of the partition with a unit (KiB, MiB, GiB). If zero, the partition will fill the remainder of the disk.  start  (string): the start of the partition with a unit (KiB, MiB, GiB). If zero, the partition will be positioned at the earliest available part of the disk.  type_guid  (string): the GPT  partition type GUID . If omitted, the default will be 0FC63DAF-8483-4772-8E79-3D69D8477DE4 (Linux filesystem data). The keywords  linux_filesystem_data ,  raid_partition ,  swap_partition , and  raid_containing_root  can also be used.  guid  (string): the GPT unique partition GUID.      raid  (list of objects): the list of RAID arrays to be configured.  name  (string, required): the name to use for the resulting md device.  level  (string, required): the redundancy level of the array (e.g. linear, raid1, raid5, etc.).  devices  (list of strings, required): the list of devices (referenced by their absolute path) in the array.  spares  (integer): the number of spares (if applicable) in the array.  options  (list of strings): any additional options to be passed to mdadm.    filesystems  (list of objects): the list of filesystems to be configured and/or used in the \"files\" section. Either \"mount\" or \"path\" needs to be specified.  name  (string): the identifier for the filesystem, internal to Ignition. This is only required if the filesystem needs to be referenced in the \"files\" section.  mount  (object): contains the set of mount and formatting options for the filesystem. A non-null entry indicates that the filesystem should be mounted before it is used by Ignition.  device  (string, required): the absolute path to the device. Devices are typically referenced by the  /dev/disk/by-*  symlinks.  format  (string, required): the filesystem format (ext4, btrfs, or xfs).  wipe_filesystem  (boolean): whether or not to wipe the device before filesystem creation, see  Ignition's documentation on filesystems  for more information.  label  (string): the label of the filesystem.  uuid  (string): the uuid of the filesystem.  options  (list of strings): any additional options to be passed to the format-specific mkfs utility.  create  (object, DEPRECATED): contains the set of options to be used when creating the filesystem. A non-null entry indicates that the filesystem shall be created.  force  (boolean, DEPRECATED): whether or not the create operation shall overwrite an existing filesystem.  options  (list of strings, DEPRECATED): any additional options to be passed to the format-specific mkfs utility.      path  (string): the mount-point of the filesystem. A non-null entry indicates that the filesystem has already been mounted by the system at the specified path. This is really only useful for \"/sysroot\".    files  (list of objects): the list of files, rooted in this particular filesystem, to be written.  filesystem  (string, required): the internal identifier of the filesystem. This matches the last filesystem with the given identifier.  path  (string, required): the absolute path to the file.  overwrite  (boolean): whether to delete preexisting nodes at the path. Defaults to true.  append  (boolean): whether to append to the specified file. Creates a new file if nothing exists at the path. Cannot be set if overwrite is set to true.  contents  (object): options related to the contents of the file.  inline  (string): the contents of the file.  local  (string): the path to a local file, relative to the  --files-dir  directory. When using local files, the  --files-dir  flag must be passed to  ct . The file contents are included in the generated config.  remote  (object): options related to the fetching of remote file contents. Remote files are fetched by Ignition when Ignition runs, the contents are not included in the generated config.  compression  (string): the type of compression used on the contents (null or gzip)  url  (string): the URL of the file contents. Supported schemes are http, https, tftp, s3, and  data . Note: When using http, it is advisable to use the verification option to ensure the contents haven't been modified.  verification  (object): options related to the verification of the file contents.  hash  (object): the hash of the config  function  (string): the function used to hash the config. Supported functions are sha512.  sum  (string): the resulting sum of the hash applied to the contents.          mode  (integer): the file's permission mode.  user  (object): specifies the file's owner.  id  (integer): the user ID of the owner.  name  (string): the user name of the owner.    group  (object): specifies the group of the owner.  id  (integer): the group ID of the owner.  name  (string): the group name of the owner.      directories  (list of objects): the list of directories to be created.  filesystem  (string, required): the internal identifier of the filesystem in which to create the directory. This matches the last filesystem with the given identifier.  path  (string, required): the absolute path to the directory.  overwrite  (boolean): whether to delete preexisting nodes at the path.  mode  (integer): the directory's permission mode.  user  (object): specifies the directory's owner.  id  (integer): the user ID of the owner.  name  (string): the user name of the owner.    group  (object): specifies the group of the owner.  id  (integer): the group ID of the owner.  name  (string): the group name of the owner.      links  (list of objects): the list of links to be created  filesystem  (string, required): the internal identifier of the filesystem in which to write the link. This matches the last filesystem with the given identifier.  path  (string, required): the absolute path to the link  overwrite  (boolean): whether to delete preexisting nodes at the path.  user  (object): specifies the symbolic link's owner.  id  (integer): the user ID of the owner.  name  (string): the user name of the owner.    group  (object): specifies the group of the owner.  id  (integer): the group ID of the owner.  name  (string): the group name of the owner.    target  (string, required): the target path of the link  hard  (boolean): a symbolic link is created if this is false, a hard one if this is true.      systemd  (object): describes the desired state of the systemd units.  units  (list of objects): the list of systemd units.  name  (string, required): the name of the unit. This must be suffixed with a valid unit type (e.g. \"thing.service\").  enable  (boolean, DEPRECATED): whether or not the service shall be enabled. When true, the service is enabled. In order for this to have any effect, the unit must have an install section.  enabled  (boolean): whether or not the service shall be enabled. When true, the service is enabled. When false, the service is disabled. When omitted, the service is unmodified. In order for this to have any effect, the unit must have an install section.  mask  (boolean): whether or not the service shall be masked. When true, the service is masked by symlinking it to  /dev/null .  contents  (string): the contents of the unit.  dropins  (list of objects): the list of drop-ins for the unit.  name  (string, required): the name of the drop-in. This must be suffixed with \".conf\".  contents  (string): the contents of the drop-in.        networkd  (object): describes the desired state of the networkd files.  units  (list of objects): the list of networkd files.  name  (string, required): the name of the file. This must be suffixed with a valid unit type (e.g. \"00-eth0.network\").  contents  (string): the contents of the networkd file.  dropins  (list of objects): the list of drop-ins for the unit.  name  (string, required): the name of the drop-in. This must be suffixed with \".conf\".  contents  (string): the contents of the drop-in.        passwd  (object): describes the desired additions to the passwd database.  users  (list of objects): the list of accounts that shall exist.  name  (string, required): the username for the account.  password_hash  (string): the encrypted password for the account.  ssh_authorized_keys  (list of strings): a list of SSH keys to be added to the user's authorized_keys.  uid  (integer): the user ID of the account.  gecos  (string): the GECOS field of the account.  home_dir  (string): the home directory of the account.  no_create_home  (boolean): whether or not to create the user's home directory. This only has an effect if the account doesn't exist yet.  primary_group  (string): the name of the primary group of the account.  groups  (list of strings): the list of supplementary groups of the account.  no_user_group  (boolean): whether or not to create a group with the same name as the user. This only has an effect if the account doesn't exist yet.  no_log_init  (boolean): whether or not to add the user to the lastlog and faillog databases. This only has an effect if the account doesn't exist yet.  shell  (string): the login shell of the new account.  system  (bool): whether or not to make the account a system account. This only has an effect if the account doesn't exist yet.  create  (object, DEPRECATED): contains the set of options to be used when creating the user. A non-null entry indicates that the user account shall be created.  uid  (integer, DEPRECATED): the user ID of the new account.  gecos  (string, DEPRECATED): the GECOS field of the new account.  home_dir  (string, DEPRECATED): the home directory of the new account.  no_create_home  (boolean, DEPRECATED): whether or not to create the user's home directory.  primary_group  (string, DEPRECATED): the name or ID of the primary group of the new account.  groups  (list of strings, DEPRECATED): the list of supplementary groups of the new account.  no_user_group  (boolean, DEPRECATED): whether or not to create a group with the same name as the user.  no_log_init  (boolean, DEPRECATED): whether or not to add the user to the lastlog and faillog databases.  shell  (string, DEPRECATED): the login shell of the new account.      groups  (list of objects): the list of groups to be added.  name  (string, required): the name of the group.  gid  (integer): the group ID of the new group.  password_hash  (string): the encrypted password of the new group.      etcd  version  (string): the version of etcd to be run  other options  (string): this section accepts any valid etcd options for the version of etcd specified. For a comprehensive list, please consult etcd's documentation. Note all options here should be in snake_case, not spine-case.    flannel  version  (string): the version of flannel to be run  network_config  (string): the flannel configuration to be written into etcd before flannel starts.  other options  (string): this section accepts any valid flannel options for the version of flannel specified. For a comprehensive list, please consult flannel's documentation. Note all options here should be in snake_case, not spine-case.    docker  flags  (list of strings): additional flags to pass to the docker daemon when it is started    update  group  (string): the update group to follow. Most users will want one of: stable, beta, alpha.  server  (string): the server to fetch updates from.    locksmith  reboot_strategy  (string): the reboot strategy for locksmithd to follow. Must be one of: reboot, etcd-lock, off.  window_start  (string, required if window-length isn't empty): the start of the window that locksmithd can reboot the machine during  window_length  (string, required if window-start isn't empty): the duration of the window that locksmithd can reboot the machine during  group  (string): the locksmith etcd group to be part of for reboot control  etcd_endpoints  (string): the endpoints of etcd locksmith should use  etcd_cafile  (string): the tls CA file to use when communicating with etcd  etcd_certfile  (string): the tls cert file to use when communicating with etcd  etcd_keyfile  (string): the tls key file to use when communicating with etcd",
            "title": "Configuration Specification"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/",
            "text": "Referencing dynamic data\n\u00b6\n\n\nOverview\n\u00b6\n\n\nSometimes it can be useful to refer to data in a Container Linux Config that isn't known until a machine boots, like its network address. This can be accomplished with \ncoreos-metadata\n. coreos-metadata is a very basic utility that fetches information about the current machine and makes it available for consumption. By making it a dependency of services which requires this information, systemd will ensure that coreos-metadata has successfully completed before starting these services. These services can then simply source the fetched information and let systemd perform the environment variable expansions.\n\n\nAs of version 0.2.0, ct has support for making this easy for users. In specific sections of a config, users can enter in dynamic data between \n{}\n, and ct will handle enabling the coreos-metadata service and using the information it provides.\n\n\nThe available information varies by provider, and is expressed in different variables by coreos-metadata. If this feature is used a \n--provider\n flag must be passed to ct. Currently, the \netcd\n and \nflannel\n sections are the only ones which support this feature.\n\n\nSupported data by provider\n\u00b6\n\n\nThis is the information available in each provider.\n\n\n\n\n\n\n\n\n\n\nHOSTNAME\n\n\nPRIVATE_IPV4\n\n\nPUBLIC_IPV4\n\n\nPRIVATE_IPV6\n\n\nPUBLIC_IPV6\n\n\n\n\n\n\n\n\n\n\nAzure\n\n\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nDigital Ocean\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\nEC2\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nGCE\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nPacket\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\u2713\n\n\n\n\n\n\nOpenStack-Metadata\n\n\n\u2713\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\nVagrant-Virtualbox\n\n\n\u2713\n\n\n\u2713\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom metadata providers\n\u00b6\n\n\nct\n also supports custom metadata providers. To use the \ncustom\n platform, modify the coreos-metadata service unit to execute your own custom metadata fetcher. The custom metadata fetcher must write an environment file \n/run/metadata/coreos\n defining a \nCOREOS_CUSTOM_*\n environment variable for every piece of dynamic data used in the specified Container Linux Config. The environment variables are the same as in the Container Linux Config, but prefixed with \nCOREOS_CUSTOM_\n.\n\n\nExample\n\u00b6\n\n\nAssume \nhttps://example.com/metadata-script.sh\n is a script which communicates with a metadata service and then writes the following file to \n/run/metadata/coreos\n:\n\nCOREOS_CUSTOM_HOSTNAME=foobar\nCOREOS_CUSTOM_PRIVATE_IPV4=<The instance's private ipv4 address>\nCOREOS_CUSTOM_PUBLIC_IPV4=<The instance's public ipv4 address>\n\n\nThe following Container Linux Config downloads the metadata fetching script, replaces the ExecStart line in \ncoreos-metadata\n service to use the script instead, and configures etcd using the metadata provided. Use the \n--platform=custom\n flag when transpiling.\n\nstorage:\n  files:\n    - filesystem: \"root\"\n      path: \"/opt/get-metadata.sh\"\n      mode: 0755\n      contents:\n        remote:\n          url: \"https://example.com/metadata-script.sh\"\n\nsystemd:\n  units:\n    - name: \"coreos-metadata.service\"\n      dropins:\n       - name: \"use-script.conf\"\n         contents: |\n           [Service]\n           # Empty ExecStart= prevents the previously defined ExecStart from running\n           ExecStart=\n           ExecStart=/opt/get-metadata.sh\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"\n\n\nBehind the scenes\n\u00b6\n\n\nFor a more in-depth walk through of how this feature works, let's look at the etcd example from the \nexamples document\n.\n\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"\n\n\n\nIf we give this example to ct with the \n--platform=ec2\n tag, it produces the following drop-in:\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nEnvironment=\"ETCD_IMAGE_TAG=v3.0.15\"\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --name=\"${COREOS_EC2_HOSTNAME}\" \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --initial-cluster=\"${COREOS_EC2_HOSTNAME}=http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2379\"\n\n\n\nThis drop-in specifies that etcd should run after the coreos-metadata service, and it uses \n/run/metadata/coreos\n as an \nEnvironmentFile\n. This enables the coreos-metadata service, and puts the information it discovers into environment variables. These environment variables are then expanded by systemd when the service starts, inserting the dynamic data into the command-line flags to etcd.",
            "title": "Referencing dynamic data"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#referencing-dynamic-data",
            "text": "",
            "title": "Referencing dynamic data"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#overview",
            "text": "Sometimes it can be useful to refer to data in a Container Linux Config that isn't known until a machine boots, like its network address. This can be accomplished with  coreos-metadata . coreos-metadata is a very basic utility that fetches information about the current machine and makes it available for consumption. By making it a dependency of services which requires this information, systemd will ensure that coreos-metadata has successfully completed before starting these services. These services can then simply source the fetched information and let systemd perform the environment variable expansions.  As of version 0.2.0, ct has support for making this easy for users. In specific sections of a config, users can enter in dynamic data between  {} , and ct will handle enabling the coreos-metadata service and using the information it provides.  The available information varies by provider, and is expressed in different variables by coreos-metadata. If this feature is used a  --provider  flag must be passed to ct. Currently, the  etcd  and  flannel  sections are the only ones which support this feature.",
            "title": "Overview"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#supported-data-by-provider",
            "text": "This is the information available in each provider.      HOSTNAME  PRIVATE_IPV4  PUBLIC_IPV4  PRIVATE_IPV6  PUBLIC_IPV6      Azure   \u2713  \u2713      Digital Ocean  \u2713  \u2713  \u2713  \u2713  \u2713    EC2  \u2713  \u2713  \u2713      GCE  \u2713  \u2713  \u2713      Packet  \u2713  \u2713  \u2713   \u2713    OpenStack-Metadata  \u2713  \u2713  \u2713      Vagrant-Virtualbox  \u2713  \u2713",
            "title": "Supported data by provider"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#custom-metadata-providers",
            "text": "ct  also supports custom metadata providers. To use the  custom  platform, modify the coreos-metadata service unit to execute your own custom metadata fetcher. The custom metadata fetcher must write an environment file  /run/metadata/coreos  defining a  COREOS_CUSTOM_*  environment variable for every piece of dynamic data used in the specified Container Linux Config. The environment variables are the same as in the Container Linux Config, but prefixed with  COREOS_CUSTOM_ .",
            "title": "Custom metadata providers"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#example",
            "text": "Assume  https://example.com/metadata-script.sh  is a script which communicates with a metadata service and then writes the following file to  /run/metadata/coreos : COREOS_CUSTOM_HOSTNAME=foobar\nCOREOS_CUSTOM_PRIVATE_IPV4=<The instance's private ipv4 address>\nCOREOS_CUSTOM_PUBLIC_IPV4=<The instance's public ipv4 address>  The following Container Linux Config downloads the metadata fetching script, replaces the ExecStart line in  coreos-metadata  service to use the script instead, and configures etcd using the metadata provided. Use the  --platform=custom  flag when transpiling. storage:\n  files:\n    - filesystem: \"root\"\n      path: \"/opt/get-metadata.sh\"\n      mode: 0755\n      contents:\n        remote:\n          url: \"https://example.com/metadata-script.sh\"\n\nsystemd:\n  units:\n    - name: \"coreos-metadata.service\"\n      dropins:\n       - name: \"use-script.conf\"\n         contents: |\n           [Service]\n           # Empty ExecStart= prevents the previously defined ExecStart from running\n           ExecStart=\n           ExecStart=/opt/get-metadata.sh\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"",
            "title": "Example"
        },
        {
            "location": "/container-linux-config-transpiler/doc/dynamic-data/#behind-the-scenes",
            "text": "For a more in-depth walk through of how this feature works, let's look at the etcd example from the  examples document .  etcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"  If we give this example to ct with the  --platform=ec2  tag, it produces the following drop-in:  [Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nEnvironment=\"ETCD_IMAGE_TAG=v3.0.15\"\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --name=\"${COREOS_EC2_HOSTNAME}\" \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --initial-cluster=\"${COREOS_EC2_HOSTNAME}=http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2379\"  This drop-in specifies that etcd should run after the coreos-metadata service, and it uses  /run/metadata/coreos  as an  EnvironmentFile . This enables the coreos-metadata service, and puts the information it discovers into environment variables. These environment variables are then expanded by systemd when the service starts, inserting the dynamic data into the command-line flags to etcd.",
            "title": "Behind the scenes"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/",
            "text": "Examples\n\u00b6\n\n\nHere you can find a bunch of simple examples for using ct, with some explanations about what they do. The examples here are in no way comprehensive, for a full list of all the options present in ct check out the \nconfiguration specification\n.\n\n\nUsers and groups\n\u00b6\n\n\npasswd:\n  users:\n    - name: core\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n\n\n\nThis example modifies the existing \ncore\n user, giving it a known password hash (this will enable login via password), and setting its ssh key.\n\n\npasswd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n        - key2\n    - name: user2\n      ssh_authorized_keys:\n        - key3\n\n\n\nThis example will create two users, \nuser1\n and \nuser2\n. The first user has a password set and two ssh public keys authorized to log in as the user. The second user doesn't have a password set (so log in via password will be disabled), but have one ssh key.\n\n\npasswd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n      home_dir: /home/user1\n      no_create_home: true\n      groups:\n        - wheel\n        - plugdev\n      shell: /bin/bash\n\n\n\nThis example creates one user, \nuser1\n, with the password hash \n$6$43y3tkl...\n, and sets up one ssh public key for the user. The user is also given the home directory \n/home/user1\n, but it's not created, the user is added to the \nwheel\n and \nplugdev\n groups, and the user's shell is set to \n/bin/bash\n.\n\n\nGenerating a password hash\n\u00b6\n\n\nIf you choose to use a password instead of an SSH key, generating a safe hash is extremely important to the security of your system. Simplified hashes like md5crypt are trivial to crack on modern GPU hardware. Here are a few ways to generate secure hashes:\n\n\n# On Debian/Ubuntu (via the package \"whois\")\nmkpasswd --method=SHA-512 --rounds=4096\n\n# OpenSSL (note: this will only make md5crypt.  While better than plantext it should not be considered fully secure)\nopenssl passwd -1\n\n# Python\npython -c \"import crypt,random,string; print(crypt.crypt(input('clear-text password: '), '\\$6\\$' + ''.join([random.choice(string.ascii_letters + string.digits) for _ in range(16)])))\"\n\n# Perl (change password and salt values)\nperl -e 'print crypt(\"password\",\"\\$6\\$SALT\\$\") . \"\\n\"'\n\n\n\nUsing a higher number of rounds will help create more secure passwords, but given enough time, password hashes can be reversed.  On most RPM based distributions there is a tool called mkpasswd available in the \nexpect\n package, but this does not handle \"rounds\" nor advanced hashing algorithms.\n\n\nStorage and files\n\u00b6\n\n\nFiles\n\u00b6\n\n\nstorage:\n  files:\n    - path: /opt/file1\n      filesystem: root\n      contents:\n        inline: Hello, world!\n      mode: 0644\n      user:\n        id: 500\n      group:\n        id: 501\n\n\n\nThis example creates a file at \n/opt/file\n with the contents \nHello, world!\n, permissions 0644 (so readable and writable by the owner, and only readable by everyone else), and the file is owned by user uid 500 and gid 501.\n\n\nstorage:\n  files:\n    - path: /opt/file2\n      filesystem: root\n      contents:\n        remote:\n          url: http://example.com/file2\n          compression: gzip\n          verification:\n            hash:\n              function: sha512\n              sum: 4ee6a9d20cc0e6c7ee187daffa6822bdef7f4cebe109eff44b235f97e45dc3d7a5bb932efc841192e46618f48a6f4f5bc0d15fd74b1038abf46bf4b4fd409f2e\n      mode: 0644\n\n\n\nThis example fetches a gzip-compressed file from \nhttp://example.com/file2\n, makes sure that it matches the provided sha512 hash, and writes it to \n/opt/file2\n.\n\n\nFilesystems\n\u00b6\n\n\nstorage:\n  filesystems:\n    - name: filesystem1\n      mount:\n        device: /dev/disk/by-partlabel/ROOT\n        format: btrfs\n        wipe_filesystem: true\n        label: ROOT\n\n\n\nThis example formats the root filesystem to be \nbtrfs\n, and names it \nfilesystem1\n (primarily for use in the \nfiles\n section).\n\n\nsystemd units\n\u00b6\n\n\nsystemd:\n  units:\n    - name: etcd-member.service\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=infra0\"\n\n\n\nThis example adds a drop-in for the \netcd-member\n unit, setting the name for etcd to \ninfra0\n with an environment variable. More information on systemd dropins can be found in \nthe docs\n.\n\n\nsystemd:\n  units:\n    - name: hello.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=A hello world unit!\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo \"Hello, World!\"\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nThis example creates a new systemd unit called hello.service, enables it so it will run on boot, and defines the contents to simply echo \n\"Hello, World!\"\n.\n\n\nnetworkd units\n\u00b6\n\n\nnetworkd:\n  units:\n    - name: static.network\n      contents: |\n        [Match]\n        Name=enp2s0\n\n        [Network]\n        Address=192.168.0.15/24\n        Gateway=192.168.0.1\n\n\n\nThis example creates a networkd unit to set the IP address on the \nenp2s0\n interface to the static address \n192.168.0.15/24\n, and sets an appropriate gateway. More information on networkd units in CoreOS can be found in \nthe docs\n.\n\n\netcd\n\u00b6\n\n\netcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"\n\n\n\nThis example will create a dropin for the \netcd-member\n systemd unit, configuring it to use the specified version and adding all the specified options. This will also enable the \netcd-member\n unit.\n\n\nThis is referencing dynamic data that isn't known until an instance is booted. For more information on how this works, please take a look at the \nreferencing dynamic data\n document.\n\n\nUpdates and Locksmithd\n\u00b6\n\n\nupdate:\n  group:  \"beta\"\nlocksmith:\n  reboot_strategy: \"etcd-lock\"\n  window_start:    \"Sun 1:00\"\n  window_length:   \"2h\"\n\n\n\nThis example configures the Container Linux instance to be a member of the beta group, configures locksmithd to acquire a lock in etcd before rebooting for an update, and only allows reboots during a 2 hour window starting at 1 AM on Sundays.",
            "title": "Examples"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#examples",
            "text": "Here you can find a bunch of simple examples for using ct, with some explanations about what they do. The examples here are in no way comprehensive, for a full list of all the options present in ct check out the  configuration specification .",
            "title": "Examples"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#users-and-groups",
            "text": "passwd:\n  users:\n    - name: core\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1  This example modifies the existing  core  user, giving it a known password hash (this will enable login via password), and setting its ssh key.  passwd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n        - key2\n    - name: user2\n      ssh_authorized_keys:\n        - key3  This example will create two users,  user1  and  user2 . The first user has a password set and two ssh public keys authorized to log in as the user. The second user doesn't have a password set (so log in via password will be disabled), but have one ssh key.  passwd:\n  users:\n    - name: user1\n      password_hash: \"$6$43y3tkl...\"\n      ssh_authorized_keys:\n        - key1\n      home_dir: /home/user1\n      no_create_home: true\n      groups:\n        - wheel\n        - plugdev\n      shell: /bin/bash  This example creates one user,  user1 , with the password hash  $6$43y3tkl... , and sets up one ssh public key for the user. The user is also given the home directory  /home/user1 , but it's not created, the user is added to the  wheel  and  plugdev  groups, and the user's shell is set to  /bin/bash .",
            "title": "Users and groups"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#generating-a-password-hash",
            "text": "If you choose to use a password instead of an SSH key, generating a safe hash is extremely important to the security of your system. Simplified hashes like md5crypt are trivial to crack on modern GPU hardware. Here are a few ways to generate secure hashes:  # On Debian/Ubuntu (via the package \"whois\")\nmkpasswd --method=SHA-512 --rounds=4096\n\n# OpenSSL (note: this will only make md5crypt.  While better than plantext it should not be considered fully secure)\nopenssl passwd -1\n\n# Python\npython -c \"import crypt,random,string; print(crypt.crypt(input('clear-text password: '), '\\$6\\$' + ''.join([random.choice(string.ascii_letters + string.digits) for _ in range(16)])))\"\n\n# Perl (change password and salt values)\nperl -e 'print crypt(\"password\",\"\\$6\\$SALT\\$\") . \"\\n\"'  Using a higher number of rounds will help create more secure passwords, but given enough time, password hashes can be reversed.  On most RPM based distributions there is a tool called mkpasswd available in the  expect  package, but this does not handle \"rounds\" nor advanced hashing algorithms.",
            "title": "Generating a password hash"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#storage-and-files",
            "text": "",
            "title": "Storage and files"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#files",
            "text": "storage:\n  files:\n    - path: /opt/file1\n      filesystem: root\n      contents:\n        inline: Hello, world!\n      mode: 0644\n      user:\n        id: 500\n      group:\n        id: 501  This example creates a file at  /opt/file  with the contents  Hello, world! , permissions 0644 (so readable and writable by the owner, and only readable by everyone else), and the file is owned by user uid 500 and gid 501.  storage:\n  files:\n    - path: /opt/file2\n      filesystem: root\n      contents:\n        remote:\n          url: http://example.com/file2\n          compression: gzip\n          verification:\n            hash:\n              function: sha512\n              sum: 4ee6a9d20cc0e6c7ee187daffa6822bdef7f4cebe109eff44b235f97e45dc3d7a5bb932efc841192e46618f48a6f4f5bc0d15fd74b1038abf46bf4b4fd409f2e\n      mode: 0644  This example fetches a gzip-compressed file from  http://example.com/file2 , makes sure that it matches the provided sha512 hash, and writes it to  /opt/file2 .",
            "title": "Files"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#filesystems",
            "text": "storage:\n  filesystems:\n    - name: filesystem1\n      mount:\n        device: /dev/disk/by-partlabel/ROOT\n        format: btrfs\n        wipe_filesystem: true\n        label: ROOT  This example formats the root filesystem to be  btrfs , and names it  filesystem1  (primarily for use in the  files  section).",
            "title": "Filesystems"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#systemd-units",
            "text": "systemd:\n  units:\n    - name: etcd-member.service\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=infra0\"  This example adds a drop-in for the  etcd-member  unit, setting the name for etcd to  infra0  with an environment variable. More information on systemd dropins can be found in  the docs .  systemd:\n  units:\n    - name: hello.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=A hello world unit!\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo \"Hello, World!\"\n\n        [Install]\n        WantedBy=multi-user.target  This example creates a new systemd unit called hello.service, enables it so it will run on boot, and defines the contents to simply echo  \"Hello, World!\" .",
            "title": "systemd units"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#networkd-units",
            "text": "networkd:\n  units:\n    - name: static.network\n      contents: |\n        [Match]\n        Name=enp2s0\n\n        [Network]\n        Address=192.168.0.15/24\n        Gateway=192.168.0.1  This example creates a networkd unit to set the IP address on the  enp2s0  interface to the static address  192.168.0.15/24 , and sets an appropriate gateway. More information on networkd units in CoreOS can be found in  the docs .",
            "title": "networkd units"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#etcd",
            "text": "etcd:\n  version:                     \"3.0.15\"\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"{HOSTNAME}=http://{PRIVATE_IPV4}:2380\"  This example will create a dropin for the  etcd-member  systemd unit, configuring it to use the specified version and adding all the specified options. This will also enable the  etcd-member  unit.  This is referencing dynamic data that isn't known until an instance is booted. For more information on how this works, please take a look at the  referencing dynamic data  document.",
            "title": "etcd"
        },
        {
            "location": "/container-linux-config-transpiler/doc/examples/#updates-and-locksmithd",
            "text": "update:\n  group:  \"beta\"\nlocksmith:\n  reboot_strategy: \"etcd-lock\"\n  window_start:    \"Sun 1:00\"\n  window_length:   \"2h\"  This example configures the Container Linux instance to be a member of the beta group, configures locksmithd to acquire a lock in etcd before rebooting for an update, and only allows reboots during a 2 hour window starting at 1 AM on Sundays.",
            "title": "Updates and Locksmithd"
        },
        {
            "location": "/container-linux-config-transpiler/doc/getting-started/",
            "text": "Getting started\n\u00b6\n\n\nct is a tool that will consume a Container Linux Config and produce a JSON file that can be given to a Container Linux machine when it first boots to set the machine up. Using this config, a machine can be told to create users, format the root filesystem, set up the network, install systemd units, and more.\n\n\nContainer Linux Configs are YAML files conforming to ct's schema. For more information on the schema, take a look at \ndoc/configuration.md\n.\n\n\nAs a simple example, let's use ct to set the authorized ssh key for the core user on a Container Linux machine.\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc...\n\n\n\nIn this above file, you'll want to set the \nssh-rsa AAAAB3NzaC1yc...\n line to be your ssh public key (which is probably the contents of \n~/.ssh/id_rsa.pub\n, if you're on Linux).\n\n\nIf we take this file and give it to ct:\n\n\n$ ./bin/ct --in-file example.yaml\n{\"ignition\":{\"version\":\"2.0.0\",\"config\":{}},\"storage\":{},\"systemd\":{},\"networkd\":{},\"passwd\":{\"users\":[{\"name\":\"core\",\"sshAuthorizedKeys\":[\"ssh-rsa AAAAB3NzaC1yc...\"]}]}}\n\n\n\nWe can see that it produces a JSON file. This file isn't intended to be human-friendly, and will definitely be a pain to read/edit (especially if you have multi-line things like systemd units). Luckily, you shouldn't have to care about this file! Just provide it to a booting Container Linux machine and Ignition, the utility inside of Container Linux that receives this file, will know what to do with it.\n\n\nThe method by which this file is provided to a Container Linux machine depends on the environment in which the machine is running. For instructions on a given provider, head over to the \nlist of supported platforms for Ignition\n.\n\n\nTo see some examples for what else ct can do, head over to the \nexamples\n.",
            "title": "Getting started"
        },
        {
            "location": "/container-linux-config-transpiler/doc/getting-started/#getting-started",
            "text": "ct is a tool that will consume a Container Linux Config and produce a JSON file that can be given to a Container Linux machine when it first boots to set the machine up. Using this config, a machine can be told to create users, format the root filesystem, set up the network, install systemd units, and more.  Container Linux Configs are YAML files conforming to ct's schema. For more information on the schema, take a look at  doc/configuration.md .  As a simple example, let's use ct to set the authorized ssh key for the core user on a Container Linux machine.  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc...  In this above file, you'll want to set the  ssh-rsa AAAAB3NzaC1yc...  line to be your ssh public key (which is probably the contents of  ~/.ssh/id_rsa.pub , if you're on Linux).  If we take this file and give it to ct:  $ ./bin/ct --in-file example.yaml\n{\"ignition\":{\"version\":\"2.0.0\",\"config\":{}},\"storage\":{},\"systemd\":{},\"networkd\":{},\"passwd\":{\"users\":[{\"name\":\"core\",\"sshAuthorizedKeys\":[\"ssh-rsa AAAAB3NzaC1yc...\"]}]}}  We can see that it produces a JSON file. This file isn't intended to be human-friendly, and will definitely be a pain to read/edit (especially if you have multi-line things like systemd units). Luckily, you shouldn't have to care about this file! Just provide it to a booting Container Linux machine and Ignition, the utility inside of Container Linux that receives this file, will know what to do with it.  The method by which this file is provided to a Container Linux machine depends on the environment in which the machine is running. For instructions on a given provider, head over to the  list of supported platforms for Ignition .  To see some examples for what else ct can do, head over to the  examples .",
            "title": "Getting started"
        },
        {
            "location": "/container-linux-config-transpiler/doc/operators-notes/",
            "text": "Operator Notes\n\u00b6\n\n\nType GUID aliases\n\u00b6\n\n\nThe Config Transpiler supports several aliases for GPT partition type GUIDs. They are as follows:\n\n\n\n\n\n\n\n\nAlias Name\n\n\nResolved Type GUID\n\n\n\n\n\n\n\n\n\n\nraid_containing_root\n\n\nbe9067b9-ea49-4f15-b4f6-f36f8c9e1818\n\n\n\n\n\n\nlinux_filesystem_data\n\n\n0fc63daf-8483-4772-8e79-3d69d8477de4\n\n\n\n\n\n\nswap_partition\n\n\n0657fd6d-a4ab-43c4-84e5-0933c84b4f4f\n\n\n\n\n\n\nraid_partition\n\n\na19d880f-05fc-4d3b-a006-743f0f84911e\n\n\n\n\n\n\n\n\nSee the \nRoot Filesystem Placement\n documentation for when to use \nraid_containing_root\n.",
            "title": "Operator Notes"
        },
        {
            "location": "/container-linux-config-transpiler/doc/operators-notes/#operator-notes",
            "text": "",
            "title": "Operator Notes"
        },
        {
            "location": "/container-linux-config-transpiler/doc/operators-notes/#type-guid-aliases",
            "text": "The Config Transpiler supports several aliases for GPT partition type GUIDs. They are as follows:     Alias Name  Resolved Type GUID      raid_containing_root  be9067b9-ea49-4f15-b4f6-f36f8c9e1818    linux_filesystem_data  0fc63daf-8483-4772-8e79-3d69d8477de4    swap_partition  0657fd6d-a4ab-43c4-84e5-0933c84b4f4f    raid_partition  a19d880f-05fc-4d3b-a006-743f0f84911e     See the  Root Filesystem Placement  documentation for when to use  raid_containing_root .",
            "title": "Type GUID aliases"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/",
            "text": "Config transpiler overview\n\u00b6\n\n\nThe \nConfig Transpiler\n, ct, is the utility responsible for transforming a user-provided Container Linux Configuration into an \nIgnition\n configuration. The resulting Ignition config can then be provided to a Container Linux machine when it first boots in order to provision it.\n\n\nThe Container Linux Config is intended to be human-friendly, and is thus in YAML. The syntax is rather forgiving, and things like references and multi-line strings are supported.\n\n\nThe resulting Ignition config is very much not intended to be human-friendly. It is an artifact produced by ct that users should simply pass along to their machines. JSON was chosen over a binary format to make the process more transparent and to allow power users to inspect/modify what ct produces, but it would have worked fine if the result from ct had not been human readable at all.\n\n\nWhy a two-step process?\n\u00b6\n\n\nThere are a couple factors motivating the decision to not incorporate support for Container Linux Configs directly into the boot process of Container Linux (as in, the ability to provide a Container Linux Config directly to a booting machine, instead of an Ignition config).\n\n\n\n\nBy making users run their configs through ct before they attempt to boot a machine, issues with their configs can be caught before any machine attempts to boot. This will save users time, as they can much more quickly find problems with their configs. Were users to provide Container Linux Configs directly to machines at first boot, they would need to find a way to extract the Ignition logs from a machine that may have failed to boot, which can be a slow and tedious process.\n\n\nYAML parsing is a complex process that in the past has been rather error-prone. By only doing JSON parsing in the boot path, we can guarantee that the utilities necessary for a machine to boot are simpler and more reliable. We want to allow users to use YAML however, as it's much more human-friendly than JSON, hence the decision to have a tool separate from the boot path to \"transpile\" YAML configurations to machine-appropriate JSON ones.\n\n\n\n\nTell me more about Ignition\n\u00b6\n\n\nIgnition\n is the utility inside of a Container Linux image that is responsible for setting up a machine. It takes in a configuration, written in JSON, that instructs it to do things like add users, format disks, and install systemd units. The artifacts that ct produces are Ignition configs. All of this should be an implementation detail however, users are encouraged to write Container Linux Configs for ct, and to simply pass along the produced JSON file to their machines.\n\n\nHow similar are Container Linux Configs and Ignition configs?\n\u00b6\n\n\nSome features in Container Linux Configs and Ignition configs are identical.  Both support listing users for creation, systemd unit dropins for installation, and files for writing.\n\n\nAll of the differences stem from the fact that Ignition configs are distribution agnostic. An Ignition config can't just tell Ignition to enable etcd, because Ignition doesn't know what etcd is. The config must tell Ignition what systemd unit to enable, and provide a systemd dropin to configure etcd.\n\n\nct on the other hand \ndoes\n understand the specifics of Container Linux. A user can merely specify an etcd version and some etcd options, and ct knows that there's a unit called \netcd-member\n already on the system it can enable. It knows what options are supported by etcd, so it can sanity check them for the user. It can then generate an appropriate systemd dropin with the user's options, and provide Ignition the level of verbosity it needs, that would be tedious for a human to create.",
            "title": "Config transpiler overview"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#config-transpiler-overview",
            "text": "The  Config Transpiler , ct, is the utility responsible for transforming a user-provided Container Linux Configuration into an  Ignition  configuration. The resulting Ignition config can then be provided to a Container Linux machine when it first boots in order to provision it.  The Container Linux Config is intended to be human-friendly, and is thus in YAML. The syntax is rather forgiving, and things like references and multi-line strings are supported.  The resulting Ignition config is very much not intended to be human-friendly. It is an artifact produced by ct that users should simply pass along to their machines. JSON was chosen over a binary format to make the process more transparent and to allow power users to inspect/modify what ct produces, but it would have worked fine if the result from ct had not been human readable at all.",
            "title": "Config transpiler overview"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#why-a-two-step-process",
            "text": "There are a couple factors motivating the decision to not incorporate support for Container Linux Configs directly into the boot process of Container Linux (as in, the ability to provide a Container Linux Config directly to a booting machine, instead of an Ignition config).   By making users run their configs through ct before they attempt to boot a machine, issues with their configs can be caught before any machine attempts to boot. This will save users time, as they can much more quickly find problems with their configs. Were users to provide Container Linux Configs directly to machines at first boot, they would need to find a way to extract the Ignition logs from a machine that may have failed to boot, which can be a slow and tedious process.  YAML parsing is a complex process that in the past has been rather error-prone. By only doing JSON parsing in the boot path, we can guarantee that the utilities necessary for a machine to boot are simpler and more reliable. We want to allow users to use YAML however, as it's much more human-friendly than JSON, hence the decision to have a tool separate from the boot path to \"transpile\" YAML configurations to machine-appropriate JSON ones.",
            "title": "Why a two-step process?"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#tell-me-more-about-ignition",
            "text": "Ignition  is the utility inside of a Container Linux image that is responsible for setting up a machine. It takes in a configuration, written in JSON, that instructs it to do things like add users, format disks, and install systemd units. The artifacts that ct produces are Ignition configs. All of this should be an implementation detail however, users are encouraged to write Container Linux Configs for ct, and to simply pass along the produced JSON file to their machines.",
            "title": "Tell me more about Ignition"
        },
        {
            "location": "/container-linux-config-transpiler/doc/overview/#how-similar-are-container-linux-configs-and-ignition-configs",
            "text": "Some features in Container Linux Configs and Ignition configs are identical.  Both support listing users for creation, systemd unit dropins for installation, and files for writing.  All of the differences stem from the fact that Ignition configs are distribution agnostic. An Ignition config can't just tell Ignition to enable etcd, because Ignition doesn't know what etcd is. The config must tell Ignition what systemd unit to enable, and provide a systemd dropin to configure etcd.  ct on the other hand  does  understand the specifics of Container Linux. A user can merely specify an etcd version and some etcd options, and ct knows that there's a unit called  etcd-member  already on the system it can enable. It knows what options are supported by etcd, so it can sanity check them for the user. It can then generate an appropriate systemd dropin with the user's options, and provide Ignition the level of verbosity it needs, that would be tedious for a human to create.",
            "title": "How similar are Container Linux Configs and Ignition configs?"
        },
        {
            "location": "/container-linux-config-transpiler/doc/production-users/",
            "text": "Production users\n\u00b6\n\n\nThis document tracks people and use cases for CT in production. By creating a list of production use cases, we hope to build a community that we can reach out to in order to better understand requirements and desired features. The CT development team may reach out periodically to see how CT is working in the field and update this list.\n\n\nAdobe\n\u00b6\n\n\nEnvironment: AWS",
            "title": "Production users"
        },
        {
            "location": "/container-linux-config-transpiler/doc/production-users/#production-users",
            "text": "This document tracks people and use cases for CT in production. By creating a list of production use cases, we hope to build a community that we can reach out to in order to better understand requirements and desired features. The CT development team may reach out periodically to see how CT is working in the field and update this list.",
            "title": "Production users"
        },
        {
            "location": "/container-linux-config-transpiler/doc/production-users/#adobe",
            "text": "Environment: AWS",
            "title": "Adobe"
        },
        {
            "location": "/ignition/boot-process/",
            "text": "Flatcar Container Linux startup process\n\u00b6\n\n\nThe Flatcar Container Linux startup process is built on the standard \nLinux startup process\n. Since this process is already well documented and generally well understood, this document will focus on aspects specific to booting Flatcar Container Linux.\n\n\nBootloader\n\u00b6\n\n\nGRUB\n is the first program executed when a Flatcar Container Linux system boots. The Flatcar Container Linux \nGRUB config\n has several roles.\n\n\nFirst, the GRUB config \nspecifies which \nusr\n partition to use\n from the two \nusr\n partitions Flatcar Container Linux uses to provide atomic upgrades and rollbacks.\n\n\nSecond, GRUB \nchecks for a file called \nflatcar/first_boot\n in the EFI System Partition\n to determine if this is the first time a machine has booted. If that file is found, GRUB sets the \nflatcar.first_boot=detected\n Linux kernel command line parameter. This parameter is used in later stages of the boot process.\n\n\nFinally, GRUB \nsearches for the initial disk GUID\n (00000000-0000-0000-0000-000000000001) built into Flatcar Container Linux images. This GUID is randomized later in the boot process so that individual disks may be uniquely identified. If GRUB finds this GUID it sets another Linux kernel command line parameter, \nflatcar.randomize_guid=00000000-0000-0000-0000-000000000001\n.\n\n\nEarly user space\n\u00b6\n\n\nAfter GRUB, the Flatcar Container Linux startup process moves into the initial RAM file system. The initramfs mounts the root filesystem, randomizes the disk GUID, and runs Ignition.\n\n\nIf the \nflatcar.randomize_guid\n kernel parameter is provided, the disk with the specified GUID is given a new, random GUID.\n\n\nIf the \nflatcar.first_boot\n kernel parameter is provided and non-zero, Ignition and networkd are started. networkd will use DHCP to set up temporary IP addresses and routes so that Ignition can fetch its configuration from the network.\n\n\nIgnition\n\u00b6\n\n\nWhen Ignition runs on Flatcar Container Linux, it reads the Linux command line, looking for \nflatcar.oem.id\n. Ignition uses this identifier to determine where to read the user-provided configuration and which provider-specific configuration to combine with the user's. This provider-specific configuration performs basic machine setup, and may include enabling \ncoreos-metadata-sshkeys@.service\n (covered in more detail below).\n\n\nAfter Ignition runs successfully, if \nflatcar.first_boot\n was set to the special value \ndetected\n, Ignition mounts the EFI System Partition and deletes the \nflatcar/first_boot\n file.\n\n\nUser space\n\u00b6\n\n\nAfter all of the tasks in the initramfs complete, the machine pivots into user space. It is at this point that systemd begins starting units, including, if it was enabled, \ncoreos-metadata-sshkeys@core.service\n.\n\n\nSSH keys\n\u00b6\n\n\ncoreos-metadata-sshkeys@core.service\n is responsible for fetching SSH keys from the machine's environment. The keys are written to \n~core/.ssh/authorized_keys.d/coreos-metadata\n and \nupdate-ssh-keys\n is run to update \n~core/.ssh/authorized_keys\n. On cloud platforms, the keys are read from the provider's metadata service. This service is not supported on all platforms and is enabled by Ignition \nonly\n on those which are supported.",
            "title": "Flatcar Container Linux startup process"
        },
        {
            "location": "/ignition/boot-process/#flatcar-container-linux-startup-process",
            "text": "The Flatcar Container Linux startup process is built on the standard  Linux startup process . Since this process is already well documented and generally well understood, this document will focus on aspects specific to booting Flatcar Container Linux.",
            "title": "Flatcar Container Linux startup process"
        },
        {
            "location": "/ignition/boot-process/#bootloader",
            "text": "GRUB  is the first program executed when a Flatcar Container Linux system boots. The Flatcar Container Linux  GRUB config  has several roles.  First, the GRUB config  specifies which  usr  partition to use  from the two  usr  partitions Flatcar Container Linux uses to provide atomic upgrades and rollbacks.  Second, GRUB  checks for a file called  flatcar/first_boot  in the EFI System Partition  to determine if this is the first time a machine has booted. If that file is found, GRUB sets the  flatcar.first_boot=detected  Linux kernel command line parameter. This parameter is used in later stages of the boot process.  Finally, GRUB  searches for the initial disk GUID  (00000000-0000-0000-0000-000000000001) built into Flatcar Container Linux images. This GUID is randomized later in the boot process so that individual disks may be uniquely identified. If GRUB finds this GUID it sets another Linux kernel command line parameter,  flatcar.randomize_guid=00000000-0000-0000-0000-000000000001 .",
            "title": "Bootloader"
        },
        {
            "location": "/ignition/boot-process/#early-user-space",
            "text": "After GRUB, the Flatcar Container Linux startup process moves into the initial RAM file system. The initramfs mounts the root filesystem, randomizes the disk GUID, and runs Ignition.  If the  flatcar.randomize_guid  kernel parameter is provided, the disk with the specified GUID is given a new, random GUID.  If the  flatcar.first_boot  kernel parameter is provided and non-zero, Ignition and networkd are started. networkd will use DHCP to set up temporary IP addresses and routes so that Ignition can fetch its configuration from the network.",
            "title": "Early user space"
        },
        {
            "location": "/ignition/boot-process/#ignition",
            "text": "When Ignition runs on Flatcar Container Linux, it reads the Linux command line, looking for  flatcar.oem.id . Ignition uses this identifier to determine where to read the user-provided configuration and which provider-specific configuration to combine with the user's. This provider-specific configuration performs basic machine setup, and may include enabling  coreos-metadata-sshkeys@.service  (covered in more detail below).  After Ignition runs successfully, if  flatcar.first_boot  was set to the special value  detected , Ignition mounts the EFI System Partition and deletes the  flatcar/first_boot  file.",
            "title": "Ignition"
        },
        {
            "location": "/ignition/boot-process/#user-space",
            "text": "After all of the tasks in the initramfs complete, the machine pivots into user space. It is at this point that systemd begins starting units, including, if it was enabled,  coreos-metadata-sshkeys@core.service .",
            "title": "User space"
        },
        {
            "location": "/ignition/boot-process/#ssh-keys",
            "text": "coreos-metadata-sshkeys@core.service  is responsible for fetching SSH keys from the machine's environment. The keys are written to  ~core/.ssh/authorized_keys.d/coreos-metadata  and  update-ssh-keys  is run to update  ~core/.ssh/authorized_keys . On cloud platforms, the keys are read from the provider's metadata service. This service is not supported on all platforms and is enabled by Ignition  only  on those which are supported.",
            "title": "SSH keys"
        },
        {
            "location": "/ignition/metadata/",
            "text": "Metadata\n\u00b6\n\n\nIn many cases, it is desirable to inject dynamic data into services written by Ignition. Because Ignition itself is static and cannot inject dynamic data into configs, this must be done as the system starts. Flatcar Container Linux ships with a small utility, \ncoreos-metadata\n, which fetches information specific to the environment in which Flatcar Container Linux is running. While this utility works only on officially supported platforms, it is possible to use the same paradigm to write a custom utility.\n\n\nEach of these examples is written in version 2.0.0 of the config. Ensure that any configuration matches the version that Ignition expects.\n\n\netcd2 with coreos-metadata\n\u00b6\n\n\nThis config will write a systemd drop-in (shown below) for the etcd2.service. The drop-in modifies the ExecStart option, adding a few flags to etcd2's invocation. These flags use variables defined by coreos-metadata.service to change the interfaces on which etcd2 listens. coreos-metadata is provided by Flatcar Container Linux and will read the appropriate metadata for the cloud environment (AWS in this example) and write the results to \n/run/metadata/coreos\n. For more information on the supported platforms and environment variables, refer to the \ncoreos-metadata documentation\n.\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"etcd2.service\",\n      \"enable\": true,\n      \"dropins\": [{\n        \"name\": \"metadata.conf\",\n        \"contents\": \"[Unit]\\nRequires=coreos-metadata.service\\nAfter=coreos-metadata.service\\n\\n[Service]\\nEnvironmentFile=/run/metadata/coreos\\nExecStart=\\nExecStart=/usr/bin/etcd2 --advertise-client-urls=http://${COREOS_EC2_IPV4_PUBLIC}:2379 --initial-advertise-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 --listen-client-urls=http://0.0.0.0:2379 --listen-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 --initial-cluster=%m=http://${COREOS_EC2_IPV4_LOCAL}:2380\"\n      }]\n    }]\n  }\n}\n\n\n\nmetadata.conf\n\u00b6\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/bin/etcd2 \\\n  --advertise-client-urls=http://${COREOS_EC2_IPV4_PUBLIC}:2379 \\\n  --initial-advertise-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 \\\n  --listen-client-urls=http://0.0.0.0:2379 \\\n  --listen-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 \\\n  --initial-cluster=%m=http://${COREOS_EC2_IPV4_LOCAL}:2380\n\n\n\nCustom metadata agent\n\u00b6\n\n\nWhen Flatcar Container Linux is used outside of a supported cloud environment (for example, in a PXE booted, bare metal installation), coreos-metadata won't work. However, it is possible to write a custom metadata service.\n\n\nThis config will write a single service unit with the contents of a metadata agent service (shown below). This unit will not start on its own, because it is not enabled and is not a dependency of any other units. This metadata agent will fetch instance metadata from EC2 and save it to an ephemeral file.\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"metadata.service\",\n      \"contents\": \"[Unit]\\nDescription=EC2 metadata agent\\n\\n[Service]\\nType=oneshot\\nEnvironment=OUTPUT=/run/metadata/ec2\\nExecStart=/usr/bin/mkdir --parent /run/metadata\\nExecStart=/usr/bin/bash -c 'echo \\\"CUSTOM_EC2_IPV4_PUBLIC=$(curl --url http://169.254.169.254/2009-04-04/meta-data/public-ipv4 --retry 10)\\\\nCUSTOM_EC2_IPV4_LOCAL=$(curl --url http://169.254.169.254/2009-04-04/meta-data/local-ipv4 --retry 10)\\\" > ${OUTPUT}'\\n\"\n    }]\n  }\n}\n\n\n\nmetadata.service\n\u00b6\n\n\n[Unit]\nDescription=EC2 metadata agent\n\n[Service]\nType=oneshot\nEnvironment=OUTPUT=/run/metadata/ec2\nExecStart=/usr/bin/mkdir --parent /run/metadata\nExecStart=/usr/bin/bash -c 'echo \"CUSTOM_EC2_IPV4_PUBLIC=$(curl\\\n  --url http://169.254.169.254/2009-04-04/meta-data/public-ipv4\\\n  --retry 10)\\nCUSTOM_EC2_IPV4_LOCAL=$(curl\\\n  --url http://169.254.169.254/2009-04-04/meta-data/local-ipv4\\\n  --retry 10)\" > ${OUTPUT}'",
            "title": "Metadata"
        },
        {
            "location": "/ignition/metadata/#metadata",
            "text": "In many cases, it is desirable to inject dynamic data into services written by Ignition. Because Ignition itself is static and cannot inject dynamic data into configs, this must be done as the system starts. Flatcar Container Linux ships with a small utility,  coreos-metadata , which fetches information specific to the environment in which Flatcar Container Linux is running. While this utility works only on officially supported platforms, it is possible to use the same paradigm to write a custom utility.  Each of these examples is written in version 2.0.0 of the config. Ensure that any configuration matches the version that Ignition expects.",
            "title": "Metadata"
        },
        {
            "location": "/ignition/metadata/#etcd2-with-coreos-metadata",
            "text": "This config will write a systemd drop-in (shown below) for the etcd2.service. The drop-in modifies the ExecStart option, adding a few flags to etcd2's invocation. These flags use variables defined by coreos-metadata.service to change the interfaces on which etcd2 listens. coreos-metadata is provided by Flatcar Container Linux and will read the appropriate metadata for the cloud environment (AWS in this example) and write the results to  /run/metadata/coreos . For more information on the supported platforms and environment variables, refer to the  coreos-metadata documentation .  {\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"etcd2.service\",\n      \"enable\": true,\n      \"dropins\": [{\n        \"name\": \"metadata.conf\",\n        \"contents\": \"[Unit]\\nRequires=coreos-metadata.service\\nAfter=coreos-metadata.service\\n\\n[Service]\\nEnvironmentFile=/run/metadata/coreos\\nExecStart=\\nExecStart=/usr/bin/etcd2 --advertise-client-urls=http://${COREOS_EC2_IPV4_PUBLIC}:2379 --initial-advertise-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 --listen-client-urls=http://0.0.0.0:2379 --listen-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 --initial-cluster=%m=http://${COREOS_EC2_IPV4_LOCAL}:2380\"\n      }]\n    }]\n  }\n}",
            "title": "etcd2 with coreos-metadata"
        },
        {
            "location": "/ignition/metadata/#metadataconf",
            "text": "[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/bin/etcd2 \\\n  --advertise-client-urls=http://${COREOS_EC2_IPV4_PUBLIC}:2379 \\\n  --initial-advertise-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 \\\n  --listen-client-urls=http://0.0.0.0:2379 \\\n  --listen-peer-urls=http://${COREOS_EC2_IPV4_LOCAL}:2380 \\\n  --initial-cluster=%m=http://${COREOS_EC2_IPV4_LOCAL}:2380",
            "title": "metadata.conf"
        },
        {
            "location": "/ignition/metadata/#custom-metadata-agent",
            "text": "When Flatcar Container Linux is used outside of a supported cloud environment (for example, in a PXE booted, bare metal installation), coreos-metadata won't work. However, it is possible to write a custom metadata service.  This config will write a single service unit with the contents of a metadata agent service (shown below). This unit will not start on its own, because it is not enabled and is not a dependency of any other units. This metadata agent will fetch instance metadata from EC2 and save it to an ephemeral file.  {\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"metadata.service\",\n      \"contents\": \"[Unit]\\nDescription=EC2 metadata agent\\n\\n[Service]\\nType=oneshot\\nEnvironment=OUTPUT=/run/metadata/ec2\\nExecStart=/usr/bin/mkdir --parent /run/metadata\\nExecStart=/usr/bin/bash -c 'echo \\\"CUSTOM_EC2_IPV4_PUBLIC=$(curl --url http://169.254.169.254/2009-04-04/meta-data/public-ipv4 --retry 10)\\\\nCUSTOM_EC2_IPV4_LOCAL=$(curl --url http://169.254.169.254/2009-04-04/meta-data/local-ipv4 --retry 10)\\\" > ${OUTPUT}'\\n\"\n    }]\n  }\n}",
            "title": "Custom metadata agent"
        },
        {
            "location": "/ignition/metadata/#metadataservice",
            "text": "[Unit]\nDescription=EC2 metadata agent\n\n[Service]\nType=oneshot\nEnvironment=OUTPUT=/run/metadata/ec2\nExecStart=/usr/bin/mkdir --parent /run/metadata\nExecStart=/usr/bin/bash -c 'echo \"CUSTOM_EC2_IPV4_PUBLIC=$(curl\\\n  --url http://169.254.169.254/2009-04-04/meta-data/public-ipv4\\\n  --retry 10)\\nCUSTOM_EC2_IPV4_LOCAL=$(curl\\\n  --url http://169.254.169.254/2009-04-04/meta-data/local-ipv4\\\n  --retry 10)\" > ${OUTPUT}'",
            "title": "metadata.service"
        },
        {
            "location": "/ignition/network-configuration/",
            "text": "Network configuration\n\u00b6\n\n\nConfiguring networkd with Ignition is a very straightforward task. Because Ignition runs before networkd starts, configuration is just a matter of writing the desired config to disk. The Ignition config has a specific section dedicated to this.\n\n\nEach of these examples is written in version 2.0.0 of the config. Ensure that any configuration matches the version that Ignition expects.\n\n\nStatic networking\n\u00b6\n\n\nIn this example, the network interface with the name \"eth0\" will be given the IP address 10.0.1.7. A typical interface will need more configuration and may use all of the options of a \nnetwork unit\n.\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"networkd\": {\n    \"units\": [{\n      \"name\": \"00-eth0.network\",\n      \"contents\": \"[Match]\\nName=eth0\\n\\n[Network]\\nAddress=10.0.1.7\"\n    }]\n  }\n}\n\n\n\nThis configuration will instruct Ignition to create a single network unit named \"00-eth0.network\" with the contents:\n\n\n[Match]\nName=eth0\n\n[Network]\nAddress=10.0.1.7\n\n\n\nWhen the system boots, networkd will read this config and assign the IP address to eth0.\n\n\nUsing static IP addresses with Ignition\n\u00b6\n\n\nBecause Ignition writes network configuration to disk for networkd to use later, statically-configured interfaces will be brought online only after Ignition has run. If static IP configuration is required to download remote configs before Ignition has run, use one of the following two forms of supported kernel command-line arguments.\n\n\nThis format can configure a static IP address on the named interface, or on all interfaces when unspecified.\n\n\n\n\nip=\n to specify the IP address, for example \nip=10.0.2.42\n\n\nnetmask=\n to specify the netmask, for example \nnetmask=255.255.255.0\n\n\ngateway=\n to specify the gateway address, for example \ngateway=10.0.2.2\n\n\nksdevice=\n (optional) to limit configuration to the named interface, for example \nksdevice=eth0\n\n\n\n\nThis format can be specified multiple times to apply unique static configuration to different interfaces. Omitting the \n<iface>\n parameter will apply the configuration to all interfaces that have not yet been configured.\n\n\n\n\nip=<ip>::<gateway>:<netmask>:<hostname>:<iface>:none[:<dns1>[:<dns2>]]\n, for example \nip=10.0.2.42::10.0.2.2:255.255.255.0::eth0:none:8.8.8.8:8.8.4.4\n\n\n\n\nBonded NICs\n\u00b6\n\n\nIn this example, all of the network interfaces whose names begin with \"eth\" will be bonded together to form \"bond0\". This new interface will then be configured to use DHCP.\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"networkd\": {\n    \"units\": [\n      {\n        \"name\": \"00-eth.network\",\n        \"contents\": \"[Match]\\nName=eth*\\n\\n[Network]\\nBond=bond0\"\n      },\n      {\n        \"name\": \"10-bond0.netdev\",\n        \"contents\": \"[NetDev]\\nName=bond0\\nKind=bond\"\n      },\n      {\n        \"name\": \"20-bond0.network\",\n        \"contents\": \"[Match]\\nName=bond0\\n\\n[Network]\\nDHCP=true\"\n      }\n    ]\n  }\n}",
            "title": "Network configuration"
        },
        {
            "location": "/ignition/network-configuration/#network-configuration",
            "text": "Configuring networkd with Ignition is a very straightforward task. Because Ignition runs before networkd starts, configuration is just a matter of writing the desired config to disk. The Ignition config has a specific section dedicated to this.  Each of these examples is written in version 2.0.0 of the config. Ensure that any configuration matches the version that Ignition expects.",
            "title": "Network configuration"
        },
        {
            "location": "/ignition/network-configuration/#static-networking",
            "text": "In this example, the network interface with the name \"eth0\" will be given the IP address 10.0.1.7. A typical interface will need more configuration and may use all of the options of a  network unit .  {\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"networkd\": {\n    \"units\": [{\n      \"name\": \"00-eth0.network\",\n      \"contents\": \"[Match]\\nName=eth0\\n\\n[Network]\\nAddress=10.0.1.7\"\n    }]\n  }\n}  This configuration will instruct Ignition to create a single network unit named \"00-eth0.network\" with the contents:  [Match]\nName=eth0\n\n[Network]\nAddress=10.0.1.7  When the system boots, networkd will read this config and assign the IP address to eth0.",
            "title": "Static networking"
        },
        {
            "location": "/ignition/network-configuration/#using-static-ip-addresses-with-ignition",
            "text": "Because Ignition writes network configuration to disk for networkd to use later, statically-configured interfaces will be brought online only after Ignition has run. If static IP configuration is required to download remote configs before Ignition has run, use one of the following two forms of supported kernel command-line arguments.  This format can configure a static IP address on the named interface, or on all interfaces when unspecified.   ip=  to specify the IP address, for example  ip=10.0.2.42  netmask=  to specify the netmask, for example  netmask=255.255.255.0  gateway=  to specify the gateway address, for example  gateway=10.0.2.2  ksdevice=  (optional) to limit configuration to the named interface, for example  ksdevice=eth0   This format can be specified multiple times to apply unique static configuration to different interfaces. Omitting the  <iface>  parameter will apply the configuration to all interfaces that have not yet been configured.   ip=<ip>::<gateway>:<netmask>:<hostname>:<iface>:none[:<dns1>[:<dns2>]] , for example  ip=10.0.2.42::10.0.2.2:255.255.255.0::eth0:none:8.8.8.8:8.8.4.4",
            "title": "Using static IP addresses with Ignition"
        },
        {
            "location": "/ignition/network-configuration/#bonded-nics",
            "text": "In this example, all of the network interfaces whose names begin with \"eth\" will be bonded together to form \"bond0\". This new interface will then be configured to use DHCP.  {\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"networkd\": {\n    \"units\": [\n      {\n        \"name\": \"00-eth.network\",\n        \"contents\": \"[Match]\\nName=eth*\\n\\n[Network]\\nBond=bond0\"\n      },\n      {\n        \"name\": \"10-bond0.netdev\",\n        \"contents\": \"[NetDev]\\nName=bond0\\nKind=bond\"\n      },\n      {\n        \"name\": \"20-bond0.network\",\n        \"contents\": \"[Match]\\nName=bond0\\n\\n[Network]\\nDHCP=true\"\n      }\n    ]\n  }\n}",
            "title": "Bonded NICs"
        },
        {
            "location": "/ignition/what-is-ignition/",
            "text": "What is Ignition?\n\u00b6\n\n\nIgnition is a new provisioning utility designed specifically for Flatcar Container Linux, which allows you to manipulate disks during early boot. This includes partitioning disks, formatting partitions, writing files (regular files, systemd units, networkd units, and more), and configuring users. On the first boot, Ignition reads its configuration from a source-of-truth (remote URL, network metadata service, or hypervisor bridge, for example) and applies the configuration.\n\n\nA \nseries of example configs\n are provided for reference.\n\n\nIgnition vs coreos-cloudinit\n\u00b6\n\n\nIgnition solves many of the same problems as \ncoreos-cloudinit\n but in a simpler, more predictable, and more flexible manner. This is achieved with two major changes: Ignition only runs once and it does not handle variable substitution. Ignition has also fixed a number of pain points with regard to configuration.\n\n\nInstead of YAML, Ignition uses JSON for its configuration format. JSON's typing immediately eliminates problems like \"off\" being rewritten as \"false\", the \"#cloud-config\" header being stripped because comments \nshouldn't\n have meaning, and confusion around whether those file permissions were written in octal or decimal. Ignition's configuration is also versioned, which allows future development without persistent backward compatibility.\n\n\nIgnition only runs once\n\u00b6\n\n\nEven though Ignition only runs once, during the first boot of the system, it packs a powerful punch. Because Ignition runs so early in the boot process (in the initramfs, to be exact), it is able to repartition disks, format filesystems, create users, and write files, all before the userspace begins to boot.\n\n\nBecause Ignition runs so early in the boot process, the network config is available for networkd to read when it first starts, and systemd services are already written to disk when systemd starts. \nConfiguring the network\n is no longer an issue. This results in a simple startup, a faster startup, and the ability to accurately inspect the unit dependency graphs.\n\n\nNo variable substitution\n\u00b6\n\n\nBecause Ignition only runs once, there's no reason for it to incorporate dynamic data (like  floating IP addresses, or compute regions).\n\n\nInstead, use Ignition to write static files and leverage systemd's environment variable expansion to insert dynamic data. The Ignition config should install a service which fetches the necessary runtime data, then any services which need this data (such as etcd or fleet) can rely on the installed service and source in their output. The result is that the data is only collected if and when it is needed. For supported platforms, Flatcar Container Linux provides a small utility (\ncoreos-metadata.service\n) to help fetch this data.\n\n\nThe lack of variable substitution in Ignition has an added benefit of leveling the playing field when it comes to compute providers. The user's experience is no longer crippled because the metadata for their platform isn't supported. It is possible to write a \ncustom metadata agent\n to fetch the necessary data.\n\n\nWhen is Ignition executed\n\u00b6\n\n\nOn boot, GRUB checks the EFI System Partition for a file at \nflatcar/first_boot\n and sets \nflatcar.first_boot=detected\n if found. The \nflatcar.first_boot\n parameter is processed by a \nsystemd-generator\n in the \ninitramfs\n and if the parameter value is non-zero, the Ignition units are set as dependencies of \ninitrd.target\n, causing Ignition to run. If the parameter is set to the special value \ndetected\n, the \nflatcar/first_boot\n file is deleted after Ignition runs successfully.\n\n\nNote that \nPXE\n deployments don't use GRUB to boot, so \nflatcar.first_boot=1\n must be added to the boot arguments in order for Ignition to run. \ndetected\n should not be specified so Ignition will not attempt to delete \nflatcar/first_boot\n.\n\n\nProviding Ignition a config\n\u00b6\n\n\nIgnition can read its config from a number of different locations, but only from one at a time. When running Flatcar Container Linux on the supported cloud providers, Ignition will read its config from the instance's userdata. This means that if Ignition is being used, it will not be possible to use other tools which also use this userdata (such as coreos-cloudinit). Bare metal installations and PXE boots can use the kernel boot parameters to point Ignition at the config.\n\n\nWhere is Ignition supported?\n\u00b6\n\n\nThe \nfull list of supported platforms\n is provided and will be kept up-to-date as development progresses.\n\n\nIgnition is under active development. Expect to see support for more images in the coming months.",
            "title": "What is Ignition?"
        },
        {
            "location": "/ignition/what-is-ignition/#what-is-ignition",
            "text": "Ignition is a new provisioning utility designed specifically for Flatcar Container Linux, which allows you to manipulate disks during early boot. This includes partitioning disks, formatting partitions, writing files (regular files, systemd units, networkd units, and more), and configuring users. On the first boot, Ignition reads its configuration from a source-of-truth (remote URL, network metadata service, or hypervisor bridge, for example) and applies the configuration.  A  series of example configs  are provided for reference.",
            "title": "What is Ignition?"
        },
        {
            "location": "/ignition/what-is-ignition/#ignition-vs-coreos-cloudinit",
            "text": "Ignition solves many of the same problems as  coreos-cloudinit  but in a simpler, more predictable, and more flexible manner. This is achieved with two major changes: Ignition only runs once and it does not handle variable substitution. Ignition has also fixed a number of pain points with regard to configuration.  Instead of YAML, Ignition uses JSON for its configuration format. JSON's typing immediately eliminates problems like \"off\" being rewritten as \"false\", the \"#cloud-config\" header being stripped because comments  shouldn't  have meaning, and confusion around whether those file permissions were written in octal or decimal. Ignition's configuration is also versioned, which allows future development without persistent backward compatibility.",
            "title": "Ignition vs coreos-cloudinit"
        },
        {
            "location": "/ignition/what-is-ignition/#ignition-only-runs-once",
            "text": "Even though Ignition only runs once, during the first boot of the system, it packs a powerful punch. Because Ignition runs so early in the boot process (in the initramfs, to be exact), it is able to repartition disks, format filesystems, create users, and write files, all before the userspace begins to boot.  Because Ignition runs so early in the boot process, the network config is available for networkd to read when it first starts, and systemd services are already written to disk when systemd starts.  Configuring the network  is no longer an issue. This results in a simple startup, a faster startup, and the ability to accurately inspect the unit dependency graphs.",
            "title": "Ignition only runs once"
        },
        {
            "location": "/ignition/what-is-ignition/#no-variable-substitution",
            "text": "Because Ignition only runs once, there's no reason for it to incorporate dynamic data (like  floating IP addresses, or compute regions).  Instead, use Ignition to write static files and leverage systemd's environment variable expansion to insert dynamic data. The Ignition config should install a service which fetches the necessary runtime data, then any services which need this data (such as etcd or fleet) can rely on the installed service and source in their output. The result is that the data is only collected if and when it is needed. For supported platforms, Flatcar Container Linux provides a small utility ( coreos-metadata.service ) to help fetch this data.  The lack of variable substitution in Ignition has an added benefit of leveling the playing field when it comes to compute providers. The user's experience is no longer crippled because the metadata for their platform isn't supported. It is possible to write a  custom metadata agent  to fetch the necessary data.",
            "title": "No variable substitution"
        },
        {
            "location": "/ignition/what-is-ignition/#when-is-ignition-executed",
            "text": "On boot, GRUB checks the EFI System Partition for a file at  flatcar/first_boot  and sets  flatcar.first_boot=detected  if found. The  flatcar.first_boot  parameter is processed by a  systemd-generator  in the  initramfs  and if the parameter value is non-zero, the Ignition units are set as dependencies of  initrd.target , causing Ignition to run. If the parameter is set to the special value  detected , the  flatcar/first_boot  file is deleted after Ignition runs successfully.  Note that  PXE  deployments don't use GRUB to boot, so  flatcar.first_boot=1  must be added to the boot arguments in order for Ignition to run.  detected  should not be specified so Ignition will not attempt to delete  flatcar/first_boot .",
            "title": "When is Ignition executed"
        },
        {
            "location": "/ignition/what-is-ignition/#providing-ignition-a-config",
            "text": "Ignition can read its config from a number of different locations, but only from one at a time. When running Flatcar Container Linux on the supported cloud providers, Ignition will read its config from the instance's userdata. This means that if Ignition is being used, it will not be possible to use other tools which also use this userdata (such as coreos-cloudinit). Bare metal installations and PXE boots can use the kernel boot parameters to point Ignition at the config.",
            "title": "Providing Ignition a config"
        },
        {
            "location": "/ignition/what-is-ignition/#where-is-ignition-supported",
            "text": "The  full list of supported platforms  is provided and will be kept up-to-date as development progresses.  Ignition is under active development. Expect to see support for more images in the coming months.",
            "title": "Where is Ignition supported?"
        },
        {
            "location": "/os/adding-certificate-authorities/",
            "text": "Custom certificate authorities\n\u00b6\n\n\nFlatcar Container Linux supports custom Certificate Authorities (CAs) in addition to the default list of trusted CAs. Adding your own CA allows you to:\n\n\n\n\nUse a corporate wildcard certificate\n\n\nUse your own CA to communicate with an installation of CoreUpdate\n\n\n\n\nThe setup process for any of these use-cases is the same:\n\n\n\n\n\n\nCopy the PEM-encoded certificate authority file (usually with a \n.pem\n file name extension) to \n/etc/ssl/certs\n\n\n\n\n\n\nRun the \nupdate-ca-certificates\n script to update the system bundle of Certificate Authorities. All programs running on the system will now trust the added CA.\n\n\n\n\n\n\nMore information\n\u00b6\n\n\nGenerate Self-Signed Certificates\n\n\netcd Security Model",
            "title": "Custom certificate authorities"
        },
        {
            "location": "/os/adding-certificate-authorities/#custom-certificate-authorities",
            "text": "Flatcar Container Linux supports custom Certificate Authorities (CAs) in addition to the default list of trusted CAs. Adding your own CA allows you to:   Use a corporate wildcard certificate  Use your own CA to communicate with an installation of CoreUpdate   The setup process for any of these use-cases is the same:    Copy the PEM-encoded certificate authority file (usually with a  .pem  file name extension) to  /etc/ssl/certs    Run the  update-ca-certificates  script to update the system bundle of Certificate Authorities. All programs running on the system will now trust the added CA.",
            "title": "Custom certificate authorities"
        },
        {
            "location": "/os/adding-certificate-authorities/#more-information",
            "text": "Generate Self-Signed Certificates  etcd Security Model",
            "title": "More information"
        },
        {
            "location": "/os/adding-disk-space/",
            "text": "Adding disk space to your Flatcar Container Linux machine\n\u00b6\n\n\nOn a Flatcar Container Linux machine, the operating system itself is mounted as a read-only partition at \n/usr\n. The root partition provides read-write storage by default and on a fresh install is mostly blank. The default size of this partition depends on the platform but it is usually between 3GB and 16GB. If more space is required simply extend the virtual machine's disk image and Flatcar Container Linux will fix the partition table and resize the root partition to fill the disk on the next boot.\n\n\nAmazon EC2\n\u00b6\n\n\nAmazon doesn't support directly resizing volumes, you must take a snapshot and create a new volume based on that snapshot. Refer to the AWS EC2 documentation on \nexpanding EBS volumes\n for detailed instructions.\n\n\nQEMU (qemu-img)\n\u00b6\n\n\nEven if you are not using Qemu itself the qemu-img tool is the easiest to use. It will work on raw, qcow2, vmdk, and most other formats. The command accepts either an absolute size or a relative size by by adding \n+\n prefix. Unit suffixes such as \nG\n or \nM\n are also supported.\n\n\n# Increase the disk size by 5GB\nqemu-img resize flatcar_production_qemu_image.img +5G\n\n\n\nVMware\n\u00b6\n\n\nThe interface available for resizing disks in VMware varies depending on the product. See this \nKnowledge Base article\n for details. Most products include a tool called \nvmware-vdiskmanager\n. The size must be the absolute disk size, relative sizes are not supported so be careful to only increase the size, not shrink it. The unit suffixes \nGb\n and \nMb\n are supported.\n\n\n# Set the disk size to 20GB\nvmware-vdiskmanager -x 20Gb flatcar_developer_vmware_insecure.vmx\n\n\n\nVirtualBox\n\u00b6\n\n\nUse qemu-img or vmware-vdiskmanager as described above. VirtualBox does not support resizing VMDK disk images, only VDI and VHD disks. Meanwhile VirtualBox only supports using VMDK disk images with the OVF config file format used for importing/exporting virtual machines.\n\n\nIf you have have no other options you can try converting the VMDK disk image to a VDI image and configuring a new virtual machine with it:\n\n\nVBoxManage clonehd old.vmdk new.vdi --format VDI\nVBoxManage modifyhd new.vdi --resize 20480",
            "title": "Adding disk space to your Flatcar Container Linux machine"
        },
        {
            "location": "/os/adding-disk-space/#adding-disk-space-to-your-flatcar-container-linux-machine",
            "text": "On a Flatcar Container Linux machine, the operating system itself is mounted as a read-only partition at  /usr . The root partition provides read-write storage by default and on a fresh install is mostly blank. The default size of this partition depends on the platform but it is usually between 3GB and 16GB. If more space is required simply extend the virtual machine's disk image and Flatcar Container Linux will fix the partition table and resize the root partition to fill the disk on the next boot.",
            "title": "Adding disk space to your Flatcar Container Linux machine"
        },
        {
            "location": "/os/adding-disk-space/#amazon-ec2",
            "text": "Amazon doesn't support directly resizing volumes, you must take a snapshot and create a new volume based on that snapshot. Refer to the AWS EC2 documentation on  expanding EBS volumes  for detailed instructions.",
            "title": "Amazon EC2"
        },
        {
            "location": "/os/adding-disk-space/#qemu-qemu-img",
            "text": "Even if you are not using Qemu itself the qemu-img tool is the easiest to use. It will work on raw, qcow2, vmdk, and most other formats. The command accepts either an absolute size or a relative size by by adding  +  prefix. Unit suffixes such as  G  or  M  are also supported.  # Increase the disk size by 5GB\nqemu-img resize flatcar_production_qemu_image.img +5G",
            "title": "QEMU (qemu-img)"
        },
        {
            "location": "/os/adding-disk-space/#vmware",
            "text": "The interface available for resizing disks in VMware varies depending on the product. See this  Knowledge Base article  for details. Most products include a tool called  vmware-vdiskmanager . The size must be the absolute disk size, relative sizes are not supported so be careful to only increase the size, not shrink it. The unit suffixes  Gb  and  Mb  are supported.  # Set the disk size to 20GB\nvmware-vdiskmanager -x 20Gb flatcar_developer_vmware_insecure.vmx",
            "title": "VMware"
        },
        {
            "location": "/os/adding-disk-space/#virtualbox",
            "text": "Use qemu-img or vmware-vdiskmanager as described above. VirtualBox does not support resizing VMDK disk images, only VDI and VHD disks. Meanwhile VirtualBox only supports using VMDK disk images with the OVF config file format used for importing/exporting virtual machines.  If you have have no other options you can try converting the VMDK disk image to a VDI image and configuring a new virtual machine with it:  VBoxManage clonehd old.vmdk new.vdi --format VDI\nVBoxManage modifyhd new.vdi --resize 20480",
            "title": "VirtualBox"
        },
        {
            "location": "/os/adding-swap/",
            "text": "Adding swap in Flatcar Container Linux\n\u00b6\n\n\nSwap is the process of moving pages of memory to a designated part of the hard disk, freeing up space when needed. Swap can be used to alleviate problems with low-memory environments.\n\n\nBy default Flatcar Container Linux does not include a partition for swap, however one can configure their system to have swap, either by including a dedicated partition for it or creating a swapfile.\n\n\nManaging swap with systemd\n\u00b6\n\n\nsystemd provides a specialized \n.swap\n unit file type which may be used to activate swap. The below example shows how to add a swapfile and activate it using systemd.\n\n\nCreating a swapfile\n\u00b6\n\n\nThe following commands, run as root, will make a 1GiB file suitable for use as swap.\n\n\nmkdir -p /var/vm\nfallocate -l 1024m /var/vm/swapfile1\nchmod 600 /var/vm/swapfile1\nmkswap /var/vm/swapfile1\n\n\n\nCreating the systemd unit file\n\u00b6\n\n\nThe following systemd unit activates the swapfile we created. It should be written to \n/etc/systemd/system/var-vm-swapfile1.swap\n.\n\n\n[Unit]\nDescription=Turn on swap\n\n[Swap]\nWhat=/var/vm/swapfile1\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nEnable the unit and start using swap\n\u00b6\n\n\nUse \nsystemctl\n to enable the unit once created. The \nswappiness\n value may be modified if desired.\n\n\n$ systemctl enable --now var-vm-swapfile1.swap\n# Optionally\n$ echo 'vm.swappiness=10' | sudo tee /etc/sysctl.d/80-swappiness.conf\n$ systemctl restart systemd-sysctl\n\n\n\nSwap has been enabled and will be started automatically on subsequent reboots. We can verify that the swap is activated by running \nswapon\n:\n\n\n$ swapon\nNAME              TYPE       SIZE USED PRIO\n/var/vm/swapfile1 file      1024M   0B   -1\n\n\n\nProblems and Considerations\n\u00b6\n\n\nBtrfs and xfs\n\u00b6\n\n\nSwapfiles should not be created on btrfs or xfs volumes. For systems using btrfs or xfs, it is recommended to create a dedicated swap partition.\n\n\nPartition size\n\u00b6\n\n\nThe swapfile cannot be larger than the partition on which it is stored.\n\n\nChecking if a system can use a swapfile\n\u00b6\n\n\nUse the \ndf(1)\n command to verify that a partition has the right format and enough available space:\n\n\n$ df -Th\nFilesystem     Type      Size  Used Avail Use% Mounted on\n[...]\n/dev/sdXN      ext4      2.0G  3.0M  1.8G   1% /var\n\n\n\nThe block device mounted at \n/var/\n, \n/dev/sdXN\n, is the correct filesystem type and has enough space for a 1GiB swapfile.\n\n\nAdding swap with a Container Linux Config\n\u00b6\n\n\nThe following config sets up a 1GiB swapfile located at \n/var/vm/swapfile1\n.\n\n\nstorage:\n  files:\n  - path: /etc/sysctl.d/80-swappiness.conf\n    filesystem: root\n    contents:\n      inline: \"vm.swappiness=10\"\n\nsystemd:\n  units:\n    - name: var-vm-swapfile1.swap\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Turn on swap\n        Requires=create-swapfile.service\n        After=create-swapfile.service\n\n        [Swap]\n        What=/var/vm/swapfile1\n\n        [Install]\n        WantedBy=multi-user.target\n    - name: create-swapfile.service\n      contents: |\n        [Unit]\n        Description=Create a swapfile\n        RequiresMountsFor=/var\n        ConditionPathExists=!/var/vm/swapfile1\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/mkdir -p /var/vm\n        ExecStart=/usr/bin/fallocate -l 1024m /var/vm/swapfile1\n        ExecStart=/usr/bin/chmod 600 /var/vm/swapfile1\n        ExecStart=/usr/sbin/mkswap /var/vm/swapfile1\n        RemainAfterExit=true",
            "title": "Adding swap in Flatcar Container Linux"
        },
        {
            "location": "/os/adding-swap/#adding-swap-in-flatcar-container-linux",
            "text": "Swap is the process of moving pages of memory to a designated part of the hard disk, freeing up space when needed. Swap can be used to alleviate problems with low-memory environments.  By default Flatcar Container Linux does not include a partition for swap, however one can configure their system to have swap, either by including a dedicated partition for it or creating a swapfile.",
            "title": "Adding swap in Flatcar Container Linux"
        },
        {
            "location": "/os/adding-swap/#managing-swap-with-systemd",
            "text": "systemd provides a specialized  .swap  unit file type which may be used to activate swap. The below example shows how to add a swapfile and activate it using systemd.",
            "title": "Managing swap with systemd"
        },
        {
            "location": "/os/adding-swap/#creating-a-swapfile",
            "text": "The following commands, run as root, will make a 1GiB file suitable for use as swap.  mkdir -p /var/vm\nfallocate -l 1024m /var/vm/swapfile1\nchmod 600 /var/vm/swapfile1\nmkswap /var/vm/swapfile1",
            "title": "Creating a swapfile"
        },
        {
            "location": "/os/adding-swap/#creating-the-systemd-unit-file",
            "text": "The following systemd unit activates the swapfile we created. It should be written to  /etc/systemd/system/var-vm-swapfile1.swap .  [Unit]\nDescription=Turn on swap\n\n[Swap]\nWhat=/var/vm/swapfile1\n\n[Install]\nWantedBy=multi-user.target",
            "title": "Creating the systemd unit file"
        },
        {
            "location": "/os/adding-swap/#enable-the-unit-and-start-using-swap",
            "text": "Use  systemctl  to enable the unit once created. The  swappiness  value may be modified if desired.  $ systemctl enable --now var-vm-swapfile1.swap\n# Optionally\n$ echo 'vm.swappiness=10' | sudo tee /etc/sysctl.d/80-swappiness.conf\n$ systemctl restart systemd-sysctl  Swap has been enabled and will be started automatically on subsequent reboots. We can verify that the swap is activated by running  swapon :  $ swapon\nNAME              TYPE       SIZE USED PRIO\n/var/vm/swapfile1 file      1024M   0B   -1",
            "title": "Enable the unit and start using swap"
        },
        {
            "location": "/os/adding-swap/#problems-and-considerations",
            "text": "",
            "title": "Problems and Considerations"
        },
        {
            "location": "/os/adding-swap/#btrfs-and-xfs",
            "text": "Swapfiles should not be created on btrfs or xfs volumes. For systems using btrfs or xfs, it is recommended to create a dedicated swap partition.",
            "title": "Btrfs and xfs"
        },
        {
            "location": "/os/adding-swap/#partition-size",
            "text": "The swapfile cannot be larger than the partition on which it is stored.",
            "title": "Partition size"
        },
        {
            "location": "/os/adding-swap/#checking-if-a-system-can-use-a-swapfile",
            "text": "Use the  df(1)  command to verify that a partition has the right format and enough available space:  $ df -Th\nFilesystem     Type      Size  Used Avail Use% Mounted on\n[...]\n/dev/sdXN      ext4      2.0G  3.0M  1.8G   1% /var  The block device mounted at  /var/ ,  /dev/sdXN , is the correct filesystem type and has enough space for a 1GiB swapfile.",
            "title": "Checking if a system can use a swapfile"
        },
        {
            "location": "/os/adding-swap/#adding-swap-with-a-container-linux-config",
            "text": "The following config sets up a 1GiB swapfile located at  /var/vm/swapfile1 .  storage:\n  files:\n  - path: /etc/sysctl.d/80-swappiness.conf\n    filesystem: root\n    contents:\n      inline: \"vm.swappiness=10\"\n\nsystemd:\n  units:\n    - name: var-vm-swapfile1.swap\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Turn on swap\n        Requires=create-swapfile.service\n        After=create-swapfile.service\n\n        [Swap]\n        What=/var/vm/swapfile1\n\n        [Install]\n        WantedBy=multi-user.target\n    - name: create-swapfile.service\n      contents: |\n        [Unit]\n        Description=Create a swapfile\n        RequiresMountsFor=/var\n        ConditionPathExists=!/var/vm/swapfile1\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/mkdir -p /var/vm\n        ExecStart=/usr/bin/fallocate -l 1024m /var/vm/swapfile1\n        ExecStart=/usr/bin/chmod 600 /var/vm/swapfile1\n        ExecStart=/usr/sbin/mkswap /var/vm/swapfile1\n        RemainAfterExit=true",
            "title": "Adding swap with a Container Linux Config"
        },
        {
            "location": "/os/adding-users/",
            "text": "Adding users\n\u00b6\n\n\nYou can create user accounts on a Flatcar Container Linux machine manually with \nuseradd\n or via a Container Linux Config when the machine is created.\n\n\nAdd Users via Container Linux Configs\n\u00b6\n\n\nIn your Container Linux Config, you can specify many \ndifferent parameters\n for each user. Here's an example:\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n    - name: elroy\n      password_hash: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n      groups: [ sudo, docker ]\n\n\n\nAdd user manually\n\u00b6\n\n\nIf you'd like to add a user manually, SSH to the machine and use the \nuseradd\n tool. To create the user \nuser\n, run:\n\n\nsudo useradd -p \"*\" -U -m user1 -G sudo\n\n\n\nThe \n\"*\"\n creates a user that cannot login with a password but can log in via SSH key. \n-U\n creates a group for the user, \n-G\n adds the user to the existing \nsudo\n group and \n-m\n creates a home directory. If you'd like to add a password for the user, run:\n\n\n$ sudo passwd user1\nNew password:\nRe-enter new password:\npasswd: password changed.\n\n\n\nTo assign an SSH key, run:\n\n\nupdate-ssh-keys -u user1 -a user1 user1.pem\n\n\n\nGrant sudo Access\n\u00b6\n\n\nIf you trust the user, you can grant administrative privileges using \nvisudo\n.\u00a0\nvisudo\n checks the file syntax before actually overwriting the \nsudoers\n file. This command should be run as root to avoid losing sudo access in the event of a failure. Instead of editing \n/etc/sudo.conf\n directly you will create a new file under the \n/etc/sudoers.d/\n directory. When you run visudo, it is required that you specify which file you are attempting to edit with the \n-f\n argument:\u00a0\n\n\n# visudo -f /etc/sudoers.d/user1\n\n\n\nAdd a the line:\n\n\nuser1 ALL=(ALL) NOPASSWD: ALL\n\n\n\nCheck that sudo has been granted:\n\n\n# su user1\n$ cat /etc/sudoers.d/user1\ncat: /etc/sudoers.d/user1: Permission denied\n\n$ sudo cat /etc/sudoers.d/user1\nuser1 ALL=(ALL) NOPASSWD: ALL",
            "title": "Adding users"
        },
        {
            "location": "/os/adding-users/#adding-users",
            "text": "You can create user accounts on a Flatcar Container Linux machine manually with  useradd  or via a Container Linux Config when the machine is created.",
            "title": "Adding users"
        },
        {
            "location": "/os/adding-users/#add-users-via-container-linux-configs",
            "text": "In your Container Linux Config, you can specify many  different parameters  for each user. Here's an example:  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n    - name: elroy\n      password_hash: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\"\n      groups: [ sudo, docker ]",
            "title": "Add Users via Container Linux Configs"
        },
        {
            "location": "/os/adding-users/#add-user-manually",
            "text": "If you'd like to add a user manually, SSH to the machine and use the  useradd  tool. To create the user  user , run:  sudo useradd -p \"*\" -U -m user1 -G sudo  The  \"*\"  creates a user that cannot login with a password but can log in via SSH key.  -U  creates a group for the user,  -G  adds the user to the existing  sudo  group and  -m  creates a home directory. If you'd like to add a password for the user, run:  $ sudo passwd user1\nNew password:\nRe-enter new password:\npasswd: password changed.  To assign an SSH key, run:  update-ssh-keys -u user1 -a user1 user1.pem",
            "title": "Add user manually"
        },
        {
            "location": "/os/adding-users/#grant-sudo-access",
            "text": "If you trust the user, you can grant administrative privileges using  visudo .\u00a0 visudo  checks the file syntax before actually overwriting the  sudoers  file. This command should be run as root to avoid losing sudo access in the event of a failure. Instead of editing  /etc/sudo.conf  directly you will create a new file under the  /etc/sudoers.d/  directory. When you run visudo, it is required that you specify which file you are attempting to edit with the  -f  argument:\u00a0  # visudo -f /etc/sudoers.d/user1  Add a the line:  user1 ALL=(ALL) NOPASSWD: ALL  Check that sudo has been granted:  # su user1\n$ cat /etc/sudoers.d/user1\ncat: /etc/sudoers.d/user1: Permission denied\n\n$ sudo cat /etc/sudoers.d/user1\nuser1 ALL=(ALL) NOPASSWD: ALL",
            "title": "Grant sudo Access"
        },
        {
            "location": "/os/booting-on-azure/",
            "text": "Running Flatcar Container Linux on Microsoft Azure\n\u00b6\n\n\nCreating resource group via Microsoft Azure CLI\n\u00b6\n\n\nFollow the \ninstallation and configuration guides\n for the Microsoft Azure CLI to set up your local installation.\n\n\nInstances on Microsoft Azure must be created within a resource group. Create a new resource group with the following command:\n\n\naz group create --name group-1 --location <location>\n\n\n\nNow that you have a resource group, you can choose a channel of Flatcar Container Linux you would like to install.\n\n\nChoosing a Channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be \nupdated automatically\n with different schedules per channel. This feature\ncan be \ndisabled\n, although it is not recommended to do so. The \nrelease notes\n contain\ninformation about specific features and bug fixes.\n\n\nThe following command will create a single instance. For more details, check out \nLaunching via the Microsoft Azure CLI\n.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within\n        the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-stable\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-beta\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks the master branch and is released frequently. The newest versions of system\n        libraries and utilities are available for testing in this channel. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n        and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n        \naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-edge\n\n      \n\n    \n\n  \n\n\n\n\n\nUploading an Image\n\u00b6\n\n\nOfficial Flatcar Container Linux images are not available on Azure at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.\n\n\nTo do so, run the following command:\n\ndocker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload \\\n  --resource-group <resource group> \\\n  --storage-account-name <storage account name>\n\n\nWhere:\n\n\n\n\n<resource group>\n should be a valid \nResource Group\n name.\n\n\n<storage account name>\n should be a valid \nStorage Account\n name.\n\n\n\n\nDuring execution, the script will ask you to log into your Azure account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to Azure.\n\n\nIf uploading fails with one of the following errors, it usually indicates a problem on Azure's side:\n\n\nPut https://mystorage.blob.core.windows.net/vhds?restype=container: dial tcp: lookup iago-dev.blob.core.windows.net on 80.58.61.250:53: no such host\n\n\n\nstorage: service returned error: StatusCode=403, ErrorCode=AuthenticationFailed, ErrorMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. RequestId:a3ed1ebc-701e-010c-5258-0a2e84000000 Time:2019-05-14T13:26:00.1253383Z, RequestId=a3ed1ebc-701e-010c-5258-0a2e84000000, QueryParameterName=, QueryParameterValue=\n\n\n\nThe command is idempotent and it is therefore safe to re-run it in case of failure.\n\n\nTo see all available options, run:\n\ndocker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -g, --resource-group        Azure resource group.\n  -s, --storage-account-name  Azure storage account name. Must be between 3 and 24 characters and unique within Azure.\n\n Optional arguments:\n  -c, --channel              Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version              Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name           Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n  -l, --location             Azure location to storage image. To list available locations run with '--locations'. Defaults to 'westeurope'.\n  -S, --storage-account-type Type of storage account. Defaults to 'Standard_LRS'.\n\n\nThe Dockerfile for the \nquay.io/kinvolk/azure-flatcar-image-upload\n image is managed \nhere\n.\n\n\nContainer Linux Config\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more\nvia a Container Linux Config. Head over to the \nprovisioning docs\n to learn how to use Container Linux Configs.\nNote that Microsoft Azure doesn't allow an instance's userdata to be modified after the instance had been launched. This\nisn't a problem since Ignition, the tool that consumes the userdata, only runs on the first boot.\n\n\nYou can provide a raw Ignition config (produced from a Container Linux Config) to Flatcar Container Linux via the \nMicrosoft Azure CLI\n.\n\n\nAs an example, the following config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nLaunching Instances via the Microsoft Azure CLI\n\u00b6\n\n\nYou can lunch instance of Flatcar Container Linux by executing following command:\n\n\naz vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nFor information on using Flatcar Container Linux check out the \nFlatcar Container Linux quickstart guide\n or dive into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on Microsoft Azure"
        },
        {
            "location": "/os/booting-on-azure/#running-flatcar-container-linux-on-microsoft-azure",
            "text": "",
            "title": "Running Flatcar Container Linux on Microsoft Azure"
        },
        {
            "location": "/os/booting-on-azure/#creating-resource-group-via-microsoft-azure-cli",
            "text": "Follow the  installation and configuration guides  for the Microsoft Azure CLI to set up your local installation.  Instances on Microsoft Azure must be created within a resource group. Create a new resource group with the following command:  az group create --name group-1 --location <location>  Now that you have a resource group, you can choose a channel of Flatcar Container Linux you would like to install.",
            "title": "Creating resource group via Microsoft Azure CLI"
        },
        {
            "location": "/os/booting-on-azure/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be  updated automatically  with different schedules per channel. This feature\ncan be  disabled , although it is not recommended to do so. The  release notes  contain\ninformation about specific features and bug fixes.  The following command will create a single instance. For more details, check out  Launching via the Microsoft Azure CLI .  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within\n        the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-stable \n       \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-beta \n       \n     \n     \n       \n         The Alpha channel closely tracks the master branch and is released frequently. The newest versions of system\n        libraries and utilities are available for testing in this channel. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha \n       \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n        and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n         az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-edge",
            "title": "Choosing a Channel"
        },
        {
            "location": "/os/booting-on-azure/#uploading-an-image",
            "text": "Official Flatcar Container Linux images are not available on Azure at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.  To do so, run the following command: docker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload \\\n  --resource-group <resource group> \\\n  --storage-account-name <storage account name>  Where:   <resource group>  should be a valid  Resource Group  name.  <storage account name>  should be a valid  Storage Account  name.   During execution, the script will ask you to log into your Azure account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to Azure.  If uploading fails with one of the following errors, it usually indicates a problem on Azure's side:  Put https://mystorage.blob.core.windows.net/vhds?restype=container: dial tcp: lookup iago-dev.blob.core.windows.net on 80.58.61.250:53: no such host  storage: service returned error: StatusCode=403, ErrorCode=AuthenticationFailed, ErrorMessage=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. RequestId:a3ed1ebc-701e-010c-5258-0a2e84000000 Time:2019-05-14T13:26:00.1253383Z, RequestId=a3ed1ebc-701e-010c-5258-0a2e84000000, QueryParameterName=, QueryParameterValue=  The command is idempotent and it is therefore safe to re-run it in case of failure.  To see all available options, run: docker run -it --rm quay.io/kinvolk/azure-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -g, --resource-group        Azure resource group.\n  -s, --storage-account-name  Azure storage account name. Must be between 3 and 24 characters and unique within Azure.\n\n Optional arguments:\n  -c, --channel              Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version              Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name           Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n  -l, --location             Azure location to storage image. To list available locations run with '--locations'. Defaults to 'westeurope'.\n  -S, --storage-account-type Type of storage account. Defaults to 'Standard_LRS'.  The Dockerfile for the  quay.io/kinvolk/azure-flatcar-image-upload  image is managed  here .",
            "title": "Uploading an Image"
        },
        {
            "location": "/os/booting-on-azure/#container-linux-config",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more\nvia a Container Linux Config. Head over to the  provisioning docs  to learn how to use Container Linux Configs.\nNote that Microsoft Azure doesn't allow an instance's userdata to be modified after the instance had been launched. This\nisn't a problem since Ignition, the tool that consumes the userdata, only runs on the first boot.  You can provide a raw Ignition config (produced from a Container Linux Config) to Flatcar Container Linux via the  Microsoft Azure CLI .  As an example, the following config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/booting-on-azure/#launching-instances-via-the-microsoft-azure-cli",
            "text": "You can lunch instance of Flatcar Container Linux by executing following command:  az vm create --name node-1 --resource-group group-1 --admin-username core --custom-data \"$(cat config.ign)\" --image flatcar-alpha",
            "title": "Launching Instances via the Microsoft Azure CLI"
        },
        {
            "location": "/os/booting-on-azure/#using-flatcar-container-linux",
            "text": "For information on using Flatcar Container Linux check out the  Flatcar Container Linux quickstart guide  or dive into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-digitalocean/",
            "text": "Running Flatcar Container Linux on DigitalOcean\n\u00b6\n\n\nOn Digital Ocean, users can upload Flatcar Container Linux as a \ncustom image\n. Digital Ocean offers a \nquick start guide\n that walks you through the process.\n\n\nThe \nimport URL\n should be \nhttps://<channel>.release.flatcar-linux.net/amd64-usr/<version>/flatcar_production_digitalocean_image.bin.bz2\n. See the \nrelease page\n for version and channel history.\n\n\nThe following command will create a single droplet. For more details, check out \nLaunching via the API\n.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Container Linux {{site.alpha-channel}}.\n\n        \nLaunch Flatcar Container Linux Droplet\n\n        \nLaunch via DigitalOcean API by specifying \n$REGION\n, \n$SIZE\n and \n$SSH_KEY_ID\n:\n\n        \ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-alpha\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'\n\n      \n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Container Linux {{site.beta-channel}}.\n\n        \nLaunch Flatcar Container Linux Droplet\n\n        \nLaunch via DigitalOcean API by specifying \n$REGION\n, \n$SIZE\n and \n$SSH_KEY_ID\n:\n\n        \ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-beta\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'\n\n      \n\n    \n\n    \n\n      \n\n        \n\n        \nThe Stable channel should be used by production clusters. Versions of Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Container Linux {{site.stable-channel}}.\n\n        \nLaunch Flatcar Container Linux Droplet\n\n        \nLaunch via DigitalOcean API by specifying \n$REGION\n, \n$SIZE\n and \n$SSH_KEY_ID\n:\n\n        \ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-stable\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'\n\n      \n\n      \n\n    \n\n  \n\n\n\n\n\nContainer Linux Configs\n\u00b6\n\n\nContainer Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n. Note that DigitalOcean doesn't allow an instance's userdata to be modified after the instance has been launched. This isn't a problem since Ignition only runs on the first boot.\n\n\nYou can provide a raw Ignition config to Container Linux via the DigitalOcean web console or \nvia the DigitalOcean API\n.\n\n\nAs an example, this config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nAdding more machines\n\u00b6\n\n\nTo add more instances to the cluster, just launch more with the same Container Linux Config. New instances will join the cluster regardless of region.\n\n\nSSH to your droplets\n\u00b6\n\n\nContainer Linux is set up to be a little more secure than other DigitalOcean images. By default, it uses the core user instead of root and doesn't use a password for authentication. You'll need to add an SSH key(s) via the web console or add keys/passwords via your Ignition config in order to log in.\n\n\nTo connect to a droplet after it's created, run:\n\n\nssh core@<ip address>\n\n\n\nLaunching droplets\n\u00b6\n\n\nVia the API\n\u00b6\n\n\nFor starters, generate a \nPersonal Access Token\n and save it in an environment variable:\n\n\nread TOKEN\n# Enter your Personal Access Token\n\n\n\nUpload your SSH key via \nDigitalOcean's API\n or the web console. Retrieve the SSH key ID via the \n\"list all keys\"\n method:\n\n\ncurl --request GET \"https://api.digitalocean.com/v2/account/keys\" \\\n     --header \"Authorization: Bearer $TOKEN\"\n\n\n\nSave the key ID from the previous command in an environment variable:\n\n\nread SSH_KEY_ID\n# Enter your SSH key ID\n\n\n\nCreate a 512MB droplet with private networking in NYC3 from the Container Linux Stable image:\n\n\ncurl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\n      \"region\":\"nyc3\",\n      \"image\":\"coreos-stable\",\n      \"size\":\"512mb\",\n      \"name\":\"core-1\",\n      \"private_networking\":true,\n      \"ssh_keys\":['$SSH_KEY_ID'],\n      \"user_data\": \"'\"$(cat config.ign | sed 's/\"/\\\\\"/g')\"'\"\n}'\n\n\n\n\nFor more details, check out \nDigitalOcean's API documentation\n.\n\n\nVia the web console\n\u00b6\n\n\n\n\nOpen the \n\"new droplet\"\n page in the web console.\n\n\nGive the machine a hostname, select the size, and choose a region.\n\n\n  \n\n    \n\n    \nChoosing a size and hostname\n\n  \n\n\n\n\nEnable User Data and add your Ignition config in the text box.\n\n\n  \n\n    \n\n    \nDroplet settings for networking and Ignition\n\n  \n\n\n\n\nChoose your \npreferred channel\n of Container Linux.\n\n\n  \n\n    \n\n    \nChoosing a Container Linux channel\n\n  \n\n\n\n\nSelect your SSH keys.\n\n\n\n\nNote that DigitalOcean is not able to inject a root password into Flatcar Container Linux images like it does with other images. You'll need to add your keys via the web console or add keys or passwords via your Container Linux Config in order to log in.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on DigitalOcean"
        },
        {
            "location": "/os/booting-on-digitalocean/#running-flatcar-container-linux-on-digitalocean",
            "text": "On Digital Ocean, users can upload Flatcar Container Linux as a  custom image . Digital Ocean offers a  quick start guide  that walks you through the process.  The  import URL  should be  https://<channel>.release.flatcar-linux.net/amd64-usr/<version>/flatcar_production_digitalocean_image.bin.bz2 . See the  release page  for version and channel history.  The following command will create a single droplet. For more details, check out  Launching via the API .  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n   \n   \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Container Linux {{site.alpha-channel}}. \n         Launch Flatcar Container Linux Droplet \n         Launch via DigitalOcean API by specifying  $REGION ,  $SIZE  and  $SSH_KEY_ID : \n         curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-alpha\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}' \n       \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Container Linux {{site.beta-channel}}. \n         Launch Flatcar Container Linux Droplet \n         Launch via DigitalOcean API by specifying  $REGION ,  $SIZE  and  $SSH_KEY_ID : \n         curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-beta\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}' \n       \n     \n     \n       \n         \n         The Stable channel should be used by production clusters. Versions of Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Container Linux {{site.stable-channel}}. \n         Launch Flatcar Container Linux Droplet \n         Launch via DigitalOcean API by specifying  $REGION ,  $SIZE  and  $SSH_KEY_ID : \n         curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\"region\":\"'\"${REGION}\"'\",\n        \"image\":\"flatcar-stable\",\n        \"size\":\"'\"$SIZE\"'\",\n        \"user_data\": \"'\"$(cat ~/config.ign)\"'\",\n        \"ssh_keys\":[\"'\"$SSH_KEY_ID\"'\"],\n        \"name\":\"core-1\"}'",
            "title": "Running Flatcar Container Linux on DigitalOcean"
        },
        {
            "location": "/os/booting-on-digitalocean/#container-linux-configs",
            "text": "Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features . Note that DigitalOcean doesn't allow an instance's userdata to be modified after the instance has been launched. This isn't a problem since Ignition only runs on the first boot.  You can provide a raw Ignition config to Container Linux via the DigitalOcean web console or  via the DigitalOcean API .  As an example, this config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-on-digitalocean/#adding-more-machines",
            "text": "To add more instances to the cluster, just launch more with the same Container Linux Config. New instances will join the cluster regardless of region.",
            "title": "Adding more machines"
        },
        {
            "location": "/os/booting-on-digitalocean/#ssh-to-your-droplets",
            "text": "Container Linux is set up to be a little more secure than other DigitalOcean images. By default, it uses the core user instead of root and doesn't use a password for authentication. You'll need to add an SSH key(s) via the web console or add keys/passwords via your Ignition config in order to log in.  To connect to a droplet after it's created, run:  ssh core@<ip address>",
            "title": "SSH to your droplets"
        },
        {
            "location": "/os/booting-on-digitalocean/#launching-droplets",
            "text": "",
            "title": "Launching droplets"
        },
        {
            "location": "/os/booting-on-digitalocean/#via-the-api",
            "text": "For starters, generate a  Personal Access Token  and save it in an environment variable:  read TOKEN\n# Enter your Personal Access Token  Upload your SSH key via  DigitalOcean's API  or the web console. Retrieve the SSH key ID via the  \"list all keys\"  method:  curl --request GET \"https://api.digitalocean.com/v2/account/keys\" \\\n     --header \"Authorization: Bearer $TOKEN\"  Save the key ID from the previous command in an environment variable:  read SSH_KEY_ID\n# Enter your SSH key ID  Create a 512MB droplet with private networking in NYC3 from the Container Linux Stable image:  curl --request POST \"https://api.digitalocean.com/v2/droplets\" \\\n     --header \"Content-Type: application/json\" \\\n     --header \"Authorization: Bearer $TOKEN\" \\\n     --data '{\n      \"region\":\"nyc3\",\n      \"image\":\"coreos-stable\",\n      \"size\":\"512mb\",\n      \"name\":\"core-1\",\n      \"private_networking\":true,\n      \"ssh_keys\":['$SSH_KEY_ID'],\n      \"user_data\": \"'\"$(cat config.ign | sed 's/\"/\\\\\"/g')\"'\"\n}'  For more details, check out  DigitalOcean's API documentation .",
            "title": "Via the API"
        },
        {
            "location": "/os/booting-on-digitalocean/#via-the-web-console",
            "text": "Open the  \"new droplet\"  page in the web console.  Give the machine a hostname, select the size, and choose a region. \n   \n     \n     Choosing a size and hostname \n     Enable User Data and add your Ignition config in the text box. \n   \n     \n     Droplet settings for networking and Ignition \n     Choose your  preferred channel  of Container Linux. \n   \n     \n     Choosing a Container Linux channel \n     Select your SSH keys.   Note that DigitalOcean is not able to inject a root password into Flatcar Container Linux images like it does with other images. You'll need to add your keys via the web console or add keys or passwords via your Container Linux Config in order to log in.",
            "title": "Via the web console"
        },
        {
            "location": "/os/booting-on-digitalocean/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-ec2/",
            "text": "Running Flatcar Container Linux on EC2\n\u00b6\n\n\nThe current AMIs for all Flatcar Container Linux channels and EC2 regions are listed below and updated frequently. Using CloudFormation is the easiest way to launch a cluster, but it is also possible to follow the manual steps at the end of the article. Questions can be directed to the Flatcar Container Linux \nIRC channel\n or \nuser mailing list\n.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n        View as json feed: \namd64\n\n        {% if site.data.alpha-channel-arm.amis.size > 0 %}\n        \narm64\n\n        {% endif %}\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.alpha-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.alpha-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n        View as json feed: \namd64\n\n        {% if site.data.beta-channel-arm.amis.size > 0 %}\n        \narm64\n\n        {% endif %}\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.beta-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.beta-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.edge-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.edge-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n        View as json feed: \namd64\n\n        {% if site.data.stable-channel-arm.amis.size > 0 %}\n        \narm64\n\n        {% endif %}\n      \n\n      \n\n        \n\n          \n\n            \nEC2 Region\n\n            \nAMI Type\n\n            \nAMI ID\n\n            \nCloudFormation\n\n          \n\n        \n\n        \n\n        {% for region in site.data.stable-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n        \n\n          \n{{ region.name }}\n\n          \nHVM (amd64)\n\n          \n{{ region.hvm }}\n\n          \n\n        \n\n        \n\n          {% for region_arm in site.data.stable-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n          \nHVM (arm64)\n\n          \n{{ region_arm.hvm }}\n\n          \n\n          {% endif %}\n          {% endfor %}\n        \n\n        {% endfor %}\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nCloudFormation will launch a cluster of Flatcar Container Linux machines with a security and autoscaling group.\n\n\nContainer Linux Configs\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n.\n\n\nYou can provide a raw Ignition config to Flatcar Container Linux via the Amazon web console or \nvia the EC2 API\n.\n\n\nAs an example, this Container Linux Config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nInstance storage\n\u00b6\n\n\nEphemeral disks and additional EBS volumes attached to instances can be mounted with a \n.mount\n unit. Amazon's block storage devices are attached differently \ndepending on the instance type\n. Here's the Container Linux Config to format and mount the first ephemeral disk, \nxvdb\n, on most instance types:\n\n\nstorage:\n  filesystems:\n    - mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\n\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target\n\n\n\nFor more information about mounting storage, Amazon's \nown documentation\n is the best source. You can also read about \nmounting storage on Flatcar Container Linux\n.\n\n\nAdding more machines\n\u00b6\n\n\nTo add more instances to the cluster, just launch more with the same Container Linux Config, the appropriate security group and the AMI for that region. New instances will join the cluster regardless of region if the security groups are configured correctly.\n\n\nSSH to your instances\n\u00b6\n\n\nFlatcar Container Linux is set up to be a little more secure than other cloud images. By default, it uses the \ncore\n user instead of \nroot\n and doesn't use a password for authentication. You'll need to add an SSH key(s) via the AWS console or add keys/passwords via your Container Linux Config in order to log in.\n\n\nTo connect to an instance after it's created, run:\n\n\nssh core@<ip address>\n\n\n\nMultiple clusters\n\u00b6\n\n\nIf you would like to create multiple clusters you will need to change the \"Stack Name\". You can find the direct \ntemplate file on S3\n.\n\n\nManual setup\n\u00b6\n\n\n{% for region in site.data.alpha-channel.amis %}\n  {% if region.name == 'us-east-1' %}\n\nTL;DR:\n launch three instances of \n{{region.hvm}}\n (amd64) in \n{{region.name}}\n with a security group that has open port 22, 2379, 2380, 4001, and 7001 and the same \"User Data\" of each host. SSH uses the \ncore\n user and you have \netcd\n and \nDocker\n to play with.\n  {% endif %}\n{% endfor %}\n\n\nCreating the security group\n\u00b6\n\n\nYou need open port 2379, 2380, 7001 and 4001 between servers in the \netcd\n cluster. Step by step instructions below.\n\n\nThis step is only needed once\n\n\nFirst we need to create a security group to allow Flatcar Container Linux instances to communicate with one another.\n\n\n\n\nGo to the \nsecurity group\n page in the EC2 console.\n\n\nClick \"Create Security Group\"\n\n\nName: flatcar-testing\n\n\nDescription: Flatcar Container Linux instances\n\n\nVPC: No VPC\n\n\nClick: \"Yes, Create\"\n\n\n\n\n\n\nIn the details of the security group, click the \nInbound\n tab\n\n\nFirst, create a security group rule for SSH\n\n\nCreate a new rule: \nSSH\n\n\nSource: 0.0.0.0/0\n\n\nClick: \"Add Rule\"\n\n\n\n\n\n\nAdd two security group rules for etcd communication\n\n\nCreate a new rule: \nCustom TCP rule\n\n\nPort range: 2379\n\n\nSource: type \"flatcar-testing\" until your security group auto-completes. Should be something like \"sg-8d4feabc\"\n\n\nClick: \"Add Rule\"\n\n\nRepeat this process for port range 2380, 4001 and 7001 as well\n\n\n\n\n\n\nClick \"Apply Rule Changes\"\n\n\n\n\nLaunching a test cluster\n\u00b6\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n  \n\n  \n\n    \n\n      \nWe will be launching three instances, with a few parameters in the User Data, and selecting our security group.\n\n      \n\n        \n\n        {% for region in site.data.alpha-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the \nquick launch wizard\n to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n        \n\n        \n\n          On the second page of the wizard, launch 3 servers to test our clustering\n          \n\n            \nNumber of instances: 3\n\n            \nClick \"Continue\"\n\n          \n\n        \n\n        \n\n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at \nhttps://discovery.etcd.io/new?size=3\n, configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n        \n\n        \n\n          Use \nct\n to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:3\u0003\n          \n\n            \nPaste configuration into \"User Data\"\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Storage Configuration\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Tags\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Create Key Pair\n          \n\n            \nChoose a key of your choice, it will be added in addition to the one in the gist.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Choose one or more of your existing Security Groups\n          \n\n            \n\"flatcar-testing\" as above.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Launch!\n        \n\n      \n\n    \n\n    \n\n      \nWe will be launching three instances, with a few parameters in the User Data, and selecting our security group.\n\n      \n\n        \n\n        {% for region in site.data.beta-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the \nquick launch wizard\n to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n        \n\n        \n\n          On the second page of the wizard, launch 3 servers to test our clustering\n          \n\n            \nNumber of instances: 3\n\n            \nClick \"Continue\"\n\n          \n\n        \n\n        \n\n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at \nhttps://discovery.etcd.io/new?size=3\n, configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n        \n\n        \n\n          Use \nct\n to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:4\u0003\n          \n\n            \nPaste configuration into \"User Data\"\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Storage Configuration\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Tags\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Create Key Pair\n          \n\n            \nChoose a key of your choice, it will be added in addition to the one in the gist.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Choose one or more of your existing Security Groups\n          \n\n            \n\"flatcar-testing\" as above.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Launch!\n        \n\n      \n\n    \n\n    \n\n      \nWe will be launching three instances, with a few parameters in the User Data, and selecting our security group.\n\n      \n\n        \n\n        {% for region in site.data.stable-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the \nquick launch wizard\n to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n        \n\n        \n\n          On the second page of the wizard, launch 3 servers to test our clustering\n          \n\n            \nNumber of instances: 3\n\n            \nClick \"Continue\"\n\n          \n\n        \n\n        \n\n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at \nhttps://discovery.etcd.io/new?size=3\n, configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n        \n\n        \n\n          Use \nct\n to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:5\u0003\n          \n\n            \nPaste configuration into \"User Data\"\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Storage Configuration\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Tags\n          \n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Create Key Pair\n          \n\n            \nChoose a key of your choice, it will be added in addition to the one in the gist.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Choose one or more of your existing Security Groups\n          \n\n            \n\"flatcar-testing\" as above.\n\n            \n\"Continue\"\n\n          \n\n        \n\n        \n\n          Launch!\n        \n\n      \n\n    \n\n  \n\n\n\n\n\nInstallation from a VMDK image\n\u00b6\n\n\nOne of the possible ways of installation is to import the generated VMDK Flatcar image as a snapshot. The image file will be in \nhttps://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2\n.\nMake sure you download the signature (it's available in \nhttps://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2.sig\n) and check it before proceeding.\n\n\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2.sig\n$ gpg --verify flatcar_production_ami_vmdk_image.vmdk.bz2.sig\ngpg: assuming signed data in 'flatcar_production_ami_vmdk_image.vmdk.bz2'\ngpg: Signature made Thu 15 Mar 2018 10:27:57 AM CET\ngpg:                using RSA key A621F1DA96C93C639506832D603443A1D0FC498C\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" [ultimate]\n\n\n\nThen, follow the instructions in \nImporting a Disk as a Snapshot Using VM Import/Export\n. You'll need to upload the uncompressed vmdk file to S3.\n\n\nAfter the snapshot is imported, you can go to \"Snapshots\" in the EC2 dashboard, and generate an AMI image from it.\nTo make it work, use \n/dev/sda2\n as the \"Root device name\" and you probably want to select \"Hardware-assisted virtualization\" as \"Virtualization type\".\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on EC2"
        },
        {
            "location": "/os/booting-on-ec2/#running-flatcar-container-linux-on-ec2",
            "text": "The current AMIs for all Flatcar Container Linux channels and EC2 regions are listed below and updated frequently. Using CloudFormation is the easiest way to launch a cluster, but it is also possible to follow the manual steps at the end of the article. Questions can be directed to the Flatcar Container Linux  IRC channel  or  user mailing list .",
            "title": "Running Flatcar Container Linux on EC2"
        },
        {
            "location": "/os/booting-on-ec2/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n        View as json feed:  amd64 \n        {% if site.data.alpha-channel-arm.amis.size > 0 %}\n         arm64 \n        {% endif %}\n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.alpha-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.alpha-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n        View as json feed:  amd64 \n        {% if site.data.beta-channel-arm.amis.size > 0 %}\n         arm64 \n        {% endif %}\n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.beta-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.beta-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.edge-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.edge-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n        View as json feed:  amd64 \n        {% if site.data.stable-channel-arm.amis.size > 0 %}\n         arm64 \n        {% endif %}\n       \n       \n         \n           \n             EC2 Region \n             AMI Type \n             AMI ID \n             CloudFormation \n           \n         \n         \n        {% for region in site.data.stable-channel.amis %}\n        {% capture region_domain %}{% if region.name == 'us-gov-west-1' %}amazonaws-us-gov.com{% elsif region.name == 'cn-north-1' or region.name == 'cn-northwest-1' %}amazonaws.cn{% else %}aws.amazon.com{% endif %}{% endcapture %}\n         \n           {{ region.name }} \n           HVM (amd64) \n           {{ region.hvm }} \n           \n         \n         \n          {% for region_arm in site.data.stable-channel-arm.amis %}\n          {% if region_arm.name == region.name %}\n           HVM (arm64) \n           {{ region_arm.hvm }} \n           \n          {% endif %}\n          {% endfor %}\n         \n        {% endfor %}\n         \n       \n     \n     CloudFormation will launch a cluster of Flatcar Container Linux machines with a security and autoscaling group.",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-on-ec2/#container-linux-configs",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features .  You can provide a raw Ignition config to Flatcar Container Linux via the Amazon web console or  via the EC2 API .  As an example, this Container Linux Config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-on-ec2/#instance-storage",
            "text": "Ephemeral disks and additional EBS volumes attached to instances can be mounted with a  .mount  unit. Amazon's block storage devices are attached differently  depending on the instance type . Here's the Container Linux Config to format and mount the first ephemeral disk,  xvdb , on most instance types:  storage:\n  filesystems:\n    - mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\n\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target  For more information about mounting storage, Amazon's  own documentation  is the best source. You can also read about  mounting storage on Flatcar Container Linux .",
            "title": "Instance storage"
        },
        {
            "location": "/os/booting-on-ec2/#adding-more-machines",
            "text": "To add more instances to the cluster, just launch more with the same Container Linux Config, the appropriate security group and the AMI for that region. New instances will join the cluster regardless of region if the security groups are configured correctly.",
            "title": "Adding more machines"
        },
        {
            "location": "/os/booting-on-ec2/#ssh-to-your-instances",
            "text": "Flatcar Container Linux is set up to be a little more secure than other cloud images. By default, it uses the  core  user instead of  root  and doesn't use a password for authentication. You'll need to add an SSH key(s) via the AWS console or add keys/passwords via your Container Linux Config in order to log in.  To connect to an instance after it's created, run:  ssh core@<ip address>",
            "title": "SSH to your instances"
        },
        {
            "location": "/os/booting-on-ec2/#multiple-clusters",
            "text": "If you would like to create multiple clusters you will need to change the \"Stack Name\". You can find the direct  template file on S3 .",
            "title": "Multiple clusters"
        },
        {
            "location": "/os/booting-on-ec2/#manual-setup",
            "text": "{% for region in site.data.alpha-channel.amis %}\n  {% if region.name == 'us-east-1' %} TL;DR:  launch three instances of  {{region.hvm}}  (amd64) in  {{region.name}}  with a security group that has open port 22, 2379, 2380, 4001, and 7001 and the same \"User Data\" of each host. SSH uses the  core  user and you have  etcd  and  Docker  to play with.\n  {% endif %}\n{% endfor %}",
            "title": "Manual setup"
        },
        {
            "location": "/os/booting-on-ec2/#creating-the-security-group",
            "text": "You need open port 2379, 2380, 7001 and 4001 between servers in the  etcd  cluster. Step by step instructions below.  This step is only needed once  First we need to create a security group to allow Flatcar Container Linux instances to communicate with one another.   Go to the  security group  page in the EC2 console.  Click \"Create Security Group\"  Name: flatcar-testing  Description: Flatcar Container Linux instances  VPC: No VPC  Click: \"Yes, Create\"    In the details of the security group, click the  Inbound  tab  First, create a security group rule for SSH  Create a new rule:  SSH  Source: 0.0.0.0/0  Click: \"Add Rule\"    Add two security group rules for etcd communication  Create a new rule:  Custom TCP rule  Port range: 2379  Source: type \"flatcar-testing\" until your security group auto-completes. Should be something like \"sg-8d4feabc\"  Click: \"Add Rule\"  Repeat this process for port range 2380, 4001 and 7001 as well    Click \"Apply Rule Changes\"",
            "title": "Creating the security group"
        },
        {
            "location": "/os/booting-on-ec2/#launching-a-test-cluster",
            "text": "Stable Channel \n     Beta Channel \n     Alpha Channel \n   \n   \n     \n       We will be launching three instances, with a few parameters in the User Data, and selecting our security group. \n       \n         \n        {% for region in site.data.alpha-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the  quick launch wizard  to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n         \n         \n          On the second page of the wizard, launch 3 servers to test our clustering\n           \n             Number of instances: 3 \n             Click \"Continue\" \n           \n         \n         \n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at  https://discovery.etcd.io/new?size=3 , configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n         \n         \n          Use  ct  to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:3\u0003\n           \n             Paste configuration into \"User Data\" \n             \"Continue\" \n           \n         \n         \n          Storage Configuration\n           \n             \"Continue\" \n           \n         \n         \n          Tags\n           \n             \"Continue\" \n           \n         \n         \n          Create Key Pair\n           \n             Choose a key of your choice, it will be added in addition to the one in the gist. \n             \"Continue\" \n           \n         \n         \n          Choose one or more of your existing Security Groups\n           \n             \"flatcar-testing\" as above. \n             \"Continue\" \n           \n         \n         \n          Launch!\n         \n       \n     \n     \n       We will be launching three instances, with a few parameters in the User Data, and selecting our security group. \n       \n         \n        {% for region in site.data.beta-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the  quick launch wizard  to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n         \n         \n          On the second page of the wizard, launch 3 servers to test our clustering\n           \n             Number of instances: 3 \n             Click \"Continue\" \n           \n         \n         \n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at  https://discovery.etcd.io/new?size=3 , configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n         \n         \n          Use  ct  to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:4\u0003\n           \n             Paste configuration into \"User Data\" \n             \"Continue\" \n           \n         \n         \n          Storage Configuration\n           \n             \"Continue\" \n           \n         \n         \n          Tags\n           \n             \"Continue\" \n           \n         \n         \n          Create Key Pair\n           \n             Choose a key of your choice, it will be added in addition to the one in the gist. \n             \"Continue\" \n           \n         \n         \n          Choose one or more of your existing Security Groups\n           \n             \"flatcar-testing\" as above. \n             \"Continue\" \n           \n         \n         \n          Launch!\n         \n       \n     \n     \n       We will be launching three instances, with a few parameters in the User Data, and selecting our security group. \n       \n         \n        {% for region in site.data.stable-channel.amis %}\n          {% if region.name == 'us-east-1' %}\n            Open the  quick launch wizard  to boot {{region.hvm}} (amd64).\n          {% endif %}\n        {% endfor %}\n         \n         \n          On the second page of the wizard, launch 3 servers to test our clustering\n           \n             Number of instances: 3 \n             Click \"Continue\" \n           \n         \n         \n          Next, we need to specify a discovery URL, which contains a unique token that allows us to find other hosts in our cluster. If you're launching your first machine, generate one at  https://discovery.etcd.io/new?size=3 , configure the `?size=` to your initial cluster size and add it to the metadata. You should re-use this key for each machine in the cluster.\n         \n         \n          Use  ct  to convert the following configuration into an Ignition config, and back in the EC2 dashboard, paste it into the \"User Data\" field.\n          \u0002wzxhzdk:5\u0003\n           \n             Paste configuration into \"User Data\" \n             \"Continue\" \n           \n         \n         \n          Storage Configuration\n           \n             \"Continue\" \n           \n         \n         \n          Tags\n           \n             \"Continue\" \n           \n         \n         \n          Create Key Pair\n           \n             Choose a key of your choice, it will be added in addition to the one in the gist. \n             \"Continue\" \n           \n         \n         \n          Choose one or more of your existing Security Groups\n           \n             \"flatcar-testing\" as above. \n             \"Continue\" \n           \n         \n         \n          Launch!",
            "title": "Launching a test cluster"
        },
        {
            "location": "/os/booting-on-ec2/#installation-from-a-vmdk-image",
            "text": "One of the possible ways of installation is to import the generated VMDK Flatcar image as a snapshot. The image file will be in  https://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2 .\nMake sure you download the signature (it's available in  https://${CHANNEL}.release.flatcar-linux.net/${ARCH}-usr/${VERSION}/flatcar_production_ami_vmdk_image.vmdk.bz2.sig ) and check it before proceeding.  $ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_ami_vmdk_image.vmdk.bz2.sig\n$ gpg --verify flatcar_production_ami_vmdk_image.vmdk.bz2.sig\ngpg: assuming signed data in 'flatcar_production_ami_vmdk_image.vmdk.bz2'\ngpg: Signature made Thu 15 Mar 2018 10:27:57 AM CET\ngpg:                using RSA key A621F1DA96C93C639506832D603443A1D0FC498C\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" [ultimate]  Then, follow the instructions in  Importing a Disk as a Snapshot Using VM Import/Export . You'll need to upload the uncompressed vmdk file to S3.  After the snapshot is imported, you can go to \"Snapshots\" in the EC2 dashboard, and generate an AMI image from it.\nTo make it work, use  /dev/sda2  as the \"Root device name\" and you probably want to select \"Hardware-assisted virtualization\" as \"Virtualization type\".",
            "title": "Installation from a VMDK image"
        },
        {
            "location": "/os/booting-on-ec2/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-ecs/",
            "text": "Running Flatcar Container Linux with AWS EC2 Container Service\n\u00b6\n\n\nAmazon EC2 Container Service (ECS)\n is a container management service which provides a set of APIs for scheduling container workloads across EC2 clusters. It supports Flatcar Container Linux with Docker containers.\n\n\nYour Flatcar Container Linux machines communicate with ECS via an agent. The agent interacts with Docker to start new containers and gather information about running containers.\n\n\nSet up a new cluster\n\u00b6\n\n\nWhen booting your \nFlatcar Container Linux Machines on EC2\n, configure the ECS agent to be started via \nIgnition\n.\n\n\nBe sure to change \nECS_CLUSTER\n to the cluster name you've configured via the ECS CLI or leave it empty for the default. Here's a full config example:\n\n\nstorage:\n  files:\n    - path: /var/lib/iptables/rules-save\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          *nat\n          -A PREROUTING -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 127.0.0.1:51679\n          -A OUTPUT -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679\n          COMMIT\n    - path: /etc/sysctl.d/localnet.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.ipv4.conf.all.route_localnet=1\n\nsystemd:\n units:\n   - name: iptables-restore.service\n     enable: true\n   - name: systemd-sysctl.service\n     enable: true\n   - name: amazon-ecs-agent.service\n     enable: true\n     contents: |\n       [Unit]\n       Description=AWS ECS Agent\n       Documentation=https://docs.aws.amazon.com/AmazonECS/latest/developerguide/\n       Requires=docker.socket\n       After=docker.socket\n\n       [Service]\n       Environment=ECS_CLUSTER=your_cluster_name\n       Environment=ECS_LOGLEVEL=info\n       Environment=ECS_VERSION=latest\n       Restart=on-failure\n       RestartSec=30\n       RestartPreventExitStatus=5\n       SyslogIdentifier=ecs-agent\n       ExecStartPre=-/bin/mkdir -p /var/log/ecs /var/ecs-data /etc/ecs\n       ExecStartPre=-/usr/bin/touch /etc/ecs/ecs.config\n       ExecStartPre=-/usr/bin/docker kill ecs-agent\n       ExecStartPre=-/usr/bin/docker rm ecs-agent\n       ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent:${ECS_VERSION}\n       ExecStart=/usr/bin/docker run \\\n           --name ecs-agent \\\n           --env-file=/etc/ecs/ecs.config \\\n           --volume=/var/run/docker.sock:/var/run/docker.sock \\\n           --volume=/var/log/ecs:/log \\\n           --volume=/var/ecs-data:/data \\\n           --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro \\\n           --volume=/run/docker/execdriver/native:/var/lib/docker/execdriver/native:ro \\\n           --publish=127.0.0.1:51678:51678 \\\n           --publish=127.0.0.1:51679:51679 \\\n           --env=ECS_AVAILABLE_LOGGING_DRIVERS='[\"awslogs\",\"json-file\",\"journald\",\"logentries\",\"splunk\",\"syslog\"]'\n           --env=ECS_ENABLE_TASK_IAM_ROLE=true \\\n           --env=ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true \\\n           --env=ECS_LOGFILE=/log/ecs-agent.log \\\n           --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \\\n           --env=ECS_DATADIR=/data \\\n           --env=ECS_CLUSTER=${ECS_CLUSTER} \\\n           amazon/amazon-ecs-agent:${ECS_VERSION}\n\n       [Install]\n       WantedBy=multi-user.target\n\n\n\nThe example above pulls the latest official Amazon ECS agent container from the Docker Hub when the machine starts. If you ever need to update the agent, it\u2019s as simple as restarting the amazon-ecs-agent service or the Flatcar Container Linux machine.\n\n\nIf you want to configure SSH keys in order to log in, mount disks or configure other options, see the \nContainer Linux Configs documentation\n.\n\n\nFor more information on using ECS, check out the \nofficial Amazon documentation\n.\n\n\nFor more information on using ECS, check out the \nofficial Amazon documentation\n.",
            "title": "Running Flatcar Container Linux with AWS EC2 Container Service"
        },
        {
            "location": "/os/booting-on-ecs/#running-flatcar-container-linux-with-aws-ec2-container-service",
            "text": "Amazon EC2 Container Service (ECS)  is a container management service which provides a set of APIs for scheduling container workloads across EC2 clusters. It supports Flatcar Container Linux with Docker containers.  Your Flatcar Container Linux machines communicate with ECS via an agent. The agent interacts with Docker to start new containers and gather information about running containers.",
            "title": "Running Flatcar Container Linux with AWS EC2 Container Service"
        },
        {
            "location": "/os/booting-on-ecs/#set-up-a-new-cluster",
            "text": "When booting your  Flatcar Container Linux Machines on EC2 , configure the ECS agent to be started via  Ignition .  Be sure to change  ECS_CLUSTER  to the cluster name you've configured via the ECS CLI or leave it empty for the default. Here's a full config example:  storage:\n  files:\n    - path: /var/lib/iptables/rules-save\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          *nat\n          -A PREROUTING -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination 127.0.0.1:51679\n          -A OUTPUT -d 169.254.170.2/32 -p tcp -m tcp --dport 80 -j REDIRECT --to-ports 51679\n          COMMIT\n    - path: /etc/sysctl.d/localnet.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.ipv4.conf.all.route_localnet=1\n\nsystemd:\n units:\n   - name: iptables-restore.service\n     enable: true\n   - name: systemd-sysctl.service\n     enable: true\n   - name: amazon-ecs-agent.service\n     enable: true\n     contents: |\n       [Unit]\n       Description=AWS ECS Agent\n       Documentation=https://docs.aws.amazon.com/AmazonECS/latest/developerguide/\n       Requires=docker.socket\n       After=docker.socket\n\n       [Service]\n       Environment=ECS_CLUSTER=your_cluster_name\n       Environment=ECS_LOGLEVEL=info\n       Environment=ECS_VERSION=latest\n       Restart=on-failure\n       RestartSec=30\n       RestartPreventExitStatus=5\n       SyslogIdentifier=ecs-agent\n       ExecStartPre=-/bin/mkdir -p /var/log/ecs /var/ecs-data /etc/ecs\n       ExecStartPre=-/usr/bin/touch /etc/ecs/ecs.config\n       ExecStartPre=-/usr/bin/docker kill ecs-agent\n       ExecStartPre=-/usr/bin/docker rm ecs-agent\n       ExecStartPre=/usr/bin/docker pull amazon/amazon-ecs-agent:${ECS_VERSION}\n       ExecStart=/usr/bin/docker run \\\n           --name ecs-agent \\\n           --env-file=/etc/ecs/ecs.config \\\n           --volume=/var/run/docker.sock:/var/run/docker.sock \\\n           --volume=/var/log/ecs:/log \\\n           --volume=/var/ecs-data:/data \\\n           --volume=/sys/fs/cgroup:/sys/fs/cgroup:ro \\\n           --volume=/run/docker/execdriver/native:/var/lib/docker/execdriver/native:ro \\\n           --publish=127.0.0.1:51678:51678 \\\n           --publish=127.0.0.1:51679:51679 \\\n           --env=ECS_AVAILABLE_LOGGING_DRIVERS='[\"awslogs\",\"json-file\",\"journald\",\"logentries\",\"splunk\",\"syslog\"]'\n           --env=ECS_ENABLE_TASK_IAM_ROLE=true \\\n           --env=ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true \\\n           --env=ECS_LOGFILE=/log/ecs-agent.log \\\n           --env=ECS_LOGLEVEL=${ECS_LOGLEVEL} \\\n           --env=ECS_DATADIR=/data \\\n           --env=ECS_CLUSTER=${ECS_CLUSTER} \\\n           amazon/amazon-ecs-agent:${ECS_VERSION}\n\n       [Install]\n       WantedBy=multi-user.target  The example above pulls the latest official Amazon ECS agent container from the Docker Hub when the machine starts. If you ever need to update the agent, it\u2019s as simple as restarting the amazon-ecs-agent service or the Flatcar Container Linux machine.  If you want to configure SSH keys in order to log in, mount disks or configure other options, see the  Container Linux Configs documentation .  For more information on using ECS, check out the  official Amazon documentation .  For more information on using ECS, check out the  official Amazon documentation .",
            "title": "Set up a new cluster"
        },
        {
            "location": "/os/booting-on-google-compute-engine/",
            "text": "Running Flatcar Container Linux on Google Compute Engine\n\u00b6\n\n\nBefore proceeding, you will need a GCE account (\nGCE free trial \n) and \ninstall gcloud\n on your machine. In each command below, be sure to insert your project name in place of \n<project-id>\n.\n\n\nAfter installation, log into your account with \ngcloud auth login\n and enter your project ID when prompted.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nCreate 3 instances from the image above using our Ignition from \nexample.ign\n:\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-stable --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-beta --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-alpha --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n      and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \ngcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-edge --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign\n\n    \n\n  \n\n\n\n\n\nUploading an Image\n\u00b6\n\n\nOfficial Flatcar Container Linux images are not available on Google Cloud at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.\n\n\nTo do so, run the following command:\n\ndocker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload \\\n  --bucket-name <bucket name> \\\n  --project-id <project id>\n\n\nWhere:\n\n\n\n\n<bucket name>\n should be a valid \nbucket\n name.\n\n\n<project id>\n should be your project ID.\n\n\n\n\nDuring execution, the script will ask you to log into your Google account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to the Google Cloud.\n\n\nTo see all available options, run:\n\ndocker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -b, --bucket-name Name of GCP bucket for storing images.\n  -p, --project-id  ID of the project for creating bucket.\n\n Optional arguments:\n  -c, --channel     Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version     Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name  Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n\n Optional flags:\n   -f, --force-reupload If used, image will be uploaded even if it already exist in the bucket.\n   -F, --force-recreate If user, if compute image already exist, it will be removed and recreated.\n\n\nThe Dockerfile for the \nquay.io/kinvolk/google-cloud-flatcar-image-upload\n image is managed \nhere\n.\n\n\nUpgrade from CoreOS Container Linux\n\u00b6\n\n\nYou can also \nupgrade from an existing CoreOS Container Linux system\n.\n\n\nContainer Linux Config\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n.\n\n\nYou can provide a raw Ignition config to Flatcar Container Linux via the Google Cloud console's metadata field \nuser-data\n or via a flag using \ngcloud\n.\n\n\nAs an example, this config will configure and start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nAdditional storage\n\u00b6\n\n\nAdditional disks attached to instances can be mounted with a \n.mount\n unit. Each disk can be accessed via \n/dev/disk/by-id/google-<disk-name>\n. Here's the Container Linux Config to format and mount a disk called \ndatabase-backup\n:\n\n\nstorage:\n  filesystems:\n    - mount:\n        device: /dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        format: ext4\n\nsystemd:\n  units:\n    - name: media-backup.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        Where=/media/backup\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target\n\n\n\nFor more information about mounting storage, Google's \nown documentation\n is the best source. You can also read about \nmounting storage on Flatcar Container Linux\n.\n\n\nAdding more machines\n\u00b6\n\n\nTo add more instances to the cluster, just launch more with the same Ignition config inside of the project.\n\n\nSSH and users\n\u00b6\n\n\nUsers are added to Container Linux on GCE by the user provided configuration (i.e. Ignition, cloudinit) and by either the GCE account manager or \nGCE OS Login\n. OS Login is used if it is enabled for the instance, otherwise the GCE account manager is used.\n\n\nUsing the GCE account manager\n\u00b6\n\n\nYou can log in your Flatcar Container Linux instances using:\n\n\ngcloud compute ssh --zone us-central1-a core@<instance-name>\n\n\n\nUsers other than \ncore\n, which are set up by the GCE account manager, may not be a member of required groups. If you have issues, try running commands such as \njournalctl\n with sudo.\n\n\nUsing OS Login\n\u00b6\n\n\nYou can log in using your Google account on instances with OS Login enabled. OS Login needs to be \nenabled in the GCE console\n and on the instance. It is enabled by default on instances provisioned with Container Linux 1898.0.0 or later. Once enabled, you can log into your Container Linux instances using:\n\n\ngcloud compute ssh --zone us-central1-a <instance-name>\n\n\n\nThis will use your GCE user to log in.\n\n\nDisabling OS Login on newly provisioned nodes\n\u00b6\n\n\nYou can disable the OS Login functionality by masking the \noem-gce-enable-oslogin.service\n unit:\n\n\nsystemd:\n  units:\n    - name: oem-gce-enable-oslogin.service\n      mask: true\n\n\n\nWhen disabling OS Login functionality on the instance, it is also recommended to disable it in the GCE console.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on Google Compute Engine"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#running-flatcar-container-linux-on-google-compute-engine",
            "text": "Before proceeding, you will need a GCE account ( GCE free trial  ) and  install gcloud  on your machine. In each command below, be sure to insert your project name in place of  <project-id> .  After installation, log into your account with  gcloud auth login  and enter your project ID when prompted.",
            "title": "Running Flatcar Container Linux on Google Compute Engine"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  Create 3 instances from the image above using our Ignition from  example.ign :  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-stable --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-beta --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign \n     \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-alpha --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd\n      and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       gcloud compute instances create flatcar1 flatcar2 flatcar3 --image-project flatcar-cloud --image-family flatcar-edge --zone us-central1-a --machine-type n1-standard-1 --metadata-from-file user-data=config.ign",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#uploading-an-image",
            "text": "Official Flatcar Container Linux images are not available on Google Cloud at the moment. However, you can run Flatcar Container Linux today by uploading an image to your account.  To do so, run the following command: docker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload \\\n  --bucket-name <bucket name> \\\n  --project-id <project id>  Where:   <bucket name>  should be a valid  bucket  name.  <project id>  should be your project ID.   During execution, the script will ask you to log into your Google account and then create all necessary resources for\nuploading an image. It will then download the requested Flatcar Container Linux image and upload it to the Google Cloud.  To see all available options, run: docker run -it quay.io/kinvolk/google-cloud-flatcar-image-upload --help\n\nUsage: /usr/local/bin/upload_images.sh [OPTION...]\n\n Required arguments:\n  -b, --bucket-name Name of GCP bucket for storing images.\n  -p, --project-id  ID of the project for creating bucket.\n\n Optional arguments:\n  -c, --channel     Flatcar Container Linux release channel. Defaults to 'stable'.\n  -v, --version     Flatcar Container Linux version. Defaults to 'current'.\n  -i, --image-name  Image name, which will be used later in Lokomotive configuration. Defaults to 'flatcar-<channel>'.\n\n Optional flags:\n   -f, --force-reupload If used, image will be uploaded even if it already exist in the bucket.\n   -F, --force-recreate If user, if compute image already exist, it will be removed and recreated.  The Dockerfile for the  quay.io/kinvolk/google-cloud-flatcar-image-upload  image is managed  here .",
            "title": "Uploading an Image"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#upgrade-from-coreos-container-linux",
            "text": "You can also  upgrade from an existing CoreOS Container Linux system .",
            "title": "Upgrade from CoreOS Container Linux"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#container-linux-config",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features .  You can provide a raw Ignition config to Flatcar Container Linux via the Google Cloud console's metadata field  user-data  or via a flag using  gcloud .  As an example, this config will configure and start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls:       \"http://{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"http://{PRIVATE_IPV4}:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#additional-storage",
            "text": "Additional disks attached to instances can be mounted with a  .mount  unit. Each disk can be accessed via  /dev/disk/by-id/google-<disk-name> . Here's the Container Linux Config to format and mount a disk called  database-backup :  storage:\n  filesystems:\n    - mount:\n        device: /dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        format: ext4\n\nsystemd:\n  units:\n    - name: media-backup.mount\n      enable: true\n      contents: |\n        [Mount]\n        What=/dev/disk/by-id/scsi-0Google_PersistentDisk_database-backup\n        Where=/media/backup\n        Type=ext4\n\n        [Install]\n        RequiredBy=local-fs.target  For more information about mounting storage, Google's  own documentation  is the best source. You can also read about  mounting storage on Flatcar Container Linux .",
            "title": "Additional storage"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#adding-more-machines",
            "text": "To add more instances to the cluster, just launch more with the same Ignition config inside of the project.",
            "title": "Adding more machines"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#ssh-and-users",
            "text": "Users are added to Container Linux on GCE by the user provided configuration (i.e. Ignition, cloudinit) and by either the GCE account manager or  GCE OS Login . OS Login is used if it is enabled for the instance, otherwise the GCE account manager is used.",
            "title": "SSH and users"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#using-the-gce-account-manager",
            "text": "You can log in your Flatcar Container Linux instances using:  gcloud compute ssh --zone us-central1-a core@<instance-name>  Users other than  core , which are set up by the GCE account manager, may not be a member of required groups. If you have issues, try running commands such as  journalctl  with sudo.",
            "title": "Using the GCE account manager"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#using-os-login",
            "text": "You can log in using your Google account on instances with OS Login enabled. OS Login needs to be  enabled in the GCE console  and on the instance. It is enabled by default on instances provisioned with Container Linux 1898.0.0 or later. Once enabled, you can log into your Container Linux instances using:  gcloud compute ssh --zone us-central1-a <instance-name>  This will use your GCE user to log in.",
            "title": "Using OS Login"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#disabling-os-login-on-newly-provisioned-nodes",
            "text": "You can disable the OS Login functionality by masking the  oem-gce-enable-oslogin.service  unit:  systemd:\n  units:\n    - name: oem-gce-enable-oslogin.service\n      mask: true  When disabling OS Login functionality on the instance, it is also recommended to disable it in the GCE console.",
            "title": "Disabling OS Login on newly provisioned nodes"
        },
        {
            "location": "/os/booting-on-google-compute-engine/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-vagrant/",
            "text": "Running Flatcar Container Linux on Vagrant\n\u00b6\n\n\nRunning Flatcar Container Linux with Vagrant is one way to bring up a single machine or virtualize an entire cluster on your laptop. Since the true power of Flatcar Container Linux can be seen with a cluster, we're going to concentrate on that. Instructions for a single machine can be found \ntowards the end\n of the guide.\n\n\nYou can direct questions to the \nIRC channel\n or \nmailing list\n.\n\n\nInstall Vagrant and VirtualBox\n\u00b6\n\n\nVagrant is a simple-to-use command line virtual machine manager. There are install packages available for Windows, Linux and OS X. Find the latest installer on the \nVagrant downloads page\n. Be sure to get version 2.0.4 or greater, to be able to detect Flatcar images correctly.\n\n\nVagrant can use either the free VirtualBox provider or the commercial VMware provider. Instructions for both are below. For the VirtualBox provider, version 4.3.10 or greater is required.\n\n\nInstall Flatcar Container Linux\n\u00b6\n\n\nYou can import the flatcar box and boot it with Vagrant.\nYou'll find it in \nhttps://${CHANNEL}.release.flatcar-linux.net/amd64-usr/${VERSION}/flatcar_production_vagrant.box\n.\nMake sure you download the signature (it's available in \nhttps://${CHANNEL}.release.flatcar-linux.net/amd64-usr/${VERSION}/flatcar_production_vagrant.box.sig\n) and check it before proceeding.\n\n\nFor example, to get the latest alpha:\n\n\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vagrant.box\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vagrant.box.sig\n$ gpg --verify flatcar_production_vagrant.box.sig\ngpg: assuming signed data in 'flatcar_production_vagrant.box'\ngpg: Signature made Thu 15 Mar 2018 10:29:23 AM CET\ngpg:                using RSA key A621F1DA96C93C639506832D603443A1D0FC498C\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" [ultimate]\n$ vagrant box add flatcar flatcar_production_vagrant.box\n==> box: Box file was not detected as metadata. Adding it directly...\n==> box: Adding box 'flatcar' (v0) for provider:\n    box: Unpacking necessary files from: file:///tmp/flatcar_production_vagrant.box\n==> box: Successfully added box 'flatcar' (v0) for 'virtualbox'!\n$ vagrant init flatcar\nA `Vagrantfile` has been placed in this directory. You are now\nready to `vagrant up` your first virtual environment! Please read\nthe comments in the Vagrantfile as well as documentation on\n`vagrantup.com` for more information on using Vagrant.\n$ vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Importing base box 'flatcar'...\n==> default: Matching MAC address for NAT networking...\n==> default: Setting the name of the VM: vagrant_default_1520510346048_14823\n==> default: Clearing any previously set network interfaces...\n==> default: Preparing network interfaces based on configuration...\n    default: Adapter 1: nat\n==> default: Forwarding ports...\n    default: 22 (guest) => 2222 (host) (adapter 1)\n==> default: Running 'pre-boot' VM customizations...\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n    default: SSH address: 127.0.0.1:2222\n    default: SSH username: core\n    default: SSH auth method: private key\n==> default: Machine booted and ready!\n$ vagrant ssh\nLast login: Thu Mar 15 17:02:25 UTC 2018 from 10.0.2.2 on ssh\nFlatcar Container Linux by Kinvolk alpha (1702.1.0)\ncore@localhost ~ $\n\n\n\nStarting a cluster\n\u00b6\n\n\nYou can configure your Vagrant machine by having a \nVagrantfile\n example file:\n\n\nENV[\"TERM\"] = \"xterm-256color\"\nENV[\"LC_ALL\"] = \"en_US.UTF-8\"\n\nVagrant.require_version '>= 2.0.4'\n\nVagrant.configure('2') do |config|\n  config.ssh.username = 'core'\n  config.ssh.insert_key = true\n  config.vm.synced_folder '.', '/vagrant', disabled: true\n  config.vm.provider :virtualbox do |v|\n    v.check_guest_additions = false\n    v.functional_vboxsf = false\n    v.cpus = 2\n    v.memory = 2048\n  end\nend\n\n\n\nStart machines using Vagrant's default VirtualBox provider\n\u00b6\n\n\nStart the machine(s):\n\n\nvagrant up\n\n\n\nList the status of the running machines:\n\n\n$ vagrant status\nCurrent machine states:\n\ncore-01                   running (virtualbox)\ncore-02                   running (virtualbox)\ncore-03                   running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.\n\n\n\nConnect to one of the machines:\n\n\nvagrant ssh core-01 -- -A\n\n\n\nStart machines using Vagrant's VMware provider\n\u00b6\n\n\nIf you have purchased the \nVMware Vagrant provider\n, run the following commands:\n\n\nvagrant up --provider vmware_fusion\nvagrant ssh core-01 -- -A\n\n\n\nSingle machine\n\u00b6\n\n\nTo start a single machine, we need to provide some config parameters in cloud-config format via the \nuser-data\n file.\n\n\nStart the machine:\n\n\nvagrant up\n\n\n\nConnect to the machine:\n\n\nvagrant ssh core-01 -- -A\n\n\n\nStart machine using Vagrant's VMware provider\n\u00b6\n\n\nIf you have purchased the \nVMware Vagrant provider\n, run the following commands:\n\n\nvagrant up --provider vmware_fusion\nvagrant ssh core-01 -- -A\n\n\n\nShared folder setup\n\u00b6\n\n\nOptionally, you can share a folder from your laptop into the virtual machine. This is useful for easily getting code and Dockerfiles into Flatcar Container Linux.\n\n\nconfig.vm.synced_folder \".\", \"/home/core/share\", id: \"core\", :nfs => true,  :mount_options   => ['nolock,vers=3,udp']\n\n\n\nAfter a 'vagrant reload' you will be prompted for your local machine password.\n\n\nNew box versions\n\u00b6\n\n\nFlatcar Container Linux is a rolling release distribution and versions that are out of date will automatically update. If you want to start from the most up to date version you will need to make sure that you have the latest box file of Flatcar Container Linux. You can do this using \nvagrant box update\n - or, simply remove the old box file and Vagrant will download the latest one the next time you \nvagrant up\n.\n\n\nvagrant box remove flatcar-alpha vmware_fusion\nvagrant box remove flatcar-alpha virtualbox\n\n\n\nIf you'd like to download the box separately, you can download the URL contained in the Vagrantfile and add it manually:\n\n\nvagrant box add flatcar-alpha <path-to-box-file>\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide, learn about \nFlatcar Container Linux clustering with Vagrant\n, or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on Vagrant"
        },
        {
            "location": "/os/booting-on-vagrant/#running-flatcar-container-linux-on-vagrant",
            "text": "Running Flatcar Container Linux with Vagrant is one way to bring up a single machine or virtualize an entire cluster on your laptop. Since the true power of Flatcar Container Linux can be seen with a cluster, we're going to concentrate on that. Instructions for a single machine can be found  towards the end  of the guide.  You can direct questions to the  IRC channel  or  mailing list .",
            "title": "Running Flatcar Container Linux on Vagrant"
        },
        {
            "location": "/os/booting-on-vagrant/#install-vagrant-and-virtualbox",
            "text": "Vagrant is a simple-to-use command line virtual machine manager. There are install packages available for Windows, Linux and OS X. Find the latest installer on the  Vagrant downloads page . Be sure to get version 2.0.4 or greater, to be able to detect Flatcar images correctly.  Vagrant can use either the free VirtualBox provider or the commercial VMware provider. Instructions for both are below. For the VirtualBox provider, version 4.3.10 or greater is required.",
            "title": "Install Vagrant and VirtualBox"
        },
        {
            "location": "/os/booting-on-vagrant/#install-flatcar-container-linux",
            "text": "You can import the flatcar box and boot it with Vagrant.\nYou'll find it in  https://${CHANNEL}.release.flatcar-linux.net/amd64-usr/${VERSION}/flatcar_production_vagrant.box .\nMake sure you download the signature (it's available in  https://${CHANNEL}.release.flatcar-linux.net/amd64-usr/${VERSION}/flatcar_production_vagrant.box.sig ) and check it before proceeding.  For example, to get the latest alpha:  $ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vagrant.box\n$ wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vagrant.box.sig\n$ gpg --verify flatcar_production_vagrant.box.sig\ngpg: assuming signed data in 'flatcar_production_vagrant.box'\ngpg: Signature made Thu 15 Mar 2018 10:29:23 AM CET\ngpg:                using RSA key A621F1DA96C93C639506832D603443A1D0FC498C\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" [ultimate]\n$ vagrant box add flatcar flatcar_production_vagrant.box\n==> box: Box file was not detected as metadata. Adding it directly...\n==> box: Adding box 'flatcar' (v0) for provider:\n    box: Unpacking necessary files from: file:///tmp/flatcar_production_vagrant.box\n==> box: Successfully added box 'flatcar' (v0) for 'virtualbox'!\n$ vagrant init flatcar\nA `Vagrantfile` has been placed in this directory. You are now\nready to `vagrant up` your first virtual environment! Please read\nthe comments in the Vagrantfile as well as documentation on\n`vagrantup.com` for more information on using Vagrant.\n$ vagrant up\nBringing machine 'default' up with 'virtualbox' provider...\n==> default: Importing base box 'flatcar'...\n==> default: Matching MAC address for NAT networking...\n==> default: Setting the name of the VM: vagrant_default_1520510346048_14823\n==> default: Clearing any previously set network interfaces...\n==> default: Preparing network interfaces based on configuration...\n    default: Adapter 1: nat\n==> default: Forwarding ports...\n    default: 22 (guest) => 2222 (host) (adapter 1)\n==> default: Running 'pre-boot' VM customizations...\n==> default: Booting VM...\n==> default: Waiting for machine to boot. This may take a few minutes...\n    default: SSH address: 127.0.0.1:2222\n    default: SSH username: core\n    default: SSH auth method: private key\n==> default: Machine booted and ready!\n$ vagrant ssh\nLast login: Thu Mar 15 17:02:25 UTC 2018 from 10.0.2.2 on ssh\nFlatcar Container Linux by Kinvolk alpha (1702.1.0)\ncore@localhost ~ $",
            "title": "Install Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-vagrant/#starting-a-cluster",
            "text": "You can configure your Vagrant machine by having a  Vagrantfile  example file:  ENV[\"TERM\"] = \"xterm-256color\"\nENV[\"LC_ALL\"] = \"en_US.UTF-8\"\n\nVagrant.require_version '>= 2.0.4'\n\nVagrant.configure('2') do |config|\n  config.ssh.username = 'core'\n  config.ssh.insert_key = true\n  config.vm.synced_folder '.', '/vagrant', disabled: true\n  config.vm.provider :virtualbox do |v|\n    v.check_guest_additions = false\n    v.functional_vboxsf = false\n    v.cpus = 2\n    v.memory = 2048\n  end\nend",
            "title": "Starting a cluster"
        },
        {
            "location": "/os/booting-on-vagrant/#start-machines-using-vagrants-default-virtualbox-provider",
            "text": "Start the machine(s):  vagrant up  List the status of the running machines:  $ vagrant status\nCurrent machine states:\n\ncore-01                   running (virtualbox)\ncore-02                   running (virtualbox)\ncore-03                   running (virtualbox)\n\nThis environment represents multiple VMs. The VMs are all listed\nabove with their current state. For more information about a specific\nVM, run `vagrant status NAME`.  Connect to one of the machines:  vagrant ssh core-01 -- -A",
            "title": "Start machines using Vagrant's default VirtualBox provider"
        },
        {
            "location": "/os/booting-on-vagrant/#start-machines-using-vagrants-vmware-provider",
            "text": "If you have purchased the  VMware Vagrant provider , run the following commands:  vagrant up --provider vmware_fusion\nvagrant ssh core-01 -- -A",
            "title": "Start machines using Vagrant's VMware provider"
        },
        {
            "location": "/os/booting-on-vagrant/#single-machine",
            "text": "To start a single machine, we need to provide some config parameters in cloud-config format via the  user-data  file.  Start the machine:  vagrant up  Connect to the machine:  vagrant ssh core-01 -- -A",
            "title": "Single machine"
        },
        {
            "location": "/os/booting-on-vagrant/#start-machine-using-vagrants-vmware-provider",
            "text": "If you have purchased the  VMware Vagrant provider , run the following commands:  vagrant up --provider vmware_fusion\nvagrant ssh core-01 -- -A",
            "title": "Start machine using Vagrant's VMware provider"
        },
        {
            "location": "/os/booting-on-vagrant/#shared-folder-setup",
            "text": "Optionally, you can share a folder from your laptop into the virtual machine. This is useful for easily getting code and Dockerfiles into Flatcar Container Linux.  config.vm.synced_folder \".\", \"/home/core/share\", id: \"core\", :nfs => true,  :mount_options   => ['nolock,vers=3,udp']  After a 'vagrant reload' you will be prompted for your local machine password.",
            "title": "Shared folder setup"
        },
        {
            "location": "/os/booting-on-vagrant/#new-box-versions",
            "text": "Flatcar Container Linux is a rolling release distribution and versions that are out of date will automatically update. If you want to start from the most up to date version you will need to make sure that you have the latest box file of Flatcar Container Linux. You can do this using  vagrant box update  - or, simply remove the old box file and Vagrant will download the latest one the next time you  vagrant up .  vagrant box remove flatcar-alpha vmware_fusion\nvagrant box remove flatcar-alpha virtualbox  If you'd like to download the box separately, you can download the URL contained in the Vagrantfile and add it manually:  vagrant box add flatcar-alpha <path-to-box-file>",
            "title": "New box versions"
        },
        {
            "location": "/os/booting-on-vagrant/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide, learn about  Flatcar Container Linux clustering with Vagrant , or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-virtualbox/",
            "text": "Running Flatcar Container Linux on VirtualBox\n\u00b6\n\n\nThese instructions will walk you through running Flatcar Container Linux on Oracle VM VirtualBox.\n\n\nBuilding the virtual disk\n\u00b6\n\n\nThere is a script that simplifies building the VDI image. It downloads a bare-metal image, verifies it with GPG, and converts that image to a VDI image.\n\n\nThe script is located on \nGitHub\n. The running host must support VirtualBox tools.\n\n\nAs first step, you must download the script and make it executable.\n\n\nwget https://raw.githubusercontent.com/flatcar-linux/scripts/master/contrib/create-coreos-vdi\nchmod +x create-coreos-vdi\n\n\n\nTo run the script, you can specify a destination location and the Flatcar Container Linux version.\n\n\n./create-coreos-vdi -d /data/VirtualBox/Templates\n\n\n\nChoose a channel\n\u00b6\n\n\nChoose a channel to base your disk image on. Specific versions of Flatcar Container Linux can also be referenced by version number.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \nCreate a disk image from this channel by running:\n\n\n\n./create-coreos-vdi -V alpha\n\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \nCreate a disk image from this channel by running:\n\n\n\n./create-coreos-vdi -V beta\n\n\n    \n\n  \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \nCreate a disk image from this channel by running:\n\n\n\n./create-coreos-vdi -V stable\n\n\n    \n\n  \n\n\n\n\n\nAfter the script has finished successfully, the Flatcar Container Linux image will be available at the specified destination location or at the current location. The file name will be something like:\n\n\ncoreos_production_stable.vdi\n\n\n\nCreating a config-drive\n\u00b6\n\n\nCloud-config can be specified by attaching a \nconfig-drive\n with the label \nconfig-2\n. This is commonly done through whatever interface allows for attaching CD-ROMs or new drives.\n\n\nNote that the config-drive standard was originally an OpenStack feature, which is why you'll see strings containing \nopenstack\n. This filepath needs to be retained, although Flatcar Container Linux supports config-drive on all platforms.\n\n\nFor more information on customization that can be done with cloud-config, head on over to the \ncloud-config guide\n.\n\n\nYou need a config-drive to configure at least one SSH key to access the virtual machine. If you are in hurry, you can create a basic config-drive with following steps:\n\n\nwget https://raw.github.com/flatcar-linux/scripts/master/contrib/create-basic-configdrive\nchmod +x create-basic-configdrive\n./create-basic-configdrive -H my_vm01 -S ~/.ssh/id_rsa.pub\n\n\n\nAn ISO file named \nmy_vm01.iso\n will be created that will configure a virtual machine to accept your SSH key and set its name to my_vm01.\n\n\nDeploying a new virtual machine on VirtualBox\n\u00b6\n\n\nUse the built image as the base image. Clone that image for each new virtual machine and set the desired size.\n\n\nVBoxManage clonehd coreos_production_stable.vdi my_vm01.vdi\n# Resize virtual disk to 10 GB\nVBoxManage modifyhd my_vm01.vdi --resize 10240\n\n\n\nAt boot time, the Flatcar Container Linux will detect that the volume size has changed and will resize the filesystem accordingly.\n\n\nOpen VirtualBox Manager and go to Machine > New. Type the desired machine name and choose 'Linux' as the type and 'Linux 2.6 / 3.x (64 bit)' as the version.\n\n\nNext, choose the desired memory size; at least 1 GB for an optimal experience.\n\n\nThen, choose 'Use an existing virtual hard drive file' and find the new cloned image.\n\n\nClick on the 'Create' button to create the virtual machine.\n\n\nNext, go to the settings from the created virtual machine. Then click on the Storage tab and load the created config-drive into the CD/DVD drive.\n\n\nClick on the 'OK' button and the virtual machine will be ready to be started.\n\n\nLogging in\n\u00b6\n\n\nNetworking can take a bit of time to come up under VirtualBox, and the IP is needed in order to connect to it. Press enter a few times at the login prompt to see an IP address pop up. If you see VirtualBox NAT IP 10.0.2.15, go to the virtual machine settings and click the Network tab then Port Forwarding. Add the rule \"Host Port: 2222; Guest Port 22\" then connect using the command \nssh core@localhost -p2222\n.\n\n\nNow, login using your private SSH key.\n\n\nssh core@192.168.56.101\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that the machine has booted, it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on VirtualBox"
        },
        {
            "location": "/os/booting-on-virtualbox/#running-flatcar-container-linux-on-virtualbox",
            "text": "These instructions will walk you through running Flatcar Container Linux on Oracle VM VirtualBox.",
            "title": "Running Flatcar Container Linux on VirtualBox"
        },
        {
            "location": "/os/booting-on-virtualbox/#building-the-virtual-disk",
            "text": "There is a script that simplifies building the VDI image. It downloads a bare-metal image, verifies it with GPG, and converts that image to a VDI image.  The script is located on  GitHub . The running host must support VirtualBox tools.  As first step, you must download the script and make it executable.  wget https://raw.githubusercontent.com/flatcar-linux/scripts/master/contrib/create-coreos-vdi\nchmod +x create-coreos-vdi  To run the script, you can specify a destination location and the Flatcar Container Linux version.  ./create-coreos-vdi -d /data/VirtualBox/Templates",
            "title": "Building the virtual disk"
        },
        {
            "location": "/os/booting-on-virtualbox/#choose-a-channel",
            "text": "Choose a channel to base your disk image on. Specific versions of Flatcar Container Linux can also be referenced by version number.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       Create a disk image from this channel by running:  \n./create-coreos-vdi -V alpha \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       Create a disk image from this channel by running:  \n./create-coreos-vdi -V beta \n     \n   \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       Create a disk image from this channel by running:  \n./create-coreos-vdi -V stable \n     \n     After the script has finished successfully, the Flatcar Container Linux image will be available at the specified destination location or at the current location. The file name will be something like:  coreos_production_stable.vdi",
            "title": "Choose a channel"
        },
        {
            "location": "/os/booting-on-virtualbox/#creating-a-config-drive",
            "text": "Cloud-config can be specified by attaching a  config-drive  with the label  config-2 . This is commonly done through whatever interface allows for attaching CD-ROMs or new drives.  Note that the config-drive standard was originally an OpenStack feature, which is why you'll see strings containing  openstack . This filepath needs to be retained, although Flatcar Container Linux supports config-drive on all platforms.  For more information on customization that can be done with cloud-config, head on over to the  cloud-config guide .  You need a config-drive to configure at least one SSH key to access the virtual machine. If you are in hurry, you can create a basic config-drive with following steps:  wget https://raw.github.com/flatcar-linux/scripts/master/contrib/create-basic-configdrive\nchmod +x create-basic-configdrive\n./create-basic-configdrive -H my_vm01 -S ~/.ssh/id_rsa.pub  An ISO file named  my_vm01.iso  will be created that will configure a virtual machine to accept your SSH key and set its name to my_vm01.",
            "title": "Creating a config-drive"
        },
        {
            "location": "/os/booting-on-virtualbox/#deploying-a-new-virtual-machine-on-virtualbox",
            "text": "Use the built image as the base image. Clone that image for each new virtual machine and set the desired size.  VBoxManage clonehd coreos_production_stable.vdi my_vm01.vdi\n# Resize virtual disk to 10 GB\nVBoxManage modifyhd my_vm01.vdi --resize 10240  At boot time, the Flatcar Container Linux will detect that the volume size has changed and will resize the filesystem accordingly.  Open VirtualBox Manager and go to Machine > New. Type the desired machine name and choose 'Linux' as the type and 'Linux 2.6 / 3.x (64 bit)' as the version.  Next, choose the desired memory size; at least 1 GB for an optimal experience.  Then, choose 'Use an existing virtual hard drive file' and find the new cloned image.  Click on the 'Create' button to create the virtual machine.  Next, go to the settings from the created virtual machine. Then click on the Storage tab and load the created config-drive into the CD/DVD drive.  Click on the 'OK' button and the virtual machine will be ready to be started.",
            "title": "Deploying a new virtual machine on VirtualBox"
        },
        {
            "location": "/os/booting-on-virtualbox/#logging-in",
            "text": "Networking can take a bit of time to come up under VirtualBox, and the IP is needed in order to connect to it. Press enter a few times at the login prompt to see an IP address pop up. If you see VirtualBox NAT IP 10.0.2.15, go to the virtual machine settings and click the Network tab then Port Forwarding. Add the rule \"Host Port: 2222; Guest Port 22\" then connect using the command  ssh core@localhost -p2222 .  Now, login using your private SSH key.  ssh core@192.168.56.101",
            "title": "Logging in"
        },
        {
            "location": "/os/booting-on-virtualbox/#using-flatcar-container-linux",
            "text": "Now that the machine has booted, it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-on-vmware/",
            "text": "Running Flatcar Container Linux on VMware\n\u00b6\n\n\nThese instructions walk through running Flatcar Container Linux on VMware Fusion or ESXi. If you are familiar with another VMware product, you can use these instructions as a starting point.\n\n\nRunning the VM\n\u00b6\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n       \n\n      \ncurl -LO https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vmware_ova.ova\n\n    \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \n\n      \ncurl -LO https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vmware_ova.ova\n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \n\n      \ncurl -LO https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vmware_ova.ova\n\n    \n\n  \n\n\n\n\n\nBooting with VMware ESXi\n\u00b6\n\n\nUse the vSphere Client to deploy the VM as follows:\n\n\n\n\nIn the menu, click \nFile\n > \nDeploy OVF Template...\n\n\nIn the wizard, specify the location of the OVA file downloaded earlier\n\n\nName your VM\n\n\nChoose \"thin provision\" for the disk format\n\n\nChoose your network settings\n\n\nConfirm the settings, then click \"Finish\"\n\n\n\n\nUncheck \nPower on after deployment\n in order to edit the VM before booting it the first time.\n\n\nThe last step uploads the files to the ESXi datastore and registers the new VM. You can now tweak VM settings, then power it on.\n\n\nNB: These instructions were tested with an ESXi v5.5 host.\n\n\nBooting with VMware Workstation 12 or VMware Fusion\n\u00b6\n\n\nRun VMware Workstation GUI:\n\n\n\n\nIn the menu, click \nFile\n > \nOpen...\n\n\nIn the wizard, specify the location of the OVA template downloaded earlier\n\n\nName your VM, then click \nImport\n\n\n(Press \nRetry\n \nif\n VMware Workstation raises an \"OVF specification\" warning)\n\n\nEdit VM settings if necessary\n\n\nModify the \n.vmx\n file\n to pass an Ignition config containing at least one valid SSH key\n\n\nStart your Flatcar Container Linux VM\n\n\n\n\nNB: These instructions were tested with a Fusion 8.1 host.\n\n\nInstalling via PXE or ISO image\n\u00b6\n\n\nFlatcar Container Linux can also be installed by booting the virtual machine via \nPXE\n or the \nISO image\n and then \ninstalling Flatcar Container Linux to disk\n.\n\n\nContainer Linux Configs\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n.\n\n\nYou can provide a raw Ignition config to Flatcar Container Linux via VMware's \nGuestinfo interface\n.\n\n\nAs an example, this config will start etcd:\n\n\netcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # vmware isn't currently supported for dynamic data, so we can't use {PRIVATE_IPV4}\n  advertise_client_urls:       \"http://10.0.0.10:2379\"\n  initial_advertise_peer_urls: \"http://10.0.0.10:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://10.0.0.10:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"\n\n\n\nVMware Guestinfo interface\n\u00b6\n\n\nSetting Guestinfo options\n\u00b6\n\n\nThe VMware guestinfo interface is a mechanism for VM configuration. Guestinfo properties are stored in the VMX file, or in the VMX representation in host memory. These properties are available to the VM at boot time. Within the VMX, the names of these properties are prefixed with \nguestinfo.\n. Guestinfo settings can be injected into VMs in one of four ways:\n\n\n\n\n\n\nConfigure guestinfo in the OVF for deployment. Software like \nvcloud director\n manipulates OVF descriptors for guest configuration. For details, check out this VMware blog post about \nSelf-Configuration and the OVF Environment\n.\n\n\n\n\n\n\nSet guestinfo keys and values from the Flatcar Container Linux guest itself, by using a VMware Tools command like:\n\n\n\n\n\n\n/usr/share/oem/bin/vmtoolsd --cmd \"info-set guestinfo.<variable> <value>\"\n\n\n\n\n\nGuestinfo keys and values can be set from a VMware Service Console, using the \nsetguestinfo\n subcommand:\n\n\n\n\nvmware-cmd /vmfs/volumes/[...]/<VMNAME>/<VMNAME>.vmx setguestinfo guestinfo.<property> <value>\n\n\n\n\n\nYou can manually modify the VMX and reload it on the VMware Workstation, ESXi host, or in vCenter.\n\n\n\n\nGuestinfo configuration set via the VMware API or with \nvmtoolsd\n from within the Flatcar Container Linux guest itself are stored in VM process memory and are lost on VM shutdown or reboot.\n\n\nDefining the Ignition config in Guestinfo\n\u00b6\n\n\nIf the \nguestinfo.ignition.config.data\n property is set, Ignition will apply the referenced config on first boot.\n\n\nThe Ignition config is prepared for the guestinfo facility in one of two encoding types, specified in the \nguestinfo.ignition.config.data.encoding\n variable:\n\n\n\n\n\n\n\n\nEncoding\n\n\nCommand\n\n\n\n\n\n\n\n\n\n\n<elided>\n\n\nsed -e 's/%/%%/g' -e 's/\"/%22/g' /path/to/user_data\n\n\n\n\n\n\nbase64\n\n\nbase64 -w0 /path/to/user_data && echo\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nguestinfo.ignition.config.data = \"ewogICJpZ25pdGlvbiI6IHsgInZlcnNpb24iOiAiMi4wLjAiIH0KfQo=\"\nguestinfo.ignition.config.data.encoding = \"base64\"\n\n\n\nThis example will be decoded into:\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" }\n}\n\n\n\nLogging in\n\u00b6\n\n\nNetworking can take some time to start under VMware. Once it does, press enter a few times at the login prompt and you should see an IP address printed on the console:\n\n\n\n\nIn this case the IP is \n10.0.1.81\n.\n\n\nNow you can login to the host at that IP using your SSH key, or the password set in your cloud-config:\n\n\nssh core@10.0.1.81\n\n\n\nAlternatively, appending \nflatcar.autologin\n to the kernel parameters at boot causes the console to accept the \ncore\n user's login with no password. This is handy for debugging.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted, it's time to explore. Check out the \nFlatcar Container Linux Quickstart\n guide, or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on VMware"
        },
        {
            "location": "/os/booting-on-vmware/#running-flatcar-container-linux-on-vmware",
            "text": "These instructions walk through running Flatcar Container Linux on VMware Fusion or ESXi. If you are familiar with another VMware product, you can use these instructions as a starting point.",
            "title": "Running Flatcar Container Linux on VMware"
        },
        {
            "location": "/os/booting-on-vmware/#running-the-vm",
            "text": "",
            "title": "Running the VM"
        },
        {
            "location": "/os/booting-on-vmware/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n   \n   \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n        \n       curl -LO https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vmware_ova.ova \n     \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       \n       curl -LO https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vmware_ova.ova \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       \n       curl -LO https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_vmware_ova.ova",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-on-vmware/#booting-with-vmware-esxi",
            "text": "Use the vSphere Client to deploy the VM as follows:   In the menu, click  File  >  Deploy OVF Template...  In the wizard, specify the location of the OVA file downloaded earlier  Name your VM  Choose \"thin provision\" for the disk format  Choose your network settings  Confirm the settings, then click \"Finish\"   Uncheck  Power on after deployment  in order to edit the VM before booting it the first time.  The last step uploads the files to the ESXi datastore and registers the new VM. You can now tweak VM settings, then power it on.  NB: These instructions were tested with an ESXi v5.5 host.",
            "title": "Booting with VMware ESXi"
        },
        {
            "location": "/os/booting-on-vmware/#booting-with-vmware-workstation-12-or-vmware-fusion",
            "text": "Run VMware Workstation GUI:   In the menu, click  File  >  Open...  In the wizard, specify the location of the OVA template downloaded earlier  Name your VM, then click  Import  (Press  Retry   if  VMware Workstation raises an \"OVF specification\" warning)  Edit VM settings if necessary  Modify the  .vmx  file  to pass an Ignition config containing at least one valid SSH key  Start your Flatcar Container Linux VM   NB: These instructions were tested with a Fusion 8.1 host.",
            "title": "Booting with VMware Workstation 12 or VMware Fusion"
        },
        {
            "location": "/os/booting-on-vmware/#installing-via-pxe-or-iso-image",
            "text": "Flatcar Container Linux can also be installed by booting the virtual machine via  PXE  or the  ISO image  and then  installing Flatcar Container Linux to disk .",
            "title": "Installing via PXE or ISO image"
        },
        {
            "location": "/os/booting-on-vmware/#container-linux-configs",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features .  You can provide a raw Ignition config to Flatcar Container Linux via VMware's  Guestinfo interface .  As an example, this config will start etcd:  etcd:\n  # All options get passed as command line flags to etcd.\n  # Any information inside curly braces comes from the machine at boot time.\n\n  # vmware isn't currently supported for dynamic data, so we can't use {PRIVATE_IPV4}\n  advertise_client_urls:       \"http://10.0.0.10:2379\"\n  initial_advertise_peer_urls: \"http://10.0.0.10:2380\"\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://10.0.0.10:2380\"\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery:                   \"https://discovery.etcd.io/<token>\"",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-on-vmware/#vmware-guestinfo-interface",
            "text": "",
            "title": "VMware Guestinfo interface"
        },
        {
            "location": "/os/booting-on-vmware/#setting-guestinfo-options",
            "text": "The VMware guestinfo interface is a mechanism for VM configuration. Guestinfo properties are stored in the VMX file, or in the VMX representation in host memory. These properties are available to the VM at boot time. Within the VMX, the names of these properties are prefixed with  guestinfo. . Guestinfo settings can be injected into VMs in one of four ways:    Configure guestinfo in the OVF for deployment. Software like  vcloud director  manipulates OVF descriptors for guest configuration. For details, check out this VMware blog post about  Self-Configuration and the OVF Environment .    Set guestinfo keys and values from the Flatcar Container Linux guest itself, by using a VMware Tools command like:    /usr/share/oem/bin/vmtoolsd --cmd \"info-set guestinfo.<variable> <value>\"   Guestinfo keys and values can be set from a VMware Service Console, using the  setguestinfo  subcommand:   vmware-cmd /vmfs/volumes/[...]/<VMNAME>/<VMNAME>.vmx setguestinfo guestinfo.<property> <value>   You can manually modify the VMX and reload it on the VMware Workstation, ESXi host, or in vCenter.   Guestinfo configuration set via the VMware API or with  vmtoolsd  from within the Flatcar Container Linux guest itself are stored in VM process memory and are lost on VM shutdown or reboot.",
            "title": "Setting Guestinfo options"
        },
        {
            "location": "/os/booting-on-vmware/#defining-the-ignition-config-in-guestinfo",
            "text": "If the  guestinfo.ignition.config.data  property is set, Ignition will apply the referenced config on first boot.  The Ignition config is prepared for the guestinfo facility in one of two encoding types, specified in the  guestinfo.ignition.config.data.encoding  variable:     Encoding  Command      <elided>  sed -e 's/%/%%/g' -e 's/\"/%22/g' /path/to/user_data    base64  base64 -w0 /path/to/user_data && echo",
            "title": "Defining the Ignition config in Guestinfo"
        },
        {
            "location": "/os/booting-on-vmware/#example",
            "text": "guestinfo.ignition.config.data = \"ewogICJpZ25pdGlvbiI6IHsgInZlcnNpb24iOiAiMi4wLjAiIH0KfQo=\"\nguestinfo.ignition.config.data.encoding = \"base64\"  This example will be decoded into:  {\n  \"ignition\": { \"version\": \"2.0.0\" }\n}",
            "title": "Example"
        },
        {
            "location": "/os/booting-on-vmware/#logging-in",
            "text": "Networking can take some time to start under VMware. Once it does, press enter a few times at the login prompt and you should see an IP address printed on the console:   In this case the IP is  10.0.1.81 .  Now you can login to the host at that IP using your SSH key, or the password set in your cloud-config:  ssh core@10.0.1.81  Alternatively, appending  flatcar.autologin  to the kernel parameters at boot causes the console to accept the  core  user's login with no password. This is handy for debugging.",
            "title": "Logging in"
        },
        {
            "location": "/os/booting-on-vmware/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted, it's time to explore. Check out the  Flatcar Container Linux Quickstart  guide, or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-ipxe/",
            "text": "Booting Flatcar Container Linux via iPXE\n\u00b6\n\n\nThese instructions will walk you through booting Flatcar Container Linux via iPXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be \ninstalled to disk\n.\n\n\nA minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.\n\n\nConfiguring iPXE\n\u00b6\n\n\niPXE can be used on any platform that can boot an ISO image.\nThis includes many cloud providers and physical hardware.\n\n\nTo illustrate iPXE in action we will use qemu-kvm in this guide.\n\n\nSetting up iPXE boot script\n\u00b6\n\n\nWhen configuring the Flatcar Container Linux iPXE boot script there are a few kernel options that may be useful but all are optional.\n\n\n\n\nrootfstype=tmpfs\n: Use tmpfs for the writable root filesystem. This is the default behavior.\n\n\nrootfstype=btrfs\n: Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.\n\n\nroot\n: Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g: \nroot=/dev/sda1\n, \nroot=LABEL=ROOT\n or \nroot=UUID=2c618316-d17a-4688-b43b-aa19d97ea821\n.\n\n\nsshkey\n: Add the given SSH public key to the \ncore\n user's authorized_keys file. Replace the example key below with your own (it is usually in \n~/.ssh/id_rsa.pub\n)\n\n\nconsole\n: Enable kernel output and a login prompt on a given tty. The default, \ntty0\n, generally maps to VGA. Can be used multiple times, e.g. \nconsole=tty0 console=ttyS0\n\n\nflatcar.autologin\n: Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the \nconsole\n option, e.g. \nconsole=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0\n. Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals (\ntty1\n, \ntty2\n, etc), not the VGA console itself (\ntty0\n).\n\n\nflatcar.first_boot=1\n: Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the \nconfig transpiler documentation\n for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.\n\n\nignition.config.url\n: Download the Ignition config from the specified URL. \nhttp\n, \nhttps\n, \ns3\n, and \ntftp\n schemes are supported.\n\n\nip\n: Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See \nIgnition documentation\n for the complete syntax.\n\n\n\n\nChoose a Channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nSetting up the Boot Script\n\u00b6\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://alpha.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://beta.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://edge.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \niPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a \ncustom iPXE server\n.\n\n      \n\n#!ipxe\n\nset base-url http://stable.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot\n\n    \n\n  \n\n\n\n\n\nAn easy place to host this boot script is on \nhttp://pastie.org\n. Be sure to reference the \"raw\" version of script, which is accessed by clicking on the clipboard in the top right.\n\n\nBooting iPXE\n\u00b6\n\n\nFirst, download and boot the iPXE image.\nWe will use \nqemu-kvm\n in this guide but use whatever process you normally use for booting an ISO on your platform.\n\n\nwget http://boot.ipxe.org/ipxe.iso\nqemu-kvm -m 1024 ipxe.iso --curses\n\n\n\nNext press Ctrl+B to get to the iPXE prompt and type in the following commands:\n\n\niPXE> dhcp\niPXE> chain http://${YOUR_BOOT_URL}\n\n\n\nImmediately iPXE should download your boot script URL and start grabbing the images from the Flatcar Container Linux storage site:\n\n\n${YOUR_BOOT_URL}... ok\nhttp://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz... 98%\n\n\n\nAfter a few moments of downloading Flatcar Container Linux should boot normally.\n\n\nUpdate process\n\u00b6\n\n\nSince Flatcar Container Linux's upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.\n\n\nInstallation\n\u00b6\n\n\nFlatcar Container Linux can be completely installed on disk or run from RAM but store user data on disk. Read more in our \nInstalling Flatcar Container Linux guide\n.\n\n\nAdding a custom OEM\n\u00b6\n\n\nSimilar to the \nOEM partition\n in Flatcar Container Linux disk images, iPXE images can be customized with an \nIgnition config\n bundled in the initramfs. You can view the \ninstructions on the PXE docs\n.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Booting Flatcar Container Linux via iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#booting-flatcar-container-linux-via-ipxe",
            "text": "These instructions will walk you through booting Flatcar Container Linux via iPXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be  installed to disk .  A minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.",
            "title": "Booting Flatcar Container Linux via iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#configuring-ipxe",
            "text": "iPXE can be used on any platform that can boot an ISO image.\nThis includes many cloud providers and physical hardware.  To illustrate iPXE in action we will use qemu-kvm in this guide.",
            "title": "Configuring iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#setting-up-ipxe-boot-script",
            "text": "When configuring the Flatcar Container Linux iPXE boot script there are a few kernel options that may be useful but all are optional.   rootfstype=tmpfs : Use tmpfs for the writable root filesystem. This is the default behavior.  rootfstype=btrfs : Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.  root : Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g:  root=/dev/sda1 ,  root=LABEL=ROOT  or  root=UUID=2c618316-d17a-4688-b43b-aa19d97ea821 .  sshkey : Add the given SSH public key to the  core  user's authorized_keys file. Replace the example key below with your own (it is usually in  ~/.ssh/id_rsa.pub )  console : Enable kernel output and a login prompt on a given tty. The default,  tty0 , generally maps to VGA. Can be used multiple times, e.g.  console=tty0 console=ttyS0  flatcar.autologin : Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the  console  option, e.g.  console=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0 . Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals ( tty1 ,  tty2 , etc), not the VGA console itself ( tty0 ).  flatcar.first_boot=1 : Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the  config transpiler documentation  for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.  ignition.config.url : Download the Ignition config from the specified URL.  http ,  https ,  s3 , and  tftp  schemes are supported.  ip : Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See  Ignition documentation  for the complete syntax.",
            "title": "Setting up iPXE boot script"
        },
        {
            "location": "/os/booting-with-ipxe/#choose-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.",
            "title": "Choose a Channel"
        },
        {
            "location": "/os/booting-with-ipxe/#setting-up-the-boot-script",
            "text": "Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://alpha.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://beta.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://edge.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       iPXE downloads a boot script from a publicly available URL. You will need to host this URL somewhere public and replace the example SSH key with your own. You can also run a  custom iPXE server . \n       \n#!ipxe\n\nset base-url http://stable.release.flatcar-linux.net/amd64-usr/current\nkernel ${base-url}/flatcar_production_pxe.vmlinuz initrd=flatcar_production_pxe_image.cpio.gz flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\ninitrd ${base-url}/flatcar_production_pxe_image.cpio.gz\nboot \n     \n     An easy place to host this boot script is on  http://pastie.org . Be sure to reference the \"raw\" version of script, which is accessed by clicking on the clipboard in the top right.",
            "title": "Setting up the Boot Script"
        },
        {
            "location": "/os/booting-with-ipxe/#booting-ipxe",
            "text": "First, download and boot the iPXE image.\nWe will use  qemu-kvm  in this guide but use whatever process you normally use for booting an ISO on your platform.  wget http://boot.ipxe.org/ipxe.iso\nqemu-kvm -m 1024 ipxe.iso --curses  Next press Ctrl+B to get to the iPXE prompt and type in the following commands:  iPXE> dhcp\niPXE> chain http://${YOUR_BOOT_URL}  Immediately iPXE should download your boot script URL and start grabbing the images from the Flatcar Container Linux storage site:  ${YOUR_BOOT_URL}... ok\nhttp://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz... 98%  After a few moments of downloading Flatcar Container Linux should boot normally.",
            "title": "Booting iPXE"
        },
        {
            "location": "/os/booting-with-ipxe/#update-process",
            "text": "Since Flatcar Container Linux's upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.",
            "title": "Update process"
        },
        {
            "location": "/os/booting-with-ipxe/#installation",
            "text": "Flatcar Container Linux can be completely installed on disk or run from RAM but store user data on disk. Read more in our  Installing Flatcar Container Linux guide .",
            "title": "Installation"
        },
        {
            "location": "/os/booting-with-ipxe/#adding-a-custom-oem",
            "text": "Similar to the  OEM partition  in Flatcar Container Linux disk images, iPXE images can be customized with an  Ignition config  bundled in the initramfs. You can view the  instructions on the PXE docs .",
            "title": "Adding a custom OEM"
        },
        {
            "location": "/os/booting-with-ipxe/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-iso/",
            "text": "Booting Flatcar Container Linux from an ISO\n\u00b6\n\n\nThe latest Flatcar Container Linux ISOs can be downloaded from the image storage site:\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \n\n      \nDownload Alpha ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \n\n      \nDownload Beta ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \n\n      \nDownload Edge ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \n\n      \nDownload Stable ISO\n\n      \nBrowse Storage Site\n\n      \n\n      \nAll of the files necessary to verify the image can be found on the storage site.\n\n    \n\n  \n\n\n\n\n\nKnown limitations\n\u00b6\n\n\n\n\nUEFI boot is not currently supported. Boot the system in BIOS compatibility mode.\n\n\nThere is no straightforward way to provide an \nIgnition config\n.\n\n\nA minimum of 2 GB of RAM is required to boot Flatcar Container Linux via ISO.\n\n\n\n\nInstall to disk\n\u00b6\n\n\nThe most common use-case for this ISO is to install Flatcar Container Linux to disk. You can \nfind those instructions here\n.\n\n\nNo authentication on console\n\u00b6\n\n\nThe ISO is configured to start a shell on the console without prompting for a password. This is convenient for installation and troubleshooting, but use caution.",
            "title": "Booting Flatcar Container Linux from an ISO"
        },
        {
            "location": "/os/booting-with-iso/#booting-flatcar-container-linux-from-an-iso",
            "text": "The latest Flatcar Container Linux ISOs can be downloaded from the image storage site:  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       \n       Download Alpha ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site. \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       \n       Download Beta ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site. \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       \n       Download Edge ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site. \n     \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       \n       Download Stable ISO \n       Browse Storage Site \n       \n       All of the files necessary to verify the image can be found on the storage site.",
            "title": "Booting Flatcar Container Linux from an ISO"
        },
        {
            "location": "/os/booting-with-iso/#known-limitations",
            "text": "UEFI boot is not currently supported. Boot the system in BIOS compatibility mode.  There is no straightforward way to provide an  Ignition config .  A minimum of 2 GB of RAM is required to boot Flatcar Container Linux via ISO.",
            "title": "Known limitations"
        },
        {
            "location": "/os/booting-with-iso/#install-to-disk",
            "text": "The most common use-case for this ISO is to install Flatcar Container Linux to disk. You can  find those instructions here .",
            "title": "Install to disk"
        },
        {
            "location": "/os/booting-with-iso/#no-authentication-on-console",
            "text": "The ISO is configured to start a shell on the console without prompting for a password. This is convenient for installation and troubleshooting, but use caution.",
            "title": "No authentication on console"
        },
        {
            "location": "/os/booting-with-libvirt/",
            "text": "Running Flatcar Container Linux on libvirt\n\u00b6\n\n\nThis guide explains how to run Flatcar Container Linux with libvirt using the QEMU driver. The libvirt configuration\nfile can be used (for example) with \nvirsh\n or \nvirt-manager\n. The guide assumes\nthat you already have a running libvirt setup and \nvirt-install\n tool. If you\ndon\u2019t have that, other solutions are most likely easier.\n\n\nYou can direct questions to the \nIRC channel\n or \nmailing list\n.\n\n\nDownload the Flatcar Container Linux image\n\u00b6\n\n\nIn this guide, the example virtual machine we are creating is called flatcar-linux1 and\nall files are stored in \n/var/lib/libvirt/images/flatcar-linux\n. This is not a requirement \u2014 feel free\nto substitute that path if you use another one.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \nWe start by downloading the most recent disk image:\n\n      \n\nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \nWe start by downloading the most recent disk image:\n\n      \n\nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \nWe start by downloading the most recent disk image:\n\n      \n\nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2\n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \nWe start by downloading the most recent disk image:\n\n      \n\nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2\n\n    \n\n  \n\n\n\n\n\nVirtual machine configuration\n\u00b6\n\n\nNow create a qcow2 image snapshot using the command below:\n\n\ncd /var/lib/libvirt/images/flatcar-linux\nqemu-img create -f qcow2 -b flatcar_production_qemu_image.img flatcar-linux1.qcow2\n\n\n\nThis will create a \nflatcar-linux1.qcow2\n snapshot image. Any changes to \nflatcar-linux1.qcow2\n will not be reflected in \nflatcar_production_qemu_image.img\n. Making any changes to a base image (\nflatcar_production_qemu_image.img\n in our example) will corrupt its snapshots.\n\n\nIgnition config\n\u00b6\n\n\nThe preferred way to configure a Flatcar Container Linux machine is via Ignition.\nUnfortunately, libvirt does not have direct support for Ignition yet, so configuring it involves including qemu-specific xml.\n\n\nThis configuration can be done in the following steps:\n\n\nCreate the Ignition config\n\u00b6\n\n\nTypically you won't write Ignition files yourself, rather you will typically use a tool like the \nconfig transpiler\n to generate them.\n\n\nHowever the Ignition file is created, it should be placed in a location which qemu can access. In this example, we'll place it in \n/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n.\n\n\nmkdir -p /var/lib/libvirt/flatcar-linux/flatcar-linux1/\necho '{\"ignition\":{\"version\":\"2.0.0\"}}' > /var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\n\n\nIf the host uses SELinux, allow the VM access to the config:\n\n\nsemanage fcontext -a -t virt_content_t \"/var/lib/libvirt/flatcar-linux/flatcar-linux1\"\nrestorecon -R \"/var/lib/libvirt/flatcar-linux/flatcar-linux1\"\n\n\n\nA simple Flatcar Container Linux config to add your ssh keys might look like the following:\n\n\nstorage:\n  files:\n  - path: /etc/hostname\n    filesystem: \"root\"\n    contents:\n      inline: \"flatcar-linux1\"\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"\n\n\n\nCreating the domain xml\n\u00b6\n\n\nOnce the Ignition file exists on disk, the machine can be configured to use it.\n\n\nStart by creating a libvirt \ndomain XML\n document:\n\n\nvirt-install --connect qemu:///system \\\n             --import \\\n             --name flatcar-linux1 \\\n             --ram 1024 --vcpus 1 \\\n             --os-type=linux \\\n             --os-variant=virtio26 \\\n             --disk path=/var/lib/libvirt/images/flatcar-linux/flatcar-linux1.qcow2,format=qcow2,bus=virtio \\\n             --vnc --noautoconsole \\\n             --print-xml > /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\n\n\n\nNext, modify the domain xml to reference the qemu-specific configuration needed:\n\n\n<?xml version=\"1.0\"?>\n<domain xmlns:qemu=\"http://libvirt.org/schemas/domain/qemu/1.0\" type=\"kvm\">\n  ...\n  <qemu:commandline>\n    <qemu:arg value=\"-fw_cfg\"/>\n    <qemu:arg value=\"name=opt/org.flatcar-linux/config,file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\"/>\n  </qemu:commandline>\n</domain>\n\n\n\nIf you have the \nxmlstarlet\n utility installed, the above modification can be accomplished easily with the following:\n\n\ndomain=/var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nignition_file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\nxmlstarlet ed -P -L -i \"//domain\" -t attr -n \"xmlns:qemu\" --value \"http://libvirt.org/schemas/domain/qemu/1.0\" \"${domain}\"\nxmlstarlet ed -P -L -s \"//domain\" -t elem -n \"qemu:commandline\" \"${domain}\"\nxmlstarlet ed -P -L -s \"//domain/qemu:commandline\" -t elem -n \"qemu:arg\" \"${domain}\"\nxmlstarlet ed -P -L -s \"(//domain/qemu:commandline/qemu:arg)[1]\" -t attr -n \"value\" -v \"-fw_cfg\" \"${domain}\"\nxmlstarlet ed -P -L -s \"//domain/qemu:commandline\" -t elem -n \"qemu:arg\" \"${domain}\"\nxmlstarlet ed -P -L -s \"(//domain/qemu:commandline/qemu:arg)[2]\" -t attr -n \"value\" -v \"name=opt/org.flatcar-linux/config,file=${ignition_file}\" \"${domain}\"\n\n\n\nAlternately, you can accomplish the same modification using sed:\n\n\ndomain=/var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nignition_file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\nsed -i 's|type=\"kvm\"|type=\"kvm\" xmlns:qemu=\"http://libvirt.org/schemas/domain/qemu/1.0\"|' \"${domain}\"\nsed -i \"/<\\/devices>/a <qemu:commandline>\\n  <qemu:arg value='-fw_cfg'/>\\n  <qemu:arg value='name=opt/org.flatcar-linux/config,file=${ignition_file}'/>\\n</qemu:commandline>\" \"${domain}\"\n\n\n\nDefine and start the machine\n\u00b6\n\n\nOnce the XML domain has been edited to include the Ignition file, it can be created and started using the \nvirsh\n tool included with libvirt:\n\n\nvirsh define /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nvirsh start flatcar-linux1 \n\n\n\nSSH into the machine\n\u00b6\n\n\nBy default, libvirt runs its own DHCP server which will provide an IP address to new instances. You can query it for what IP addresses have been assigned to machines:\n\n\n$ virsh net-dhcp-leases default\nExpiry Time          MAC address        Protocol  IP address                Hostname        Client ID or DUID\n-------------------------------------------------------------------------------------------------------------------\n 2017-08-09 16:32:52  52:54:00:13:12:45  ipv4      192.168.122.184/24        flatcar-linux1 ff:32:39:f9:b5:00:02:00:00:ab:11:06:6a:55:ed:5d:0a:73:ee\n\n\n\nNetwork configuration\n\u00b6\n\n\nStatic IP\n\u00b6\n\n\nBy default, Flatcar Container Linux uses DHCP to get its network configuration. In this example the VM will be attached directly to the local network via a bridge on the host's virbr0 and the local network. To configure a static address add a \nnetworkd unit\n to the Flatcar Container Linux config:\n\n\npasswd:\n  users:\n  - name: core\n    ssh_authorized_keys:\n    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\n\nstorage:\n  files:\n  - path: /etc/hostname\n    filesystem: \"root\"\n    contents: \n      inline: flatcar-linux1\n\nnetworkd:\n  units:\n  - name: 10-ens3.network\n    contents: |\n      [Match]\n      MACAddress=52:54:00:fe:b3:c0\n\n      [Network]\n      Address=192.168.122.2\n      Gateway=192.168.122.1\n      DNS=8.8.8.8\n\n\n\nUsing DHCP with a libvirt network\n\u00b6\n\n\nAn alternative to statically configuring an IP at the host level is to do so at the libvirt level. If you're using libvirt's built in DHCP server and a recent libvirt version, it allows configuring what IP address will be provided to a given machine ahead of time.\n\n\nThis can be done using the \nnet-update\n command. The following assumes you're using the \ndefault\n libvirt network and have configured the MAC Address to \n52:54:00:fe:b3:c0\n through the \n--network\n flag on \nvirt-install\n:\n\n\nip=\"192.168.122.2\"\nmac=\"52:54:00:fe:b3:c0\"\n\nvirsh net-update --network \"default\" add-last ip-dhcp-host \\\n    --xml \"<host mac='${mac}' ip='${ip}' />\" \\\n    --live --config\n\n\n\nBy executing these commands before running \nvirsh start\n, we can ensure the libvirt DHCP server will hand out a known IP.\n\n\nVirtual machine startup\n\u00b6\n\n\nNow, start this libvirt instance with the RAM, vCPU, and networking configuration defined above:\n\n\nignition_file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\ndomain=/var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nip=\"192.168.122.2\"\nmac=\"52:54:00:fe:b3:c0\"\n\nmkdir -p \"$(dirname \"${domain}\")\"\n\nvirsh net-update --network \"default\" add-last ip-dhcp-host \\\n    --xml \"<host mac='${mac}' ip='${ip}' />\" \\\n    --live --config\n\nvirt-install --connect qemu:///system --import \\\n  --name flatcar-linux1 \\\n  --ram 1024 --vcpus 1 \\\n  --os-type=linux \\\n  --os-variant=virtio26 \\\n  --disk path=/var/lib/libvirt/images/flatcar-linux/flatcar-linux1.qcow2,format=qcow2,bus=virtio \\\n  --network bridge=virbr0,mac=52:54:00:fe:b3:c0 \\\n  --vnc --noautoconsole \\\n  --print-xml > /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\n\nsed -ie 's|type=\"kvm\"|type=\"kvm\" xmlns:qemu=\"http://libvirt.org/schemas/domain/qemu/1.0\"|' \"${domain}\"\nsed -i \"/<\\/devices>/a <qemu:commandline>\\n  <qemu:arg value='-fw_cfg'/>\\n  <qemu:arg value='name=opt/org.flatcar-linux/config,file=${ignition_file}'/>\\n</qemu:commandline>\" \"${domain}\"\n\nvirsh define /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nvirsh start flatcar-linux1\n\n\n\nOnce the virtual machine has started you can log in via SSH:\n\n\nssh core@192.168.122.2\n\n\n\nSSH Config\n\u00b6\n\n\nTo simplify this and avoid potential host key errors in the future add the following to \n~/.ssh/config\n:\n\n\nHost flatcar-linux1\nHostName 192.168.122.2\nUser core\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/null\n\n\n\nNow you can log in to the virtual machine with:\n\n\nssh flatcar-linux1\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on libvirt"
        },
        {
            "location": "/os/booting-with-libvirt/#running-flatcar-container-linux-on-libvirt",
            "text": "This guide explains how to run Flatcar Container Linux with libvirt using the QEMU driver. The libvirt configuration\nfile can be used (for example) with  virsh  or  virt-manager . The guide assumes\nthat you already have a running libvirt setup and  virt-install  tool. If you\ndon\u2019t have that, other solutions are most likely easier.  You can direct questions to the  IRC channel  or  mailing list .",
            "title": "Running Flatcar Container Linux on libvirt"
        },
        {
            "location": "/os/booting-with-libvirt/#download-the-flatcar-container-linux-image",
            "text": "In this guide, the example virtual machine we are creating is called flatcar-linux1 and\nall files are stored in  /var/lib/libvirt/images/flatcar-linux . This is not a requirement \u2014 feel free\nto substitute that path if you use another one.",
            "title": "Download the Flatcar Container Linux image"
        },
        {
            "location": "/os/booting-with-libvirt/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       We start by downloading the most recent disk image: \n       \nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2 \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       We start by downloading the most recent disk image: \n       \nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2 \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       We start by downloading the most recent disk image: \n       \nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2 \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       We start by downloading the most recent disk image: \n       \nmkdir -p /var/lib/libvirt/images/flatcar-linux\ncd /var/lib/libvirt/images/flatcar-linux\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2{,.sig}\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbunzip2 flatcar_production_qemu_image.img.bz2",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-with-libvirt/#virtual-machine-configuration",
            "text": "Now create a qcow2 image snapshot using the command below:  cd /var/lib/libvirt/images/flatcar-linux\nqemu-img create -f qcow2 -b flatcar_production_qemu_image.img flatcar-linux1.qcow2  This will create a  flatcar-linux1.qcow2  snapshot image. Any changes to  flatcar-linux1.qcow2  will not be reflected in  flatcar_production_qemu_image.img . Making any changes to a base image ( flatcar_production_qemu_image.img  in our example) will corrupt its snapshots.",
            "title": "Virtual machine configuration"
        },
        {
            "location": "/os/booting-with-libvirt/#ignition-config",
            "text": "The preferred way to configure a Flatcar Container Linux machine is via Ignition.\nUnfortunately, libvirt does not have direct support for Ignition yet, so configuring it involves including qemu-specific xml.  This configuration can be done in the following steps:",
            "title": "Ignition config"
        },
        {
            "location": "/os/booting-with-libvirt/#create-the-ignition-config",
            "text": "Typically you won't write Ignition files yourself, rather you will typically use a tool like the  config transpiler  to generate them.  However the Ignition file is created, it should be placed in a location which qemu can access. In this example, we'll place it in  /var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign .  mkdir -p /var/lib/libvirt/flatcar-linux/flatcar-linux1/\necho '{\"ignition\":{\"version\":\"2.0.0\"}}' > /var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign  If the host uses SELinux, allow the VM access to the config:  semanage fcontext -a -t virt_content_t \"/var/lib/libvirt/flatcar-linux/flatcar-linux1\"\nrestorecon -R \"/var/lib/libvirt/flatcar-linux/flatcar-linux1\"  A simple Flatcar Container Linux config to add your ssh keys might look like the following:  storage:\n  files:\n  - path: /etc/hostname\n    filesystem: \"root\"\n    contents:\n      inline: \"flatcar-linux1\"\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"",
            "title": "Create the Ignition config"
        },
        {
            "location": "/os/booting-with-libvirt/#creating-the-domain-xml",
            "text": "Once the Ignition file exists on disk, the machine can be configured to use it.  Start by creating a libvirt  domain XML  document:  virt-install --connect qemu:///system \\\n             --import \\\n             --name flatcar-linux1 \\\n             --ram 1024 --vcpus 1 \\\n             --os-type=linux \\\n             --os-variant=virtio26 \\\n             --disk path=/var/lib/libvirt/images/flatcar-linux/flatcar-linux1.qcow2,format=qcow2,bus=virtio \\\n             --vnc --noautoconsole \\\n             --print-xml > /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml  Next, modify the domain xml to reference the qemu-specific configuration needed:  <?xml version=\"1.0\"?>\n<domain xmlns:qemu=\"http://libvirt.org/schemas/domain/qemu/1.0\" type=\"kvm\">\n  ...\n  <qemu:commandline>\n    <qemu:arg value=\"-fw_cfg\"/>\n    <qemu:arg value=\"name=opt/org.flatcar-linux/config,file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\"/>\n  </qemu:commandline>\n</domain>  If you have the  xmlstarlet  utility installed, the above modification can be accomplished easily with the following:  domain=/var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nignition_file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\nxmlstarlet ed -P -L -i \"//domain\" -t attr -n \"xmlns:qemu\" --value \"http://libvirt.org/schemas/domain/qemu/1.0\" \"${domain}\"\nxmlstarlet ed -P -L -s \"//domain\" -t elem -n \"qemu:commandline\" \"${domain}\"\nxmlstarlet ed -P -L -s \"//domain/qemu:commandline\" -t elem -n \"qemu:arg\" \"${domain}\"\nxmlstarlet ed -P -L -s \"(//domain/qemu:commandline/qemu:arg)[1]\" -t attr -n \"value\" -v \"-fw_cfg\" \"${domain}\"\nxmlstarlet ed -P -L -s \"//domain/qemu:commandline\" -t elem -n \"qemu:arg\" \"${domain}\"\nxmlstarlet ed -P -L -s \"(//domain/qemu:commandline/qemu:arg)[2]\" -t attr -n \"value\" -v \"name=opt/org.flatcar-linux/config,file=${ignition_file}\" \"${domain}\"  Alternately, you can accomplish the same modification using sed:  domain=/var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nignition_file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\nsed -i 's|type=\"kvm\"|type=\"kvm\" xmlns:qemu=\"http://libvirt.org/schemas/domain/qemu/1.0\"|' \"${domain}\"\nsed -i \"/<\\/devices>/a <qemu:commandline>\\n  <qemu:arg value='-fw_cfg'/>\\n  <qemu:arg value='name=opt/org.flatcar-linux/config,file=${ignition_file}'/>\\n</qemu:commandline>\" \"${domain}\"",
            "title": "Creating the domain xml"
        },
        {
            "location": "/os/booting-with-libvirt/#define-and-start-the-machine",
            "text": "Once the XML domain has been edited to include the Ignition file, it can be created and started using the  virsh  tool included with libvirt:  virsh define /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nvirsh start flatcar-linux1",
            "title": "Define and start the machine"
        },
        {
            "location": "/os/booting-with-libvirt/#ssh-into-the-machine",
            "text": "By default, libvirt runs its own DHCP server which will provide an IP address to new instances. You can query it for what IP addresses have been assigned to machines:  $ virsh net-dhcp-leases default\nExpiry Time          MAC address        Protocol  IP address                Hostname        Client ID or DUID\n-------------------------------------------------------------------------------------------------------------------\n 2017-08-09 16:32:52  52:54:00:13:12:45  ipv4      192.168.122.184/24        flatcar-linux1 ff:32:39:f9:b5:00:02:00:00:ab:11:06:6a:55:ed:5d:0a:73:ee",
            "title": "SSH into the machine"
        },
        {
            "location": "/os/booting-with-libvirt/#network-configuration",
            "text": "",
            "title": "Network configuration"
        },
        {
            "location": "/os/booting-with-libvirt/#static-ip",
            "text": "By default, Flatcar Container Linux uses DHCP to get its network configuration. In this example the VM will be attached directly to the local network via a bridge on the host's virbr0 and the local network. To configure a static address add a  networkd unit  to the Flatcar Container Linux config:  passwd:\n  users:\n  - name: core\n    ssh_authorized_keys:\n    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\n\nstorage:\n  files:\n  - path: /etc/hostname\n    filesystem: \"root\"\n    contents: \n      inline: flatcar-linux1\n\nnetworkd:\n  units:\n  - name: 10-ens3.network\n    contents: |\n      [Match]\n      MACAddress=52:54:00:fe:b3:c0\n\n      [Network]\n      Address=192.168.122.2\n      Gateway=192.168.122.1\n      DNS=8.8.8.8",
            "title": "Static IP"
        },
        {
            "location": "/os/booting-with-libvirt/#using-dhcp-with-a-libvirt-network",
            "text": "An alternative to statically configuring an IP at the host level is to do so at the libvirt level. If you're using libvirt's built in DHCP server and a recent libvirt version, it allows configuring what IP address will be provided to a given machine ahead of time.  This can be done using the  net-update  command. The following assumes you're using the  default  libvirt network and have configured the MAC Address to  52:54:00:fe:b3:c0  through the  --network  flag on  virt-install :  ip=\"192.168.122.2\"\nmac=\"52:54:00:fe:b3:c0\"\n\nvirsh net-update --network \"default\" add-last ip-dhcp-host \\\n    --xml \"<host mac='${mac}' ip='${ip}' />\" \\\n    --live --config  By executing these commands before running  virsh start , we can ensure the libvirt DHCP server will hand out a known IP.",
            "title": "Using DHCP with a libvirt network"
        },
        {
            "location": "/os/booting-with-libvirt/#virtual-machine-startup",
            "text": "Now, start this libvirt instance with the RAM, vCPU, and networking configuration defined above:  ignition_file=/var/lib/libvirt/flatcar-linux/flatcar-linux1/provision.ign\n\ndomain=/var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nip=\"192.168.122.2\"\nmac=\"52:54:00:fe:b3:c0\"\n\nmkdir -p \"$(dirname \"${domain}\")\"\n\nvirsh net-update --network \"default\" add-last ip-dhcp-host \\\n    --xml \"<host mac='${mac}' ip='${ip}' />\" \\\n    --live --config\n\nvirt-install --connect qemu:///system --import \\\n  --name flatcar-linux1 \\\n  --ram 1024 --vcpus 1 \\\n  --os-type=linux \\\n  --os-variant=virtio26 \\\n  --disk path=/var/lib/libvirt/images/flatcar-linux/flatcar-linux1.qcow2,format=qcow2,bus=virtio \\\n  --network bridge=virbr0,mac=52:54:00:fe:b3:c0 \\\n  --vnc --noautoconsole \\\n  --print-xml > /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\n\nsed -ie 's|type=\"kvm\"|type=\"kvm\" xmlns:qemu=\"http://libvirt.org/schemas/domain/qemu/1.0\"|' \"${domain}\"\nsed -i \"/<\\/devices>/a <qemu:commandline>\\n  <qemu:arg value='-fw_cfg'/>\\n  <qemu:arg value='name=opt/org.flatcar-linux/config,file=${ignition_file}'/>\\n</qemu:commandline>\" \"${domain}\"\n\nvirsh define /var/lib/libvirt/flatcar-linux/flatcar-linux1/domain.xml\nvirsh start flatcar-linux1  Once the virtual machine has started you can log in via SSH:  ssh core@192.168.122.2",
            "title": "Virtual machine startup"
        },
        {
            "location": "/os/booting-with-libvirt/#ssh-config",
            "text": "To simplify this and avoid potential host key errors in the future add the following to  ~/.ssh/config :  Host flatcar-linux1\nHostName 192.168.122.2\nUser core\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/null  Now you can log in to the virtual machine with:  ssh flatcar-linux1",
            "title": "SSH Config"
        },
        {
            "location": "/os/booting-with-libvirt/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-pxe/",
            "text": "Booting Flatcar Container Linux via PXE\n\u00b6\n\n\nThese instructions will walk you through booting Flatcar Container Linux via PXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be \ninstalled to disk\n.\n\n\nA minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.\n\n\nConfiguring pxelinux\n\u00b6\n\n\nThis guide assumes you already have a working PXE server using \npxelinux\n. If you need suggestions on how to set a server up, check out guides for \nDebian\n, \nFedora\n or \nUbuntu\n.\n\n\nSetting up pxelinux.cfg\n\u00b6\n\n\nWhen configuring the Flatcar Container Linux pxelinux.cfg there are a few kernel options that may be useful but all are optional.\n\n\n\n\nrootfstype=tmpfs\n: Use tmpfs for the writable root filesystem. This is the default behavior.\n\n\nrootfstype=btrfs\n: Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.\n\n\nroot\n: Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g: \nroot=/dev/sda1\n, \nroot=LABEL=ROOT\n or \nroot=UUID=2c618316-d17a-4688-b43b-aa19d97ea821\n.\n\n\nsshkey\n: Add the given SSH public key to the \ncore\n user's authorized_keys file. Replace the example key below with your own (it is usually in \n~/.ssh/id_rsa.pub\n)\n\n\nconsole\n: Enable kernel output and a login prompt on a given tty. The default, \ntty0\n, generally maps to VGA. Can be used multiple times, e.g. \nconsole=tty0 console=ttyS0\n\n\nflatcar.autologin\n: Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the \nconsole\n option, e.g. \nconsole=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0\n. Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals (\ntty1\n, \ntty2\n, etc), not the VGA console itself (\ntty0\n).\n\n\nflatcar.first_boot=1\n: Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the \nconfig transpiler documentation\n for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.\n\n\nignition.config.url\n: Download the Ignition config from the specified URL. \nhttp\n, \nhttps\n, \ns3\n, and \ntftp\n schemes are supported.\n\n\nip\n: Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See \nIgnition documentation\n for the complete syntax.\n\n\n\n\nThis is an example pxelinux.cfg file that assumes Flatcar Container Linux is the only option. You should be able to copy this verbatim into \n/var/lib/tftpboot/pxelinux.cfg/default\n after providing an Ignition config URL:\n\n\ndefault flatcar\nprompt 1\ntimeout 15\n\ndisplay boot.msg\n\nlabel flatcar\n  menu default\n  kernel flatcar_production_pxe.vmlinuz\n  initrd flatcar_production_pxe_image.cpio.gz\n  append flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign\n\n\n\nHere's a common config example which should be located at the URL from above:\n\n\nsystemd:\n  units:\n    - name: etcd2.service\n      enable: true\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq...\n\n\n\nChoose a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nPXE booted machines cannot currently update themselves when new versions are released to a channel. To update to the latest version of Flatcar Container Linux download/verify these files again and reboot.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \nIn the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root.\n\n      \nThe \nflatcar_production_pxe.vmlinuz.sig\n and \nflatcar_production_pxe_image.cpio.gz.sig\n files can be used to \nverify the downloaded files\n.\n\n      \n\ncd /var/lib/tftpboot\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n      \n\n    \n\n  \n\n\n\n\n\nBooting the box\n\u00b6\n\n\nAfter setting up the PXE server as outlined above you can start the target machine in PXE boot mode. The machine should grab the image from the server and boot into Flatcar Container Linux. If something goes wrong you can direct questions to the \nIRC channel\n or \nmailing list\n.\n\n\nThis is localhost.unknown_domain (Linux x86_64 3.10.10+) 19:53:36\nSSH host key: 24:2e:f1:3f:5f:9c:63:e5:8c:17:47:32:f4:09:5d:78 (RSA)\nSSH host key: ed:84:4d:05:e3:7d:e3:d0:b9:58:90:58:3b:99:3a:4c (DSA)\nens0: 10.0.2.15 fe80::5054:ff:fe12:3456\nlocalhost login:\n\n\n\nLogging in\n\u00b6\n\n\nThe IP address for the machine should be printed out to the terminal for convenience. If it doesn't show up immediately, press enter a few times and it should show up. Now you can simply SSH in using public key authentication:\n\n\nssh core@10.0.2.15\n\n\n\nUpdate Process\n\u00b6\n\n\nSince our upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.\n\n\nInstallation\n\u00b6\n\n\nOnce booted it is possible to \ninstall Flatcar Container Linux on a local disk\n or to just use local storage for the writable root filesystem while continuing to boot Flatcar Container Linux itself via PXE.\n\n\nIf you plan on using Docker we recommend using a local ext4 filesystem with overlayfs, however, btrfs is also available to use if needed.\n\n\nFor example, to setup an ext4 root filesystem on \n/dev/sda\n:\n\n\nstorage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: ext4\n      wipe_filesystem: true\n      label: ROOT\n\n\n\nAnd add \nroot=/dev/sda1\n or \nroot=LABEL=ROOT\n to the kernel options as documented above.\n\n\nSimilarly, to setup a btrfs root filesystem on \n/dev/sda\n:\n\n\nstorage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: btrfs\n      wipe_filesystem: true\n      label: ROOT\n\n\n\nAdding a Custom OEM\n\u00b6\n\n\nSimilar to the \nOEM partition\n in Flatcar Container Linux disk images, PXE images can be customized with an \nIgnition config\n bundled in the initramfs. Simply create a \n./usr/share/oem/\n directory, add a \nconfig.ign\n file containing the Ignition config, and add the directory tree as an additional initramfs:\n\n\nmkdir -p usr/share/oem\ncp example.ign ./usr/share/oem/config.ign\nfind usr | cpio -o -H newc -O oem.cpio\ngzip oem.cpio\n\n\n\nConfirm the archive looks correct and has your config inside of it:\n\n\ngzip --stdout --decompress oem.cpio.gz | cpio -it\n./\nusr\nusr/share\nusr/share/oem\nusr/share/oem/config.ign\n\n\n\nAdd the \noem.cpio.gz\n file to your PXE boot directory, then \nappend it\n to the \ninitrd\n line in your \npxelinux.cfg\n:\n\n\n...\ninitrd flatcar_production_pxe_image.cpio.gz,oem.cpio.gz\nkernel flatcar_production_pxe.vmlinuz flatcar.first_boot=1\n...\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Booting Flatcar Container Linux via PXE"
        },
        {
            "location": "/os/booting-with-pxe/#booting-flatcar-container-linux-via-pxe",
            "text": "These instructions will walk you through booting Flatcar Container Linux via PXE on real or virtual hardware. By default, this will run Flatcar Container Linux completely out of RAM. Flatcar Container Linux can also be  installed to disk .  A minimum of 3 GB of RAM is required to boot Flatcar Container Linux via PXE.",
            "title": "Booting Flatcar Container Linux via PXE"
        },
        {
            "location": "/os/booting-with-pxe/#configuring-pxelinux",
            "text": "This guide assumes you already have a working PXE server using  pxelinux . If you need suggestions on how to set a server up, check out guides for  Debian ,  Fedora  or  Ubuntu .",
            "title": "Configuring pxelinux"
        },
        {
            "location": "/os/booting-with-pxe/#setting-up-pxelinuxcfg",
            "text": "When configuring the Flatcar Container Linux pxelinux.cfg there are a few kernel options that may be useful but all are optional.   rootfstype=tmpfs : Use tmpfs for the writable root filesystem. This is the default behavior.  rootfstype=btrfs : Use btrfs in RAM for the writable root filesystem. The filesystem will consume more RAM as it grows, up to a max of 50%. The limit isn't currently configurable.  root : Use a local filesystem for root instead of one of two in-ram options above. The filesystem must be formatted (perhaps using Ignition) but may be completely blank; it will be initialized on boot. The filesystem may be specified by any of the usual ways including device, label, or UUID; e.g:  root=/dev/sda1 ,  root=LABEL=ROOT  or  root=UUID=2c618316-d17a-4688-b43b-aa19d97ea821 .  sshkey : Add the given SSH public key to the  core  user's authorized_keys file. Replace the example key below with your own (it is usually in  ~/.ssh/id_rsa.pub )  console : Enable kernel output and a login prompt on a given tty. The default,  tty0 , generally maps to VGA. Can be used multiple times, e.g.  console=tty0 console=ttyS0  flatcar.autologin : Drop directly to a shell on a given console without prompting for a password. Useful for troubleshooting but use with caution. For any console that doesn't normally get a login prompt by default be sure to combine with the  console  option, e.g.  console=tty0 console=ttyS0 flatcar.autologin=tty1 flatcar.autologin=ttyS0 . Without any argument it enables access on all consoles. Note that for the VGA console the login prompts are on virtual terminals ( tty1 ,  tty2 , etc), not the VGA console itself ( tty0 ).  flatcar.first_boot=1 : Download an Ignition config and use it to provision your booted system. Ignition configs are generated from Container Linux Configs. See the  config transpiler documentation  for more information. If a local filesystem is used for the root partition, pass this parameter only on the first boot.  ignition.config.url : Download the Ignition config from the specified URL.  http ,  https ,  s3 , and  tftp  schemes are supported.  ip : Configure temporary static networking for initramfs. This parameter does not influence the final network configuration of the node and is mostly useful for first-boot provisioning of systems in DHCP-less environments. See  Ignition documentation  for the complete syntax.   This is an example pxelinux.cfg file that assumes Flatcar Container Linux is the only option. You should be able to copy this verbatim into  /var/lib/tftpboot/pxelinux.cfg/default  after providing an Ignition config URL:  default flatcar\nprompt 1\ntimeout 15\n\ndisplay boot.msg\n\nlabel flatcar\n  menu default\n  kernel flatcar_production_pxe.vmlinuz\n  initrd flatcar_production_pxe_image.cpio.gz\n  append flatcar.first_boot=1 ignition.config.url=https://example.com/pxe-config.ign  Here's a common config example which should be located at the URL from above:  systemd:\n  units:\n    - name: etcd2.service\n      enable: true\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq...",
            "title": "Setting up pxelinux.cfg"
        },
        {
            "location": "/os/booting-with-pxe/#choose-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  PXE booted machines cannot currently update themselves when new versions are released to a channel. To update to the latest version of Flatcar Container Linux download/verify these files again and reboot.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n       \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n       \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig\n       \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       In the config above you can see that a Kernel image and a initramfs file is needed. Download these two files into your tftp root. \n       The  flatcar_production_pxe.vmlinuz.sig  and  flatcar_production_pxe_image.cpio.gz.sig  files can be used to  verify the downloaded files . \n       \ncd /var/lib/tftpboot\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe.vmlinuz.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_pxe_image.cpio.gz.sig\ngpg --verify flatcar_production_pxe.vmlinuz.sig\ngpg --verify flatcar_production_pxe_image.cpio.gz.sig",
            "title": "Choose a channel"
        },
        {
            "location": "/os/booting-with-pxe/#booting-the-box",
            "text": "After setting up the PXE server as outlined above you can start the target machine in PXE boot mode. The machine should grab the image from the server and boot into Flatcar Container Linux. If something goes wrong you can direct questions to the  IRC channel  or  mailing list .  This is localhost.unknown_domain (Linux x86_64 3.10.10+) 19:53:36\nSSH host key: 24:2e:f1:3f:5f:9c:63:e5:8c:17:47:32:f4:09:5d:78 (RSA)\nSSH host key: ed:84:4d:05:e3:7d:e3:d0:b9:58:90:58:3b:99:3a:4c (DSA)\nens0: 10.0.2.15 fe80::5054:ff:fe12:3456\nlocalhost login:",
            "title": "Booting the box"
        },
        {
            "location": "/os/booting-with-pxe/#logging-in",
            "text": "The IP address for the machine should be printed out to the terminal for convenience. If it doesn't show up immediately, press enter a few times and it should show up. Now you can simply SSH in using public key authentication:  ssh core@10.0.2.15",
            "title": "Logging in"
        },
        {
            "location": "/os/booting-with-pxe/#update-process",
            "text": "Since our upgrade process requires a disk, this image does not have the option to update itself. Instead, the box simply needs to be rebooted and will be running the latest version, assuming that the image served by the PXE server is regularly updated.",
            "title": "Update Process"
        },
        {
            "location": "/os/booting-with-pxe/#installation",
            "text": "Once booted it is possible to  install Flatcar Container Linux on a local disk  or to just use local storage for the writable root filesystem while continuing to boot Flatcar Container Linux itself via PXE.  If you plan on using Docker we recommend using a local ext4 filesystem with overlayfs, however, btrfs is also available to use if needed.  For example, to setup an ext4 root filesystem on  /dev/sda :  storage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: ext4\n      wipe_filesystem: true\n      label: ROOT  And add  root=/dev/sda1  or  root=LABEL=ROOT  to the kernel options as documented above.  Similarly, to setup a btrfs root filesystem on  /dev/sda :  storage:\n  disks:\n  - device: /dev/sda\n    wipe_table: true\n    partitions:\n    - label: ROOT\n  filesystems:\n  - mount:\n      device: /dev/disk/by-partlabel/ROOT\n      format: btrfs\n      wipe_filesystem: true\n      label: ROOT",
            "title": "Installation"
        },
        {
            "location": "/os/booting-with-pxe/#adding-a-custom-oem",
            "text": "Similar to the  OEM partition  in Flatcar Container Linux disk images, PXE images can be customized with an  Ignition config  bundled in the initramfs. Simply create a  ./usr/share/oem/  directory, add a  config.ign  file containing the Ignition config, and add the directory tree as an additional initramfs:  mkdir -p usr/share/oem\ncp example.ign ./usr/share/oem/config.ign\nfind usr | cpio -o -H newc -O oem.cpio\ngzip oem.cpio  Confirm the archive looks correct and has your config inside of it:  gzip --stdout --decompress oem.cpio.gz | cpio -it\n./\nusr\nusr/share\nusr/share/oem\nusr/share/oem/config.ign  Add the  oem.cpio.gz  file to your PXE boot directory, then  append it  to the  initrd  line in your  pxelinux.cfg :  ...\ninitrd flatcar_production_pxe_image.cpio.gz,oem.cpio.gz\nkernel flatcar_production_pxe.vmlinuz flatcar.first_boot=1\n...",
            "title": "Adding a Custom OEM"
        },
        {
            "location": "/os/booting-with-pxe/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-qemu/",
            "text": "Running Flatcar Container Linux on QEMU\n\u00b6\n\n\nThese instructions will bring up a single Flatcar Container Linux instance under QEMU, the small Swiss Army knife of virtual machine and CPU emulators. If you need to do more such as \nconfiguring networks\n differently refer to the \nQEMU Wiki\n and \nUser Documentation\n.\n\n\nYou can direct questions to the \nIRC channel\n or \nmailing list\n.\n\n\nInstall QEMU\n\u00b6\n\n\nIn addition to Linux it can be run on Windows and OS X but works best on Linux. It should be available on just about any distro.\n\n\nDebian or Ubuntu\n\u00b6\n\n\nDocumentation for \nDebian\n has more details but to get started all you need is:\n\n\nsudo apt-get install qemu-system-x86 qemu-utils\n\n\n\nFedora or RedHat\n\u00b6\n\n\nThe Fedora wiki has a \nquick howto\n but the basic install is easy:\n\n\nsudo yum install qemu-system-x86 qemu-img\n\n\n\nArch\n\u00b6\n\n\nThis is all you need to get started:\n\n\nsudo pacman -S qemu\n\n\n\nMore details can be found on \nArch's QEMU wiki page\n.\n\n\nGentoo\n\u00b6\n\n\nAs to be expected, Gentoo can be a little more complicated but all the required kernel options and USE flags are covered in the \nGentoo Wiki\n. Usually this should be sufficient:\n\n\necho app-emulation/qemu qemu_softmmu_targets_x86_64 virtfs xattr >> /etc/portage/package.use\nemerge -av app-emulation/qemu\n\n\n\nStartup Flatcar Container Linux\n\u00b6\n\n\nOnce QEMU is installed you can download and start the latest Flatcar Container Linux image.\n\n\nChoosing a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \n\n        \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n       \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n    \n\n      \n\n        \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n    \n\n      \n\n        \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n    \n\n      \n\n        \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \n\n      \nThere are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU.\n\n      \nmkdir flatcar; cd flatcar\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh\n\n    \n\n  \n\n\n\n\n\nStarting is as simple as:\n\n\n./flatcar_production_qemu.sh -nographic\n\n\n\nSSH keys\n\u00b6\n\n\nIn order to log in to the virtual machine you will need to use ssh keys. If you don't already have a ssh key pair you can generate one simply by running the command \nssh-keygen\n. The wrapper script will automatically look for public keys in ssh-agent if available and at the default locations \n~/.ssh/id_dsa.pub\n or \n~/.ssh/id_rsa.pub\n. If you need to provide an alternate location use the -a option:\n\n\n./flatcar_production_qemu.sh -a ~/.ssh/authorized_keys -- -nographic\n\n\n\nNote: Options such as \n-a\n for the wrapper script must be specified before any options for QEMU. To make the separation between the two explicit you can use \n--\n but that isn't required. See \n./flatcar_production_qemu.sh -h\n for details.\n\n\nOnce the virtual machine has started you can log in via SSH:\n\n\nssh -l core -p 2222 localhost\n\n\n\nSSH config\n\u00b6\n\n\nTo simplify this and avoid potential host key errors in the future add the following to \n~/.ssh/config\n:\n\n\nHost flatcar\nHostName localhost\nPort 2222\nUser core\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/null\n\n\n\nNow you can log in to the virtual machine with:\n\n\nssh flatcar\n\n\n\nContainer Linux Configs\n\u00b6\n\n\nFlatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the \ndocs to learn about the supported features\n. An Ignition config can be passed to the virtual machine using the QEMU Firmware Configuration Device. The wrapper script provides a method for doing so:\n\n\n./flatcar_production_qemu.sh -i config.ign -- -nographic\n\n\n\nThis will pass the contents of \nconfig.ign\n through to Ignition, which runs in the virtual machine.\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Running Flatcar Container Linux on QEMU"
        },
        {
            "location": "/os/booting-with-qemu/#running-flatcar-container-linux-on-qemu",
            "text": "These instructions will bring up a single Flatcar Container Linux instance under QEMU, the small Swiss Army knife of virtual machine and CPU emulators. If you need to do more such as  configuring networks  differently refer to the  QEMU Wiki  and  User Documentation .  You can direct questions to the  IRC channel  or  mailing list .",
            "title": "Running Flatcar Container Linux on QEMU"
        },
        {
            "location": "/os/booting-with-qemu/#install-qemu",
            "text": "In addition to Linux it can be run on Windows and OS X but works best on Linux. It should be available on just about any distro.",
            "title": "Install QEMU"
        },
        {
            "location": "/os/booting-with-qemu/#debian-or-ubuntu",
            "text": "Documentation for  Debian  has more details but to get started all you need is:  sudo apt-get install qemu-system-x86 qemu-utils",
            "title": "Debian or Ubuntu"
        },
        {
            "location": "/os/booting-with-qemu/#fedora-or-redhat",
            "text": "The Fedora wiki has a  quick howto  but the basic install is easy:  sudo yum install qemu-system-x86 qemu-img",
            "title": "Fedora or RedHat"
        },
        {
            "location": "/os/booting-with-qemu/#arch",
            "text": "This is all you need to get started:  sudo pacman -S qemu  More details can be found on  Arch's QEMU wiki page .",
            "title": "Arch"
        },
        {
            "location": "/os/booting-with-qemu/#gentoo",
            "text": "As to be expected, Gentoo can be a little more complicated but all the required kernel options and USE flags are covered in the  Gentoo Wiki . Usually this should be sufficient:  echo app-emulation/qemu qemu_softmmu_targets_x86_64 virtfs xattr >> /etc/portage/package.use\nemerge -av app-emulation/qemu",
            "title": "Gentoo"
        },
        {
            "location": "/os/booting-with-qemu/#startup-flatcar-container-linux",
            "text": "Once QEMU is installed you can download and start the latest Flatcar Container Linux image.",
            "title": "Startup Flatcar Container Linux"
        },
        {
            "location": "/os/booting-with-qemu/#choosing-a-channel",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       \n         The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n        \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     \n       \n         The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     \n       \n         The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://beta.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     \n       \n         The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       \n       There are two files you need: the disk image (provided in qcow2\n      format) and the wrapper shell script to start QEMU. \n       mkdir flatcar; cd flatcar\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu.sh.sig\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\nwget https://edge.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\ngpg --verify flatcar_production_qemu.sh.sig\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\nbzip2 -d flatcar_production_qemu_image.img.bz2\nchmod +x flatcar_production_qemu.sh \n     \n     Starting is as simple as:  ./flatcar_production_qemu.sh -nographic",
            "title": "Choosing a channel"
        },
        {
            "location": "/os/booting-with-qemu/#ssh-keys",
            "text": "In order to log in to the virtual machine you will need to use ssh keys. If you don't already have a ssh key pair you can generate one simply by running the command  ssh-keygen . The wrapper script will automatically look for public keys in ssh-agent if available and at the default locations  ~/.ssh/id_dsa.pub  or  ~/.ssh/id_rsa.pub . If you need to provide an alternate location use the -a option:  ./flatcar_production_qemu.sh -a ~/.ssh/authorized_keys -- -nographic  Note: Options such as  -a  for the wrapper script must be specified before any options for QEMU. To make the separation between the two explicit you can use  --  but that isn't required. See  ./flatcar_production_qemu.sh -h  for details.  Once the virtual machine has started you can log in via SSH:  ssh -l core -p 2222 localhost",
            "title": "SSH keys"
        },
        {
            "location": "/os/booting-with-qemu/#ssh-config",
            "text": "To simplify this and avoid potential host key errors in the future add the following to  ~/.ssh/config :  Host flatcar\nHostName localhost\nPort 2222\nUser core\nStrictHostKeyChecking no\nUserKnownHostsFile /dev/null  Now you can log in to the virtual machine with:  ssh flatcar",
            "title": "SSH config"
        },
        {
            "location": "/os/booting-with-qemu/#container-linux-configs",
            "text": "Flatcar Container Linux allows you to configure machine parameters, configure networking, launch systemd units on startup, and more via Container Linux Configs. These configs are then transpiled into Ignition configs and given to booting machines. Head over to the  docs to learn about the supported features . An Ignition config can be passed to the virtual machine using the QEMU Firmware Configuration Device. The wrapper script provides a method for doing so:  ./flatcar_production_qemu.sh -i config.ign -- -nographic  This will pass the contents of  config.ign  through to Ignition, which runs in the virtual machine.",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/booting-with-qemu/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/btrfs-troubleshooting/",
            "text": "Working with btrfs and common troubleshooting\n\u00b6\n\n\nbtrfs is a copy-on-write filesystem with full support in the upstream Linux kernel and several desirable features. In the past, Flatcar Container Linux shipped with a btrfs root filesystem to support Docker filesystem requirements at the time. As of version 561.0.0, Flatcar Container Linux ships with ext4 as the default root filesystem by default while still supporting Docker. Btrfs is still supported and works with the latest Flatcar Container Linux releases and Docker, but we recommend using ext4.\n\n\nbtrfs was marked as experimental for a long time, but it's now fully production-ready and supported by a number of Linux distributions.\n\n\nNotable Features of btrfs:\n\n\n\n\nAbility to add/remove block devices without interruption\n\n\nAbility to balance the filesystem without interruption\n\n\nRAID 0, RAID 1, RAID 5, RAID 6 and RAID 10\n\n\nSnapshots and file cloning\n\n\n\n\nThis guide won't cover these topics \u2014 it's mostly focused on troubleshooting.\n\n\nFor a more complete troubleshooting experience, let's explore how btrfs works under the hood.\n\n\nbtrfs stores data in chunks across all of the block devices on the system. The total storage across these devices is shown in the standard output of \ndf -h\n.\n\n\nRaw data and filesystem metadata are stored in one or many chunks, typically ~1GiB in size. When RAID is configured, these chunks are replicated instead of individual files.\n\n\nA copy-on-write filesystem maintains many changes of a single file, which is helpful for snapshotting and other advanced features, but can lead to fragmentation with some workloads.\n\n\nNo space left on device\n\u00b6\n\n\nWhen the filesystem is out of chunks to write data into, \nNo space left on device\n will be reported. This will prevent journal files from being recorded, containers from starting and so on.\n\n\nThe common reaction to this error is to run \ndf -h\n and you'll see that there is still some free space. That command isn't measuring the btrfs primitives (chunks, metadata, etc), which is what really matters.\n\n\nRunning \nsudo btrfs fi show\n will give you the btrfs view of how much free space you have. When starting/stopping many Docker containers or doing a large amount of random writes, chunks will become duplicated in an inefficient manner over time.\n\n\nRe-balancing the filesystem (\nofficial btrfs docs\n) will relocate data from empty or near-empty chunks to free up space. This operation can be done without downtime.\n\n\nFirst, let's see how much free space we have:\n\n\n$ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.44GiB\n  devid    1 size 32.68GiB used 32.68GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414\n\n\n\nThe answer: not a lot. We can re-balance to fix that.\n\n\nThe re-balance command can be configured to only relocate data in chunks up to a certain percentage used. This will prevent you from moving around a lot of data without a lot of benefit. If your disk is completely full, you may need to delete a few containers to create space for the re-balance operation to work with.\n\n\nLet's try to relocate chunks with less than 5% of usage:\n\n\n$ sudo btrfs fi balance start -dusage=5 /\nDone, had to relocate 5 out of 45 chunks\n$ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.39GiB\n  devid    1 size 32.68GiB used 28.93GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414\n\n\n\nThe operation took about a minute on a cloud server and gained us 4GiB of space on the filesystem. It's up to you to find out what percentage works best for your workload, the speed of your disks, etc.\n\n\nIf your balance operation is taking a long time, you can open a new shell and find the status:\n\n\n$ sudo btrfs balance status /\nBalance on '/' is running\n0 out of about 1 chunks balanced (1 considered), 100% left\n\n\n\nAdding a new physical disk\n\u00b6\n\n\nNew physical disks can be added to an existing btrfs filesystem. The first step is to have the new block device \nmounted on the machine\n. Afterwards, let btrfs know about the new device and re-balance the file system. The key step here is re-balancing, which will move the data and metadata across both block devices. Expect this process to take some time:\n\n\n$ btrfs device add /dev/sdc /\n$ btrfs filesystem balance /\n\n\n\nDisable copy-on-write\n\u00b6\n\n\nCopy-On-write isn't ideal for workloads that create or modify many small files, such as databases. Without disabling COW, you can heavily fragment the file system as explained above.\n\n\nThe best strategy for successfully running a database in a container is to disable COW on directory/volume that is mounted into the container.\n\n\nThe COW setting is stored as a file attribute and is modified with a utility called \nchattr\n. To disable COW for a MySQL container's volume, run:\n\n\n$ sudo mkdir /var/lib/mysql\n$ sudo chattr -R +C /var/lib/mysql\n\n\n\nThe directory \n/var/lib/mysql\n is now ready to be used by a Docker container without COW. Let's break down the command:\n\n\n-R\n indicates that want to recursively change the file attribute\n\n+C\n means we want to set the NOCOW attribute on the file/directory\n\n\nTo verify, we can run:\n\n\n$ sudo lsattr /var/lib/\n---------------- /var/lib/portage\n---------------- /var/lib/gentoo\n---------------- /var/lib/iptables\n---------------- /var/lib/ip6tables\n---------------- /var/lib/arpd\n---------------- /var/lib/ipset\n---------------- /var/lib/dbus\n---------------- /var/lib/systemd\n---------------- /var/lib/polkit-1\n---------------- /var/lib/dhcpcd\n---------------- /var/lib/ntp\n---------------- /var/lib/nfs\n---------------- /var/lib/etcd\n---------------- /var/lib/docker\n---------------- /var/lib/update_engine\n---------------C /var/lib/mysql\n\n\n\nDisable via a unit file\n\u00b6\n\n\nSetting the file attributes can be done via a systemd unit using two \nExecStartPre\n commands:\n\n\nExecStartPre=/usr/bin/mkdir -p /var/lib/mysql\nExecStartPre=/usr/bin/chattr -R +C /var/lib/mysql",
            "title": "Working with btrfs and common troubleshooting"
        },
        {
            "location": "/os/btrfs-troubleshooting/#working-with-btrfs-and-common-troubleshooting",
            "text": "btrfs is a copy-on-write filesystem with full support in the upstream Linux kernel and several desirable features. In the past, Flatcar Container Linux shipped with a btrfs root filesystem to support Docker filesystem requirements at the time. As of version 561.0.0, Flatcar Container Linux ships with ext4 as the default root filesystem by default while still supporting Docker. Btrfs is still supported and works with the latest Flatcar Container Linux releases and Docker, but we recommend using ext4.  btrfs was marked as experimental for a long time, but it's now fully production-ready and supported by a number of Linux distributions.  Notable Features of btrfs:   Ability to add/remove block devices without interruption  Ability to balance the filesystem without interruption  RAID 0, RAID 1, RAID 5, RAID 6 and RAID 10  Snapshots and file cloning   This guide won't cover these topics \u2014 it's mostly focused on troubleshooting.  For a more complete troubleshooting experience, let's explore how btrfs works under the hood.  btrfs stores data in chunks across all of the block devices on the system. The total storage across these devices is shown in the standard output of  df -h .  Raw data and filesystem metadata are stored in one or many chunks, typically ~1GiB in size. When RAID is configured, these chunks are replicated instead of individual files.  A copy-on-write filesystem maintains many changes of a single file, which is helpful for snapshotting and other advanced features, but can lead to fragmentation with some workloads.",
            "title": "Working with btrfs and common troubleshooting"
        },
        {
            "location": "/os/btrfs-troubleshooting/#no-space-left-on-device",
            "text": "When the filesystem is out of chunks to write data into,  No space left on device  will be reported. This will prevent journal files from being recorded, containers from starting and so on.  The common reaction to this error is to run  df -h  and you'll see that there is still some free space. That command isn't measuring the btrfs primitives (chunks, metadata, etc), which is what really matters.  Running  sudo btrfs fi show  will give you the btrfs view of how much free space you have. When starting/stopping many Docker containers or doing a large amount of random writes, chunks will become duplicated in an inefficient manner over time.  Re-balancing the filesystem ( official btrfs docs ) will relocate data from empty or near-empty chunks to free up space. This operation can be done without downtime.  First, let's see how much free space we have:  $ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.44GiB\n  devid    1 size 32.68GiB used 32.68GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414  The answer: not a lot. We can re-balance to fix that.  The re-balance command can be configured to only relocate data in chunks up to a certain percentage used. This will prevent you from moving around a lot of data without a lot of benefit. If your disk is completely full, you may need to delete a few containers to create space for the re-balance operation to work with.  Let's try to relocate chunks with less than 5% of usage:  $ sudo btrfs fi balance start -dusage=5 /\nDone, had to relocate 5 out of 45 chunks\n$ sudo btrfs fi show\nLabel: 'ROOT'  uuid: 82a40c46-557e-4848-ad4d-10c6e36ed5ad\n  Total devices 1 FS bytes used 13.39GiB\n  devid    1 size 32.68GiB used 28.93GiB path /dev/xvda9\n\nBtrfs v3.14_pre20140414  The operation took about a minute on a cloud server and gained us 4GiB of space on the filesystem. It's up to you to find out what percentage works best for your workload, the speed of your disks, etc.  If your balance operation is taking a long time, you can open a new shell and find the status:  $ sudo btrfs balance status /\nBalance on '/' is running\n0 out of about 1 chunks balanced (1 considered), 100% left",
            "title": "No space left on device"
        },
        {
            "location": "/os/btrfs-troubleshooting/#adding-a-new-physical-disk",
            "text": "New physical disks can be added to an existing btrfs filesystem. The first step is to have the new block device  mounted on the machine . Afterwards, let btrfs know about the new device and re-balance the file system. The key step here is re-balancing, which will move the data and metadata across both block devices. Expect this process to take some time:  $ btrfs device add /dev/sdc /\n$ btrfs filesystem balance /",
            "title": "Adding a new physical disk"
        },
        {
            "location": "/os/btrfs-troubleshooting/#disable-copy-on-write",
            "text": "Copy-On-write isn't ideal for workloads that create or modify many small files, such as databases. Without disabling COW, you can heavily fragment the file system as explained above.  The best strategy for successfully running a database in a container is to disable COW on directory/volume that is mounted into the container.  The COW setting is stored as a file attribute and is modified with a utility called  chattr . To disable COW for a MySQL container's volume, run:  $ sudo mkdir /var/lib/mysql\n$ sudo chattr -R +C /var/lib/mysql  The directory  /var/lib/mysql  is now ready to be used by a Docker container without COW. Let's break down the command:  -R  indicates that want to recursively change the file attribute +C  means we want to set the NOCOW attribute on the file/directory  To verify, we can run:  $ sudo lsattr /var/lib/\n---------------- /var/lib/portage\n---------------- /var/lib/gentoo\n---------------- /var/lib/iptables\n---------------- /var/lib/ip6tables\n---------------- /var/lib/arpd\n---------------- /var/lib/ipset\n---------------- /var/lib/dbus\n---------------- /var/lib/systemd\n---------------- /var/lib/polkit-1\n---------------- /var/lib/dhcpcd\n---------------- /var/lib/ntp\n---------------- /var/lib/nfs\n---------------- /var/lib/etcd\n---------------- /var/lib/docker\n---------------- /var/lib/update_engine\n---------------C /var/lib/mysql",
            "title": "Disable copy-on-write"
        },
        {
            "location": "/os/btrfs-troubleshooting/#disable-via-a-unit-file",
            "text": "Setting the file attributes can be done via a systemd unit using two  ExecStartPre  commands:  ExecStartPre=/usr/bin/mkdir -p /var/lib/mysql\nExecStartPre=/usr/bin/chattr -R +C /var/lib/mysql",
            "title": "Disable via a unit file"
        },
        {
            "location": "/os/cluster-architectures/",
            "text": "Flatcar Container Linux cluster architectures\n\u00b6\n\n\nOverview\n\u00b6\n\n\nDepending on the size and expected use of your Flatcar Container Linux cluster, you will have different architectural requirements. A few of the common cluster architectures, as well as their strengths and weaknesses, are described below.\n\n\nMost of these scenarios dedicate a few machines, bare metal or virtual, to running central cluster services. These may include etcd and the distributed controllers for applications like Kubernetes, Mesos, and OpenStack. Isolating these services onto a few known machines helps to ensure they are distributed across cabinets or availability zones. It also helps in setting up static networking to allow for easy bootstrapping. This architecture helps to resolve concerns about relying on a discovery service.\n\n\nDocker dev environment on laptop\n\u00b6\n\n\n\n\nLaptop development environment with Flatcar Container Linux VM\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nLow\n\n\nLaptop development\n\n\nMinutes\n\n\nNo\n\n\n\n\n\n\n\n\nIf you're developing locally but plan to run containers in production, it's best practice to mirror that environment locally. Run Docker commands on your laptop that control a Flatcar Container Linux VM in VMware Fusion or Virtual box to mirror your container production environment locally.\n\n\nConfiguring your laptop\n\u00b6\n\n\nStart a single Flatcar Container Linux VM with the Docker remote socket enabled in the Container Linux Config (CL Config). Here's what the CL Config looks like:\n\n\nsystemd:\n  units:\n    - name: docker-tcp.socket\n      enable: yes\n      mask: false\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\n    - name: enable-docker-tcp.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable the Docker Socket for the API\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/systemctl enable docker-tcp.socket\n\n\n\nThis file is used to provision your local Flatcar Container Linux machine on its first boot. This sets up and enables the Docker API, which is how you can use Docker on your laptop. The Docker CLI manages containers running within the VM, \nnot\n on your personal operating system.\n\n\nUsing the CL Config Transpiler, or \nct\n, (\ndownload\n) convert the above yaml into an \nIgnition\n. Alternatively, copy the contents of the Igntion tab in the above example. Once you have the Ignition configuration file, pass it to your provider (\ncomplete list of supported Ignition platforms\n).\n\n\nOnce the local VM is running, tell your Docker binary on your personal operating system to use the remote port by exporting an environment variable and start running Docker commands. Run these commands in a terminal \non your local operating system (MacOS or Linux), not in the Flatcar Container Linux virtual machine\n:\n\n\n$ export DOCKER_HOST=tcp://localhost:2375\n$ docker ps\n\n\n\nThis avoids discrepancies between your development and production environments.\n\n\nRelated local installation tools\n\u00b6\n\n\nThere are several different options for testing Flatcar Container Linux locally:\n\n\n\n\nFlatcar Container Linux on QEMU\n is a feature rich way of running Flatcar Container Linux locally, provisioned by Ignition configs like the one shown above.\n\n\nMinikube\n is used for local Kubernetes development. This does not use Flatcar Container Linux but is very fast to setup and is the easiest way to test-drive use Kubernetes.\n\n\n\n\nSmall cluster\n\u00b6\n\n\n\n\nSmall Flatcar Container Linux cluster running etcd on all machines\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nLow\n\n\nSmall clusters, trying out Flatcar Container Linux\n\n\nMinutes\n\n\nYes\n\n\n\n\n\n\n\n\nFor small clusters, between 3-9 machines, running etcd on all of the machines allows for high availability without paying for extra machines that just run etcd.\n\n\nGetting started is easy \u2014 a single CL Config can be used to provision all machines in your environment.\n\n\nOnce you have a small cluster up and running, you can install a Kubernetes on the cluster. You can do this easily using \nTyphoon\n.\n\n\nConfiguring the machines\n\u00b6\n\n\nFor more information on getting started with this architecture, see the Flatcar Container Linux documentation on \nsupported platforms\n. These include \nAmazon EC2\n, \nOpenstack\n, \nAzure\n, \nGoogle Compute Platform\n, \nbare metal iPXE\n, \nDigital Ocean\n, and many more community supported platforms.\n\n\nBoot the desired number of machines with the same CL Config and discovery token. The CL Config specifies which services will be started on each machine.\n\n\nEasy development/testing cluster\n\u00b6\n\n\n\n\nFlatcar Container Linux cluster optimized for development and testing\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nLow\n\n\nDevelopment/Testing\n\n\nMinutes\n\n\nNo\n\n\n\n\n\n\n\n\nWhen getting started with Flatcar Container Linux, it's common to frequently boot, reboot, and destroy machines while tweaking your configuration. To avoid the need to generate new discovery URLs and bootstrap etcd, start a single etcd node, and build your cluster around it.\n\n\nYou can now boot as many machines as you'd like as test workers that read from the etcd node. All the features of Locksmith and etcdctl will continue to work properly but will connect to the etcd node instead of using a local etcd instance. Since etcd isn't running on all of the machines you'll gain a little bit of extra CPU and RAM to play with.\n\n\nYou can easily provision the remaining (non-etcd) nodes with Kubernetes using \nTyphoon\n to start running containerized app with your cluster.\n\n\nOnce this environment is set up, it's ready to be tested. Destroy a machine, and watch Kubernetes reschedule the units, max out the CPU, and rebuild your setup automatically.\n\n\nConfiguration for etcd role\n\u00b6\n\n\nSince we're only using a single etcd node, there is no need to include a discovery token. There isn't any high availability for etcd in this configuration, but that's assumed to be OK for development and testing. Boot this machine first so you can configure the rest with its IP address, which is specified with the networkd unit.\n\n\nThe networkd unit is typically used for bare metal installations that require static networking. See your provider's documentation for specific examples.\n\n\nHere's the CL Config for the etcd machine:\n\n\netcd:\n  version: 3.1.5\n  name: \"etcdserver\"\n  initial_cluster: \"etcdserver=http://10.0.0.101:2380\"\n  initial_advertise_peer_urls: \"http://10.0.0.101:2380\"\n  advertise_client_urls: \"http://10.0.0.101:2379\"\n  listen_client_urls: \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n  listen_peer_urls: \"http://0.0.0.0:2380\"\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=etcdserver\"\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n\n\n\nConfiguration for worker role\n\u00b6\n\n\nThis architecture allows you to boot any number of workers, from a single unit to a large cluster designed for load testing. The notable configuration difference for this role is specifying that applications like Kubernetes should use our etcd proxy instead of starting etcd server locally.\n\n\nProduction cluster with central services\n\u00b6\n\n\n\n\nFlatcar Container Linux cluster separated into central services and workers.\n\n\n\n\n\n\n\n\nCost\n\n\nGreat For\n\n\nSet Up Time\n\n\nProduction\n\n\n\n\n\n\n\n\n\n\nHigh\n\n\nLarge bare-metal installations\n\n\nHours\n\n\nYes\n\n\n\n\n\n\n\n\nFor large clusters, it's recommended to set aside 3-5 machines to run central services. Once those are set up, you can boot as many workers as you wish. Each of the workers will use your distributed etcd cluster on the central machines via local etcd proxies. This is explained in greater depth below.\n\n\nConfiguration for central services role\n\u00b6\n\n\nOur central services machines will run services like etcd and Kubernetes controllers that support the rest of the cluster. etcd is configured with static networking and a peers list.\n\n\nFlatcar Container Linux Support\n customers can also specify a \nCoreUpdate\n group ID which allows you to subscribe these machines to a different update channel, controlling updates separately from the worker machines.\n\n\nHere's an example CL Config for one of the central service machines. Be sure to generate a new discovery token with the initial size of your cluster:\n\n\netcd:\n  version: 3.0.15\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi-region and multi-cloud deployments must use $public_ipv4\n  advertise_client_urls: http://10.0.0.101:2379\n  initial_advertise_peer_urls: http://10.0.0.101:2380\n  listen_client_urls: http://0.0.0.0:2379\n  listen_peer_urls: http://10.0.0.101:2380\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Flatcar Container Linux cluster architectures"
        },
        {
            "location": "/os/cluster-architectures/#flatcar-container-linux-cluster-architectures",
            "text": "",
            "title": "Flatcar Container Linux cluster architectures"
        },
        {
            "location": "/os/cluster-architectures/#overview",
            "text": "Depending on the size and expected use of your Flatcar Container Linux cluster, you will have different architectural requirements. A few of the common cluster architectures, as well as their strengths and weaknesses, are described below.  Most of these scenarios dedicate a few machines, bare metal or virtual, to running central cluster services. These may include etcd and the distributed controllers for applications like Kubernetes, Mesos, and OpenStack. Isolating these services onto a few known machines helps to ensure they are distributed across cabinets or availability zones. It also helps in setting up static networking to allow for easy bootstrapping. This architecture helps to resolve concerns about relying on a discovery service.",
            "title": "Overview"
        },
        {
            "location": "/os/cluster-architectures/#docker-dev-environment-on-laptop",
            "text": "Laptop development environment with Flatcar Container Linux VM     Cost  Great For  Set Up Time  Production      Low  Laptop development  Minutes  No     If you're developing locally but plan to run containers in production, it's best practice to mirror that environment locally. Run Docker commands on your laptop that control a Flatcar Container Linux VM in VMware Fusion or Virtual box to mirror your container production environment locally.",
            "title": "Docker dev environment on laptop"
        },
        {
            "location": "/os/cluster-architectures/#configuring-your-laptop",
            "text": "Start a single Flatcar Container Linux VM with the Docker remote socket enabled in the Container Linux Config (CL Config). Here's what the CL Config looks like:  systemd:\n  units:\n    - name: docker-tcp.socket\n      enable: yes\n      mask: false\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\n    - name: enable-docker-tcp.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable the Docker Socket for the API\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/systemctl enable docker-tcp.socket  This file is used to provision your local Flatcar Container Linux machine on its first boot. This sets up and enables the Docker API, which is how you can use Docker on your laptop. The Docker CLI manages containers running within the VM,  not  on your personal operating system.  Using the CL Config Transpiler, or  ct , ( download ) convert the above yaml into an  Ignition . Alternatively, copy the contents of the Igntion tab in the above example. Once you have the Ignition configuration file, pass it to your provider ( complete list of supported Ignition platforms ).  Once the local VM is running, tell your Docker binary on your personal operating system to use the remote port by exporting an environment variable and start running Docker commands. Run these commands in a terminal  on your local operating system (MacOS or Linux), not in the Flatcar Container Linux virtual machine :  $ export DOCKER_HOST=tcp://localhost:2375\n$ docker ps  This avoids discrepancies between your development and production environments.",
            "title": "Configuring your laptop"
        },
        {
            "location": "/os/cluster-architectures/#related-local-installation-tools",
            "text": "There are several different options for testing Flatcar Container Linux locally:   Flatcar Container Linux on QEMU  is a feature rich way of running Flatcar Container Linux locally, provisioned by Ignition configs like the one shown above.  Minikube  is used for local Kubernetes development. This does not use Flatcar Container Linux but is very fast to setup and is the easiest way to test-drive use Kubernetes.",
            "title": "Related local installation tools"
        },
        {
            "location": "/os/cluster-architectures/#small-cluster",
            "text": "Small Flatcar Container Linux cluster running etcd on all machines     Cost  Great For  Set Up Time  Production      Low  Small clusters, trying out Flatcar Container Linux  Minutes  Yes     For small clusters, between 3-9 machines, running etcd on all of the machines allows for high availability without paying for extra machines that just run etcd.  Getting started is easy \u2014 a single CL Config can be used to provision all machines in your environment.  Once you have a small cluster up and running, you can install a Kubernetes on the cluster. You can do this easily using  Typhoon .",
            "title": "Small cluster"
        },
        {
            "location": "/os/cluster-architectures/#configuring-the-machines",
            "text": "For more information on getting started with this architecture, see the Flatcar Container Linux documentation on  supported platforms . These include  Amazon EC2 ,  Openstack ,  Azure ,  Google Compute Platform ,  bare metal iPXE ,  Digital Ocean , and many more community supported platforms.  Boot the desired number of machines with the same CL Config and discovery token. The CL Config specifies which services will be started on each machine.",
            "title": "Configuring the machines"
        },
        {
            "location": "/os/cluster-architectures/#easy-developmenttesting-cluster",
            "text": "Flatcar Container Linux cluster optimized for development and testing     Cost  Great For  Set Up Time  Production      Low  Development/Testing  Minutes  No     When getting started with Flatcar Container Linux, it's common to frequently boot, reboot, and destroy machines while tweaking your configuration. To avoid the need to generate new discovery URLs and bootstrap etcd, start a single etcd node, and build your cluster around it.  You can now boot as many machines as you'd like as test workers that read from the etcd node. All the features of Locksmith and etcdctl will continue to work properly but will connect to the etcd node instead of using a local etcd instance. Since etcd isn't running on all of the machines you'll gain a little bit of extra CPU and RAM to play with.  You can easily provision the remaining (non-etcd) nodes with Kubernetes using  Typhoon  to start running containerized app with your cluster.  Once this environment is set up, it's ready to be tested. Destroy a machine, and watch Kubernetes reschedule the units, max out the CPU, and rebuild your setup automatically.",
            "title": "Easy development/testing cluster"
        },
        {
            "location": "/os/cluster-architectures/#configuration-for-etcd-role",
            "text": "Since we're only using a single etcd node, there is no need to include a discovery token. There isn't any high availability for etcd in this configuration, but that's assumed to be OK for development and testing. Boot this machine first so you can configure the rest with its IP address, which is specified with the networkd unit.  The networkd unit is typically used for bare metal installations that require static networking. See your provider's documentation for specific examples.  Here's the CL Config for the etcd machine:  etcd:\n  version: 3.1.5\n  name: \"etcdserver\"\n  initial_cluster: \"etcdserver=http://10.0.0.101:2380\"\n  initial_advertise_peer_urls: \"http://10.0.0.101:2380\"\n  advertise_client_urls: \"http://10.0.0.101:2379\"\n  listen_client_urls: \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n  listen_peer_urls: \"http://0.0.0.0:2380\"\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\n      dropins:\n        - name: conf1.conf\n          contents: |\n            [Service]\n            Environment=\"ETCD_NAME=etcdserver\"\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Configuration for etcd role"
        },
        {
            "location": "/os/cluster-architectures/#configuration-for-worker-role",
            "text": "This architecture allows you to boot any number of workers, from a single unit to a large cluster designed for load testing. The notable configuration difference for this role is specifying that applications like Kubernetes should use our etcd proxy instead of starting etcd server locally.",
            "title": "Configuration for worker role"
        },
        {
            "location": "/os/cluster-architectures/#production-cluster-with-central-services",
            "text": "Flatcar Container Linux cluster separated into central services and workers.     Cost  Great For  Set Up Time  Production      High  Large bare-metal installations  Hours  Yes     For large clusters, it's recommended to set aside 3-5 machines to run central services. Once those are set up, you can boot as many workers as you wish. Each of the workers will use your distributed etcd cluster on the central machines via local etcd proxies. This is explained in greater depth below.",
            "title": "Production cluster with central services"
        },
        {
            "location": "/os/cluster-architectures/#configuration-for-central-services-role",
            "text": "Our central services machines will run services like etcd and Kubernetes controllers that support the rest of the cluster. etcd is configured with static networking and a peers list.  Flatcar Container Linux Support  customers can also specify a  CoreUpdate  group ID which allows you to subscribe these machines to a different update channel, controlling updates separately from the worker machines.  Here's an example CL Config for one of the central service machines. Be sure to generate a new discovery token with the initial size of your cluster:  etcd:\n  version: 3.0.15\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi-region and multi-cloud deployments must use $public_ipv4\n  advertise_client_urls: http://10.0.0.101:2379\n  initial_advertise_peer_urls: http://10.0.0.101:2380\n  listen_client_urls: http://0.0.0.0:2379\n  listen_peer_urls: http://10.0.0.101:2380\nsystemd:\n  units:\n    - name: etcd-member.service\n      enable: true\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Configuration for central services role"
        },
        {
            "location": "/os/cluster-discovery/",
            "text": "Flatcar Container Linux cluster discovery\n\u00b6\n\n\nOverview\n\u00b6\n\n\nFlatcar Container Linux uses etcd, a service running on each machine, to handle coordination between software running on the cluster. For a group of Flatcar Container Linux machines to form a cluster, their etcd instances need to be connected.\n\n\nA discovery service, \nhttps://discovery.etcd.io\n, is provided as a free service to help connect etcd instances together by storing a list of peer addresses, metadata and the initial size of the cluster under a unique address, known as the discovery URL. You can generate them very easily:\n\n\n$ curl -w \"\\n\" 'https://discovery.etcd.io/new?size=3'\nhttps://discovery.etcd.io/6a28e078895c5ec737174db2419bb2f3\n\n\n\nThe discovery URL can be provided to each Flatcar Container Linux machine via \nContainer Linux Configs\n. The rest of this guide will explain what's happening behind the scenes, but if you're trying to get clustered as quickly as possible, all you need to do is provide a \nfresh, unique\n discovery token in your config.\n\n\nBoot each one of the machines with identical Container Linux Config and they should be automatically clustered:\n\n\netcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls: http://{PRIVATE_IPV4}:2379,http://{PRIVATE_IPV4}:4001\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://{PRIVATE_IPV4}:2380\n\n\n\nSpecific documentation are provided for each platform's guide. Not all providers support the \n{PRIVATE_IPV4}\n variable substitution.\n\n\nNew clusters\n\u00b6\n\n\nStarting a Flatcar Container Linux cluster requires one of the new machines to become the first leader of the cluster. The initial leader is stored as metadata with the discovery URL in order to inform the other members of the new cluster. Let's walk through a timeline a new three-machine Flatcar Container Linux cluster discovering each other:\n\n\n\n\nAll three machines are booted via a cloud-provider with the same config in the user-data.\n\n\nMachine 1 starts up first. It requests information about the cluster from the discovery token and submits its \n-initial-advertise-peer-urls\n address \n10.10.10.1\n.\n\n\nNo state is recorded into the discovery URL metadata, so machine 1 becomes the leader and records the state as \nstarted\n.\n\n\nMachine 2 boots and submits its \n-initial-advertise-peer-urls\n address \n10.10.10.2\n. It also reads back the list of existing peers (only \n10.10.10.1\n) and attempts to connect to the address listed.\n\n\nMachine 2 connects to Machine 1 and is now part of the cluster as a follower.\n\n\nMachine 3 boots and submits its \n-initial-advertise-peer-urls\n address \n10.10.10.3\n. It reads back the list of peers (\n10.10.10.1\n and \n10.10.10.2\n) and selects one of the addresses to try first. When it connects to a machine in the cluster, the machine is given a full list of the existing other members of the cluster.\n\n\nThe cluster is now bootstrapped with an initial leader and two followers.\n\n\n\n\nThere are a few interesting things happening during this process.\n\n\nFirst, each machine is configured with the same discovery URL and etcd figured out what to do. This allows you to load the same Container Linux Config into an auto-scaling group and it will work whether it is the first or 30\nth\n machine in the group.\n\n\nSecond, machine 3 only needed to use one of the addresses stored in the discovery URL to connect to the cluster. Since etcd uses the Raft consensus algorithm, existing machines in the cluster already maintain a list of healthy members in order for the algorithm to function properly. This list is given to the new machine and it starts normal operations with each of the other cluster members.\n\n\nThird, if you specified \n?size=3\n upon discovery URL creation, any other machines that join the cluster in the future will automatically start as etcd proxies.\n\n\nCommon problems with cluster discovery\n\u00b6\n\n\nExisting clusters\n\u00b6\n\n\nDo not use the public discovery service to reconfigure a running etcd cluster.\n The public discovery service is a convenience for bootstrapping new clusters, especially on cloud providers with dynamic IP assignment, but is not designed for the later case when the cluster is running and member IPs are known.\n\n\nTo promote proxy members or join new members into an existing etcd cluster, configure static discovery and add members. The \netcd cluster reconfiguration guide\n details the steps for performing this reconfiguration on Flatcar Container Linux systems that were originally deployed with public discovery. The more general \netcd cluster reconfiguration document\n explains the operations for removing and adding cluster members in a cluster already configured with static discovery.\n\n\nStale tokens\n\u00b6\n\n\nA common problem with cluster discovery is attempting to boot a new cluster with a stale discovery URL. As explained above, the initial leader election is recorded into the URL, which indicates that the new etcd instance should be joining an existing cluster.\n\n\nIf you provide a stale discovery URL, the new machines will attempt to connect to each of the old peer addresses, which will fail since they don't exist, and the bootstrapping process will fail.\n\n\nIf you're thinking, why can't the new machines just form a new cluster if they're all down. There's a really great reason for this \u2014 if an etcd peer was in a network partition, it would look exactly like the \"full-down\" situation and starting a new cluster would form a split-brain. Since etcd will never be able to determine whether a token has been reused or not, it must assume the worst and abort the cluster discovery.\n\n\nIf you're running into problems with your discovery URL, there are a few sources of information that can help you see what's going on. First, you can open the URL in a browser to see what information etcd is using to bootstrap itself:\n\n\n{\n  action: \"get\",\n  node: {\n    key: \"/_etcd/registry/506f6c1bc729377252232a0121247119\",\n    dir: true,\n    nodes: [\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/0d79b4791be9688332cc05367366551e\",\n        value: \"http://10.183.202.105:7001\",\n        expiration: \"2014-08-17T16:21:37.426001686Z\",\n        ttl: 576008,\n        modifiedIndex: 72783864,\n        createdIndex: 72783864\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/c72c63ffce6680737ea2b670456aaacd\",\n        value: \"http://10.65.177.56:7001\",\n        expiration: \"2014-08-17T12:05:57.717243529Z\",\n        ttl: 560669,\n        modifiedIndex: 72626400,\n        createdIndex: 72626400\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/f7a93d1f0cd4d318c9ad0b624afb9cf9\",\n        value: \"http://10.29.193.50:7001\",\n        expiration: \"2014-08-17T17:18:25.045563473Z\",\n        ttl: 579416,\n        modifiedIndex: 72821950,\n        createdIndex: 72821950\n      }\n    ],\n    modifiedIndex: 69367741,\n    createdIndex: 69367741\n  }\n}\n\n\n\nTo rule out firewall settings as a source of your issue, ensure that you can curl each of the IPs from machines in your cluster.\n\n\nIf all of the IPs can be reached, the etcd log can provide more clues:\n\n\njournalctl -u etcd-member\n\n\n\nCommunicating with discovery.etcd.io\n\u00b6\n\n\nIf your Flatcar Container Linux cluster can't communicate out to the public internet, \nhttps://discovery.etcd.io\n won't work and you'll have to run your own discovery endpoint, which is described below.\n\n\nSetting advertised client addresses correctly\n\u00b6\n\n\nEach etcd instance submits the list of \n-initial-advertise-peer-urls\n of each etcd instance to the configured discovery service. It's important to select an address that \nall\n peers in the cluster can communicate with. If you are configuring a list of addresses, make sure each member can communicate with at least one of the addresses.\n\n\nFor example, if you're located in two regions of a cloud provider, configuring a private \n10.x\n address will not work between the two regions, and communication will not be possible between all peers. The \n-listen-client-urls\n flag allows you to bind to a specific list of interfaces and ports (or all interfaces) to ensure your etcd traffic is routed properly.\n\n\nRunning your own discovery service\n\u00b6\n\n\nThe public discovery service is just an etcd cluster made available to the public internet. Since the discovery service conducts and stores the result of the first leader election, it needs to be consistent. You wouldn't want two machines in the same cluster to think they were both the leader.\n\n\nSince etcd is designed to this type of leader election, it was an obvious choice to use it for everyone's initial leader election. This means that it's easy to run your own etcd cluster for this purpose.\n\n\nIf you're interested in how discovery API works behind the scenes in etcd, read about \netcd clustering\n.",
            "title": "Flatcar Container Linux cluster discovery"
        },
        {
            "location": "/os/cluster-discovery/#flatcar-container-linux-cluster-discovery",
            "text": "",
            "title": "Flatcar Container Linux cluster discovery"
        },
        {
            "location": "/os/cluster-discovery/#overview",
            "text": "Flatcar Container Linux uses etcd, a service running on each machine, to handle coordination between software running on the cluster. For a group of Flatcar Container Linux machines to form a cluster, their etcd instances need to be connected.  A discovery service,  https://discovery.etcd.io , is provided as a free service to help connect etcd instances together by storing a list of peer addresses, metadata and the initial size of the cluster under a unique address, known as the discovery URL. You can generate them very easily:  $ curl -w \"\\n\" 'https://discovery.etcd.io/new?size=3'\nhttps://discovery.etcd.io/6a28e078895c5ec737174db2419bb2f3  The discovery URL can be provided to each Flatcar Container Linux machine via  Container Linux Configs . The rest of this guide will explain what's happening behind the scenes, but if you're trying to get clustered as quickly as possible, all you need to do is provide a  fresh, unique  discovery token in your config.  Boot each one of the machines with identical Container Linux Config and they should be automatically clustered:  etcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  # multi_region and multi_cloud deployments need to use {PUBLIC_IPV4}\n  advertise_client_urls: http://{PRIVATE_IPV4}:2379,http://{PRIVATE_IPV4}:4001\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://{PRIVATE_IPV4}:2380  Specific documentation are provided for each platform's guide. Not all providers support the  {PRIVATE_IPV4}  variable substitution.",
            "title": "Overview"
        },
        {
            "location": "/os/cluster-discovery/#new-clusters",
            "text": "Starting a Flatcar Container Linux cluster requires one of the new machines to become the first leader of the cluster. The initial leader is stored as metadata with the discovery URL in order to inform the other members of the new cluster. Let's walk through a timeline a new three-machine Flatcar Container Linux cluster discovering each other:   All three machines are booted via a cloud-provider with the same config in the user-data.  Machine 1 starts up first. It requests information about the cluster from the discovery token and submits its  -initial-advertise-peer-urls  address  10.10.10.1 .  No state is recorded into the discovery URL metadata, so machine 1 becomes the leader and records the state as  started .  Machine 2 boots and submits its  -initial-advertise-peer-urls  address  10.10.10.2 . It also reads back the list of existing peers (only  10.10.10.1 ) and attempts to connect to the address listed.  Machine 2 connects to Machine 1 and is now part of the cluster as a follower.  Machine 3 boots and submits its  -initial-advertise-peer-urls  address  10.10.10.3 . It reads back the list of peers ( 10.10.10.1  and  10.10.10.2 ) and selects one of the addresses to try first. When it connects to a machine in the cluster, the machine is given a full list of the existing other members of the cluster.  The cluster is now bootstrapped with an initial leader and two followers.   There are a few interesting things happening during this process.  First, each machine is configured with the same discovery URL and etcd figured out what to do. This allows you to load the same Container Linux Config into an auto-scaling group and it will work whether it is the first or 30 th  machine in the group.  Second, machine 3 only needed to use one of the addresses stored in the discovery URL to connect to the cluster. Since etcd uses the Raft consensus algorithm, existing machines in the cluster already maintain a list of healthy members in order for the algorithm to function properly. This list is given to the new machine and it starts normal operations with each of the other cluster members.  Third, if you specified  ?size=3  upon discovery URL creation, any other machines that join the cluster in the future will automatically start as etcd proxies.",
            "title": "New clusters"
        },
        {
            "location": "/os/cluster-discovery/#common-problems-with-cluster-discovery",
            "text": "",
            "title": "Common problems with cluster discovery"
        },
        {
            "location": "/os/cluster-discovery/#existing-clusters",
            "text": "Do not use the public discovery service to reconfigure a running etcd cluster.  The public discovery service is a convenience for bootstrapping new clusters, especially on cloud providers with dynamic IP assignment, but is not designed for the later case when the cluster is running and member IPs are known.  To promote proxy members or join new members into an existing etcd cluster, configure static discovery and add members. The  etcd cluster reconfiguration guide  details the steps for performing this reconfiguration on Flatcar Container Linux systems that were originally deployed with public discovery. The more general  etcd cluster reconfiguration document  explains the operations for removing and adding cluster members in a cluster already configured with static discovery.",
            "title": "Existing clusters"
        },
        {
            "location": "/os/cluster-discovery/#stale-tokens",
            "text": "A common problem with cluster discovery is attempting to boot a new cluster with a stale discovery URL. As explained above, the initial leader election is recorded into the URL, which indicates that the new etcd instance should be joining an existing cluster.  If you provide a stale discovery URL, the new machines will attempt to connect to each of the old peer addresses, which will fail since they don't exist, and the bootstrapping process will fail.  If you're thinking, why can't the new machines just form a new cluster if they're all down. There's a really great reason for this \u2014 if an etcd peer was in a network partition, it would look exactly like the \"full-down\" situation and starting a new cluster would form a split-brain. Since etcd will never be able to determine whether a token has been reused or not, it must assume the worst and abort the cluster discovery.  If you're running into problems with your discovery URL, there are a few sources of information that can help you see what's going on. First, you can open the URL in a browser to see what information etcd is using to bootstrap itself:  {\n  action: \"get\",\n  node: {\n    key: \"/_etcd/registry/506f6c1bc729377252232a0121247119\",\n    dir: true,\n    nodes: [\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/0d79b4791be9688332cc05367366551e\",\n        value: \"http://10.183.202.105:7001\",\n        expiration: \"2014-08-17T16:21:37.426001686Z\",\n        ttl: 576008,\n        modifiedIndex: 72783864,\n        createdIndex: 72783864\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/c72c63ffce6680737ea2b670456aaacd\",\n        value: \"http://10.65.177.56:7001\",\n        expiration: \"2014-08-17T12:05:57.717243529Z\",\n        ttl: 560669,\n        modifiedIndex: 72626400,\n        createdIndex: 72626400\n      },\n      {\n        key: \"/_etcd/registry/506f6c1bc729377252232a0121247119/f7a93d1f0cd4d318c9ad0b624afb9cf9\",\n        value: \"http://10.29.193.50:7001\",\n        expiration: \"2014-08-17T17:18:25.045563473Z\",\n        ttl: 579416,\n        modifiedIndex: 72821950,\n        createdIndex: 72821950\n      }\n    ],\n    modifiedIndex: 69367741,\n    createdIndex: 69367741\n  }\n}  To rule out firewall settings as a source of your issue, ensure that you can curl each of the IPs from machines in your cluster.  If all of the IPs can be reached, the etcd log can provide more clues:  journalctl -u etcd-member",
            "title": "Stale tokens"
        },
        {
            "location": "/os/cluster-discovery/#communicating-with-discoveryetcdio",
            "text": "If your Flatcar Container Linux cluster can't communicate out to the public internet,  https://discovery.etcd.io  won't work and you'll have to run your own discovery endpoint, which is described below.",
            "title": "Communicating with discovery.etcd.io"
        },
        {
            "location": "/os/cluster-discovery/#setting-advertised-client-addresses-correctly",
            "text": "Each etcd instance submits the list of  -initial-advertise-peer-urls  of each etcd instance to the configured discovery service. It's important to select an address that  all  peers in the cluster can communicate with. If you are configuring a list of addresses, make sure each member can communicate with at least one of the addresses.  For example, if you're located in two regions of a cloud provider, configuring a private  10.x  address will not work between the two regions, and communication will not be possible between all peers. The  -listen-client-urls  flag allows you to bind to a specific list of interfaces and ports (or all interfaces) to ensure your etcd traffic is routed properly.",
            "title": "Setting advertised client addresses correctly"
        },
        {
            "location": "/os/cluster-discovery/#running-your-own-discovery-service",
            "text": "The public discovery service is just an etcd cluster made available to the public internet. Since the discovery service conducts and stores the result of the first leader election, it needs to be consistent. You wouldn't want two machines in the same cluster to think they were both the leader.  Since etcd is designed to this type of leader election, it was an obvious choice to use it for everyone's initial leader election. This means that it's easy to run your own etcd cluster for this purpose.  If you're interested in how discovery API works behind the scenes in etcd, read about  etcd clustering .",
            "title": "Running your own discovery service"
        },
        {
            "location": "/os/collecting-crash-logs/",
            "text": "Collecting crash logs\n\u00b6\n\n\nIn the unfortunate case that an OS crashes, it's often extremely helpful to gather information about the event. There are two popular tools used to accomplished this goal: kdump and pstore. Flatcar Container Linux relies on pstore, a persistent storage abstraction provided by the Linux kernel, to store logs in the event of a kernel panic. Since this mechanism is just an abstraction, it depends on hardware support to actually persist the data across reboots. If the hardware support is absent, the pstore will remain empty. On AMD64 machines, pstore is typically backed by the ACPI error record serialization table (ERST).\n\n\nUsing pstore\n\u00b6\n\n\nOn Flatcar Container Linux, the pstore is automatically mounted to \n/sys/fs/pstore\n. The contents of the store can be explored using standard filesystem tools:\n\n\n$ ls /sys/fs/pstore/\n\n\n\nOn this particular machine, there isn't anything in the pstore yet. In order to test the mechanism, a kernel panic can be triggered:\n\n\n$ echo c > /proc/sysrq-trigger\n\n\n\nOnce the machine boots, the pstore can again be inspected:\n\n\n$ ls /sys/fs/pstore/\ndmesg-erst-6319986351055831041  dmesg-erst-6319986351055831044\ndmesg-erst-6319986351055831042  dmesg-erst-6319986351055831045\ndmesg-erst-6319986351055831043\n\n\n\nNow there are a series of dmesg logs, stored in the ACPI ERST. Looking at the first file, the cause of the panic can be discovered:\n\n\n$ cat /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n...\n<6>[  201.650687] sysrq: SysRq : Trigger a crash\n<1>[  201.654822] BUG: unable to handle kernel NULL pointer dereference at           (null)\n<1>[  201.662670] IP: [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.668783] PGD 0 \n<4>[  201.670809] Oops: 0002 [#1] SMP\n<4>[  201.673948] Modules linked in: coretemp sb_edac edac_core x86_pkg_temp_thermal kvm_intel ipmi_ssif kvm mei_me irqbypass i2c_i801 mousedev evdev mei ipmi_si ipmi_msghandler tpm_tis button tpm sch_fq_codel ip_tables hid_generic usbhid hid sd_mod squashfs loop igb ahci xhci_pci ehci_pci i2c_algo_bit libahci xhci_hcd ehci_hcd i2c_core libata i40e hwmon usbcore ptp crc32c_intel scsi_mod usb_common pps_core dm_mirror dm_region_hash dm_log dm_mod autofs4\n<4>[  201.714354] CPU: 0 PID: 1899 Comm: bash Not tainted 4.7.0-coreos #1\n<4>[  201.720612] Hardware name: Supermicro SYS-F618R3-FT/X10DRFF, BIOS 1.0b 01/07/2015\n<4>[  201.728083] task: ffff881fdca79d40 ti: ffff881fd92d0000 task.ti: ffff881fd92d0000\n<4>[  201.735553] RIP: 0010:[<ffffffffbd3d1956>]  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.744083] RSP: 0018:ffff881fd92d3d98  EFLAGS: 00010286\n<4>[  201.749388] RAX: 000000000000000f RBX: 0000000000000063 RCX: 0000000000000000\n<4>[  201.756511] RDX: 0000000000000000 RSI: ffff881fff80dbc8 RDI: 0000000000000063\n<4>[  201.763635] RBP: ffff881fd92d3d98 R08: ffff88407ff57b80 R09: 00000000000000c2\n<4>[  201.770759] R10: ffff881fe4fab624 R11: 00000000000005dd R12: 0000000000000007\n<4>[  201.777885] R13: 0000000000000000 R14: ffffffffbdac37a0 R15: 0000000000000000\n<4>[  201.785009] FS:  00007fa68acee700(0000) GS:ffff881fff800000(0000) knlGS:0000000000000000\n<4>[  201.793085] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n<4>[  201.798825] CR2: 0000000000000000 CR3: 0000001fdcc97000 CR4: 00000000001406f0\n<4>[  201.805949] Stack:\n<4>[  201.807961]  ffff881fd92d3dc8 ffffffffbd3d2146 0000000000000002 fffffffffffffffb\n<4>[  201.815413]  00007fa68acf6000 ffff883fe2e46f00 ffff881fd92d3de0 ffffffffbd3d259f\n<4>[  201.822866]  ffff881fe4fab5c0 ffff881fd92d3e00 ffffffffbd24fda8 ffff883fe2e46f00\n<4>[  201.830320] Call Trace:\n<4>[  201.832769]  [<ffffffffbd3d2146>] __handle_sysrq+0xf6/0x150\n<4>[  201.838331]  [<ffffffffbd3d259f>] write_sysrq_trigger+0x2f/0x40\n<4>[  201.844244]  [<ffffffffbd24fda8>] proc_reg_write+0x48/0x70\n<4>[  201.849723]  [<ffffffffbd1e4697>] __vfs_write+0x37/0x140\n<4>[  201.855038]  [<ffffffffbd283e0d>] ? security_file_permission+0x3d/0xc0\n<4>[  201.861561]  [<ffffffffbd0c1062>] ? percpu_down_read+0x12/0x60\n<4>[  201.867383]  [<ffffffffbd1e55b8>] vfs_write+0xb8/0x1a0\n<4>[  201.872514]  [<ffffffffbd1e6a25>] SyS_write+0x55/0xc0\n<4>[  201.877562]  [<ffffffffbd003c6d>] do_syscall_64+0x5d/0x150\n<4>[  201.883047]  [<ffffffffbd58e161>] entry_SYSCALL64_slow_path+0x25/0x25\n<4>[  201.889474] Code: df ff 48 c7 c7 f3 a3 7d bd e8 47 c5 d3 ff e9 de fe ff ff 66 90 0f 1f 44 00 00 55 c7 05 48 b4 66 00 01 00 00 00 48 89 e5 0f ae f8 <c6> 04 25 00 00 00 00 01 5d c3 0f 1f 44 00 00 55 31 c0 c7 05 5e \n<1>[  201.909425] RIP  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.915615]  RSP <ffff881fd92d3d98>\n<4>[  201.919097] CR2: 0000000000000000\n<4>[  201.922450] ---[ end trace 8794939ba0598b91 ]---\n\n\n\nThe cause of the panic was a system request! The remaining files in the pstore contain more of the logs leading up to the panic as well as more context. Each of the files has a small, descriptive header describing the source of the logs. Looking at each of the headers shows the rough structure of the logs:\n\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831042\nOops#1 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831043\nPanic#2 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831044\nPanic#2 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831045\nPanic#2 Part3\n\n\n\nIt is important to note that the pstore typically has very limited storage space (on the order of kilobytes) and will not overwrite entries when out of space. The files in \n/sys/fs/pstore\n must be removed to free up space. The typical approach is to move the files from the pstore to a more permanent storage location on boot, but Flatcar Container Linux will not do this automatically for you.",
            "title": "Collecting crash logs"
        },
        {
            "location": "/os/collecting-crash-logs/#collecting-crash-logs",
            "text": "In the unfortunate case that an OS crashes, it's often extremely helpful to gather information about the event. There are two popular tools used to accomplished this goal: kdump and pstore. Flatcar Container Linux relies on pstore, a persistent storage abstraction provided by the Linux kernel, to store logs in the event of a kernel panic. Since this mechanism is just an abstraction, it depends on hardware support to actually persist the data across reboots. If the hardware support is absent, the pstore will remain empty. On AMD64 machines, pstore is typically backed by the ACPI error record serialization table (ERST).",
            "title": "Collecting crash logs"
        },
        {
            "location": "/os/collecting-crash-logs/#using-pstore",
            "text": "On Flatcar Container Linux, the pstore is automatically mounted to  /sys/fs/pstore . The contents of the store can be explored using standard filesystem tools:  $ ls /sys/fs/pstore/  On this particular machine, there isn't anything in the pstore yet. In order to test the mechanism, a kernel panic can be triggered:  $ echo c > /proc/sysrq-trigger  Once the machine boots, the pstore can again be inspected:  $ ls /sys/fs/pstore/\ndmesg-erst-6319986351055831041  dmesg-erst-6319986351055831044\ndmesg-erst-6319986351055831042  dmesg-erst-6319986351055831045\ndmesg-erst-6319986351055831043  Now there are a series of dmesg logs, stored in the ACPI ERST. Looking at the first file, the cause of the panic can be discovered:  $ cat /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n...\n<6>[  201.650687] sysrq: SysRq : Trigger a crash\n<1>[  201.654822] BUG: unable to handle kernel NULL pointer dereference at           (null)\n<1>[  201.662670] IP: [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.668783] PGD 0 \n<4>[  201.670809] Oops: 0002 [#1] SMP\n<4>[  201.673948] Modules linked in: coretemp sb_edac edac_core x86_pkg_temp_thermal kvm_intel ipmi_ssif kvm mei_me irqbypass i2c_i801 mousedev evdev mei ipmi_si ipmi_msghandler tpm_tis button tpm sch_fq_codel ip_tables hid_generic usbhid hid sd_mod squashfs loop igb ahci xhci_pci ehci_pci i2c_algo_bit libahci xhci_hcd ehci_hcd i2c_core libata i40e hwmon usbcore ptp crc32c_intel scsi_mod usb_common pps_core dm_mirror dm_region_hash dm_log dm_mod autofs4\n<4>[  201.714354] CPU: 0 PID: 1899 Comm: bash Not tainted 4.7.0-coreos #1\n<4>[  201.720612] Hardware name: Supermicro SYS-F618R3-FT/X10DRFF, BIOS 1.0b 01/07/2015\n<4>[  201.728083] task: ffff881fdca79d40 ti: ffff881fd92d0000 task.ti: ffff881fd92d0000\n<4>[  201.735553] RIP: 0010:[<ffffffffbd3d1956>]  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.744083] RSP: 0018:ffff881fd92d3d98  EFLAGS: 00010286\n<4>[  201.749388] RAX: 000000000000000f RBX: 0000000000000063 RCX: 0000000000000000\n<4>[  201.756511] RDX: 0000000000000000 RSI: ffff881fff80dbc8 RDI: 0000000000000063\n<4>[  201.763635] RBP: ffff881fd92d3d98 R08: ffff88407ff57b80 R09: 00000000000000c2\n<4>[  201.770759] R10: ffff881fe4fab624 R11: 00000000000005dd R12: 0000000000000007\n<4>[  201.777885] R13: 0000000000000000 R14: ffffffffbdac37a0 R15: 0000000000000000\n<4>[  201.785009] FS:  00007fa68acee700(0000) GS:ffff881fff800000(0000) knlGS:0000000000000000\n<4>[  201.793085] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n<4>[  201.798825] CR2: 0000000000000000 CR3: 0000001fdcc97000 CR4: 00000000001406f0\n<4>[  201.805949] Stack:\n<4>[  201.807961]  ffff881fd92d3dc8 ffffffffbd3d2146 0000000000000002 fffffffffffffffb\n<4>[  201.815413]  00007fa68acf6000 ffff883fe2e46f00 ffff881fd92d3de0 ffffffffbd3d259f\n<4>[  201.822866]  ffff881fe4fab5c0 ffff881fd92d3e00 ffffffffbd24fda8 ffff883fe2e46f00\n<4>[  201.830320] Call Trace:\n<4>[  201.832769]  [<ffffffffbd3d2146>] __handle_sysrq+0xf6/0x150\n<4>[  201.838331]  [<ffffffffbd3d259f>] write_sysrq_trigger+0x2f/0x40\n<4>[  201.844244]  [<ffffffffbd24fda8>] proc_reg_write+0x48/0x70\n<4>[  201.849723]  [<ffffffffbd1e4697>] __vfs_write+0x37/0x140\n<4>[  201.855038]  [<ffffffffbd283e0d>] ? security_file_permission+0x3d/0xc0\n<4>[  201.861561]  [<ffffffffbd0c1062>] ? percpu_down_read+0x12/0x60\n<4>[  201.867383]  [<ffffffffbd1e55b8>] vfs_write+0xb8/0x1a0\n<4>[  201.872514]  [<ffffffffbd1e6a25>] SyS_write+0x55/0xc0\n<4>[  201.877562]  [<ffffffffbd003c6d>] do_syscall_64+0x5d/0x150\n<4>[  201.883047]  [<ffffffffbd58e161>] entry_SYSCALL64_slow_path+0x25/0x25\n<4>[  201.889474] Code: df ff 48 c7 c7 f3 a3 7d bd e8 47 c5 d3 ff e9 de fe ff ff 66 90 0f 1f 44 00 00 55 c7 05 48 b4 66 00 01 00 00 00 48 89 e5 0f ae f8 <c6> 04 25 00 00 00 00 01 5d c3 0f 1f 44 00 00 55 31 c0 c7 05 5e \n<1>[  201.909425] RIP  [<ffffffffbd3d1956>] sysrq_handle_crash+0x16/0x20\n<4>[  201.915615]  RSP <ffff881fd92d3d98>\n<4>[  201.919097] CR2: 0000000000000000\n<4>[  201.922450] ---[ end trace 8794939ba0598b91 ]---  The cause of the panic was a system request! The remaining files in the pstore contain more of the logs leading up to the panic as well as more context. Each of the files has a small, descriptive header describing the source of the logs. Looking at each of the headers shows the rough structure of the logs:  $ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831041\nOops#1 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831042\nOops#1 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831043\nPanic#2 Part1\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831044\nPanic#2 Part2\n\n$ head --lines=1 /sys/fs/pstore/dmesg-erst-6319986351055831045\nPanic#2 Part3  It is important to note that the pstore typically has very limited storage space (on the order of kilobytes) and will not overwrite entries when out of space. The files in  /sys/fs/pstore  must be removed to free up space. The typical approach is to move the files from the pstore to a more permanent storage location on boot, but Flatcar Container Linux will not do this automatically for you.",
            "title": "Using pstore"
        },
        {
            "location": "/os/configuring-date-and-timezone/",
            "text": "Configuring date and time zone\n\u00b6\n\n\nBy default, Flatcar Container Linux machines keep time in the Coordinated Universal Time (UTC) zone and synchronize their clocks with the Network Time Protocol (NTP). This page contains information about customizing those defaults, explains the change in NTP client daemons in recent Flatcar Container Linux versions, and offers advice on best practices for timekeeping in Flatcar Container Linux clusters.\n\n\nViewing and changing time and date\n\u00b6\n\n\nThe \ntimedatectl(1)\n command displays and sets the date, time, and time zone.\n\n\n$ timedatectl status\n      Local time: Wed 2015-08-26 19:29:12 UTC\n  Universal time: Wed 2015-08-26 19:29:12 UTC\n        RTC time: Wed 2015-08-26 19:29:12\n       Time zone: UTC (UTC, +0000)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: n/a\n\n\n\nRecommended: UTC time\n\u00b6\n\n\nTo avoid time zone confusion and the complexities of adjusting clocks for daylight saving time (or not) in accordance with regional custom, we recommend that all machines in Flatcar Container Linux clusters use UTC. This is the default time zone. To reset a machine to this default:\n\n\n$ sudo timedatectl set-timezone UTC\n\n\n\nChanging the time zone\n\u00b6\n\n\nIf your site or application requires a different system time zone, start by listing the available options:\n\n\n$ timedatectl list-timezones\nAfrica/Abidjan\nAfrica/Accra\nAfrica/Addis_Ababa\n\u2026\n\n\n\nPick a time zone from the list and set it:\n\n\n$ sudo timedatectl set-timezone America/New_York\n\n\n\nCheck the changes:\n\n\n$ timedatectl\n      Local time: Wed 2015-08-26 15:44:07 EDT\n  Universal time: Wed 2015-08-26 19:44:07 UTC\n        RTC time: Wed 2015-08-26 19:44:07\n       Time zone: America/New_York (EDT, -0400)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: yes\n Last DST change: DST began at\n                  Sun 2015-03-08 01:59:59 EST\n                  Sun 2015-03-08 03:00:00 EDT\n Next DST change: DST ends (the clock jumps one hour backwards) at\n                  Sun 2015-11-01 01:59:59 EDT\n                  Sun 2015-11-01 01:00:00 EST\n\n\n\nTime synchronization\n\u00b6\n\n\nFlatcar Container Linux clusters use NTP to synchronize the clocks of member nodes, and all machines start an NTP client at boot. Flatcar Container Linux versions later than \n681.0.0\n use \nsystemd-timesyncd(8)\n as the default NTP client. Earlier versions used \nntpd(8)\n. Use \nsystemctl\n to check which service is running:\n\n\n$ systemctl status systemd-timesyncd ntpd\n\u25cf systemd-timesyncd.service - Network Time Synchronization\n   Loaded: loaded (/usr/lib64/systemd/system/systemd-timesyncd.service; disabled; vendor preset: disabled)\n   Active: active (running) since Thu 2015-05-14 05:43:20 UTC; 5 days ago\n     Docs: man:systemd-timesyncd.service(8)\n Main PID: 480 (systemd-timesyn)\n   Status: \"Using Time Server 169.254.169.254:123 (169.254.169.254).\"\n   Memory: 448.0K\n   CGroup: /system.slice/systemd-timesyncd.service\n           \u2514\u2500480 /usr/lib/systemd/systemd-timesyncd\n\n\u25cf ntpd.service - Network Time Service\n   Loaded: loaded (/usr/lib64/systemd/system/ntpd.service; disabled; vendor preset: disabled)\n   Active: inactive (dead)\n\n\n\nRecommended NTP sources\n\u00b6\n\n\nUnless you have a highly reliable and precise time server pool, use your cloud provider's NTP source, or, on bare metal, the default Flatcar Container Linux NTP servers:\n\n\n0.flatcar.pool.ntp.org\n1.flatcar.pool.ntp.org\n2.flatcar.pool.ntp.org\n3.flatcar.pool.ntp.org\n\n\n\nChanging NTP time sources\n\u00b6\n\n\nSystemd-timesyncd\n can discover NTP servers from DHCP, individual \nnetwork\n configs, the file \ntimesyncd.conf\n, or the default \n*.flatcar.pool.ntp.org\n pool.\n\n\nThe default behavior uses NTP servers provided by DHCP. To disable this, write a configuration listing your preferred NTP servers into the file \n/etc/systemd/network/50-dhcp-no-ntp.conf\n:\n\n\n[Network]\nDHCP=v4\nNTP=0.pool.example.com 1.pool.example.com\n\n[DHCP]\nUseMTU=true\nUseDomains=true\nUseNTP=false\n\n\n\nThen restart the network daemon:\n\n\n$ sudo systemctl restart systemd-networkd\n\n\n\nNTP time sources can be set in \ntimesyncd.conf\n with a \nContainer Linux Config\n snippet like:\n\n\nstorage:\n  files:\n    - path: /etc/systemd/timesyncd.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Time]\n          NTP=0.pool.example.com 1.pool.example.com\n\n\n\nSwitching from timesyncd to ntpd\n\u00b6\n\n\nOn Flatcar Container Linux 681.0.0 or later, you can switch from \nsystemd-timesyncd\n back to \nntpd\n with the following commands:\n\n\n$ sudo systemctl stop systemd-timesyncd\n$ sudo systemctl mask systemd-timesyncd\n$ sudo systemctl enable ntpd\n$ sudo systemctl start ntpd\n\n\n\nor with this Container Linux Config snippet:\n\n\nsystemd:\n  units:\n    - name: systemd-timesyncd.service\n      mask: true\n    - name: ntpd.service\n      enable: true\n\n\n\nBecause \ntimesyncd\n and \nntpd\n are mutually exclusive, it's important to \nmask\n the \nsystemd-tinesyncd\n service. \nSystemctl disable\n or \nstop\n alone will not prevent a default service from starting again.\n\n\nConfiguring ntpd\n\u00b6\n\n\nThe \nntpd\n service reads all configuration from the file \n/etc/ntp.conf\n. It does not use DHCP or other configuration sources. To use a different set of NTP servers, replace the \n/etc/ntp.conf\n symlink with something like the following:\n\n\nserver 0.pool.example.com\nserver 1.pool.example.com\n\nrestrict default nomodify nopeer noquery limited kod\nrestrict 127.0.0.1\nrestrict [::1]\n\n\n\nThen ask \nntpd\n to reload its configuration:\n\n\n$ sudo systemctl reload ntpd\n\n\n\nOr, in a \nContainer Linux Config\n:\n\n\nstorage:\n  files:\n    - path: /etc/ntp.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          server 0.pool.example.com\n          server 1.pool.example.com\n\n          # - Allow only time queries, at a limited rate.\n          # - Allow all local queries (IPv4, IPv6)\n          restrict default nomodify nopeer noquery limited kod\n          restrict 127.0.0.1\n          restrict [::1]",
            "title": "Configuring date and time zone"
        },
        {
            "location": "/os/configuring-date-and-timezone/#configuring-date-and-time-zone",
            "text": "By default, Flatcar Container Linux machines keep time in the Coordinated Universal Time (UTC) zone and synchronize their clocks with the Network Time Protocol (NTP). This page contains information about customizing those defaults, explains the change in NTP client daemons in recent Flatcar Container Linux versions, and offers advice on best practices for timekeeping in Flatcar Container Linux clusters.",
            "title": "Configuring date and time zone"
        },
        {
            "location": "/os/configuring-date-and-timezone/#viewing-and-changing-time-and-date",
            "text": "The  timedatectl(1)  command displays and sets the date, time, and time zone.  $ timedatectl status\n      Local time: Wed 2015-08-26 19:29:12 UTC\n  Universal time: Wed 2015-08-26 19:29:12 UTC\n        RTC time: Wed 2015-08-26 19:29:12\n       Time zone: UTC (UTC, +0000)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: n/a",
            "title": "Viewing and changing time and date"
        },
        {
            "location": "/os/configuring-date-and-timezone/#recommended-utc-time",
            "text": "To avoid time zone confusion and the complexities of adjusting clocks for daylight saving time (or not) in accordance with regional custom, we recommend that all machines in Flatcar Container Linux clusters use UTC. This is the default time zone. To reset a machine to this default:  $ sudo timedatectl set-timezone UTC",
            "title": "Recommended: UTC time"
        },
        {
            "location": "/os/configuring-date-and-timezone/#changing-the-time-zone",
            "text": "If your site or application requires a different system time zone, start by listing the available options:  $ timedatectl list-timezones\nAfrica/Abidjan\nAfrica/Accra\nAfrica/Addis_Ababa\n\u2026  Pick a time zone from the list and set it:  $ sudo timedatectl set-timezone America/New_York  Check the changes:  $ timedatectl\n      Local time: Wed 2015-08-26 15:44:07 EDT\n  Universal time: Wed 2015-08-26 19:44:07 UTC\n        RTC time: Wed 2015-08-26 19:44:07\n       Time zone: America/New_York (EDT, -0400)\n Network time on: no\nNTP synchronized: yes\n RTC in local TZ: no\n      DST active: yes\n Last DST change: DST began at\n                  Sun 2015-03-08 01:59:59 EST\n                  Sun 2015-03-08 03:00:00 EDT\n Next DST change: DST ends (the clock jumps one hour backwards) at\n                  Sun 2015-11-01 01:59:59 EDT\n                  Sun 2015-11-01 01:00:00 EST",
            "title": "Changing the time zone"
        },
        {
            "location": "/os/configuring-date-and-timezone/#time-synchronization",
            "text": "Flatcar Container Linux clusters use NTP to synchronize the clocks of member nodes, and all machines start an NTP client at boot. Flatcar Container Linux versions later than  681.0.0  use  systemd-timesyncd(8)  as the default NTP client. Earlier versions used  ntpd(8) . Use  systemctl  to check which service is running:  $ systemctl status systemd-timesyncd ntpd\n\u25cf systemd-timesyncd.service - Network Time Synchronization\n   Loaded: loaded (/usr/lib64/systemd/system/systemd-timesyncd.service; disabled; vendor preset: disabled)\n   Active: active (running) since Thu 2015-05-14 05:43:20 UTC; 5 days ago\n     Docs: man:systemd-timesyncd.service(8)\n Main PID: 480 (systemd-timesyn)\n   Status: \"Using Time Server 169.254.169.254:123 (169.254.169.254).\"\n   Memory: 448.0K\n   CGroup: /system.slice/systemd-timesyncd.service\n           \u2514\u2500480 /usr/lib/systemd/systemd-timesyncd\n\n\u25cf ntpd.service - Network Time Service\n   Loaded: loaded (/usr/lib64/systemd/system/ntpd.service; disabled; vendor preset: disabled)\n   Active: inactive (dead)",
            "title": "Time synchronization"
        },
        {
            "location": "/os/configuring-date-and-timezone/#recommended-ntp-sources",
            "text": "Unless you have a highly reliable and precise time server pool, use your cloud provider's NTP source, or, on bare metal, the default Flatcar Container Linux NTP servers:  0.flatcar.pool.ntp.org\n1.flatcar.pool.ntp.org\n2.flatcar.pool.ntp.org\n3.flatcar.pool.ntp.org",
            "title": "Recommended NTP sources"
        },
        {
            "location": "/os/configuring-date-and-timezone/#changing-ntp-time-sources",
            "text": "Systemd-timesyncd  can discover NTP servers from DHCP, individual  network  configs, the file  timesyncd.conf , or the default  *.flatcar.pool.ntp.org  pool.  The default behavior uses NTP servers provided by DHCP. To disable this, write a configuration listing your preferred NTP servers into the file  /etc/systemd/network/50-dhcp-no-ntp.conf :  [Network]\nDHCP=v4\nNTP=0.pool.example.com 1.pool.example.com\n\n[DHCP]\nUseMTU=true\nUseDomains=true\nUseNTP=false  Then restart the network daemon:  $ sudo systemctl restart systemd-networkd  NTP time sources can be set in  timesyncd.conf  with a  Container Linux Config  snippet like:  storage:\n  files:\n    - path: /etc/systemd/timesyncd.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Time]\n          NTP=0.pool.example.com 1.pool.example.com",
            "title": "Changing NTP time sources"
        },
        {
            "location": "/os/configuring-date-and-timezone/#switching-from-timesyncd-to-ntpd",
            "text": "On Flatcar Container Linux 681.0.0 or later, you can switch from  systemd-timesyncd  back to  ntpd  with the following commands:  $ sudo systemctl stop systemd-timesyncd\n$ sudo systemctl mask systemd-timesyncd\n$ sudo systemctl enable ntpd\n$ sudo systemctl start ntpd  or with this Container Linux Config snippet:  systemd:\n  units:\n    - name: systemd-timesyncd.service\n      mask: true\n    - name: ntpd.service\n      enable: true  Because  timesyncd  and  ntpd  are mutually exclusive, it's important to  mask  the  systemd-tinesyncd  service.  Systemctl disable  or  stop  alone will not prevent a default service from starting again.",
            "title": "Switching from timesyncd to ntpd"
        },
        {
            "location": "/os/configuring-date-and-timezone/#configuring-ntpd",
            "text": "The  ntpd  service reads all configuration from the file  /etc/ntp.conf . It does not use DHCP or other configuration sources. To use a different set of NTP servers, replace the  /etc/ntp.conf  symlink with something like the following:  server 0.pool.example.com\nserver 1.pool.example.com\n\nrestrict default nomodify nopeer noquery limited kod\nrestrict 127.0.0.1\nrestrict [::1]  Then ask  ntpd  to reload its configuration:  $ sudo systemctl reload ntpd  Or, in a  Container Linux Config :  storage:\n  files:\n    - path: /etc/ntp.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          server 0.pool.example.com\n          server 1.pool.example.com\n\n          # - Allow only time queries, at a limited rate.\n          # - Allow all local queries (IPv4, IPv6)\n          restrict default nomodify nopeer noquery limited kod\n          restrict 127.0.0.1\n          restrict [::1]",
            "title": "Configuring ntpd"
        },
        {
            "location": "/os/configuring-dns/",
            "text": "DNS Configuration\n\u00b6\n\n\nBy default, DNS resolution on Flatcar Container Linux is handled through \n/etc/resolv.conf\n, which is a symlink to \n/run/systemd/resolve/resolv.conf\n. This file is managed by \nsystemd-resolved\n. Normally, \nsystemd-resolved\n gets DNS IP addresses from \nsystemd-networkd\n, either via DHCP or static configuration. DNS IP addresses can also be set via \nsystemd-resolved\n's \nresolved.conf\n. See \nNetwork configuration with networkd\n for more information on \nsystemd-networkd\n.\n\n\nUsing a local DNS cache\n\u00b6\n\n\nsystemd-resolved\n includes a caching DNS resolver. To use it for DNS resolution and caching, you must enable it via \nnsswitch.conf\n by adding \nresolve\n to the \nhosts\n section.\n\n\nHere is an example \nContainer Linux Config\n snippet to do that:\n\n\nstorage:\n  files:\n    - path: /etc/nsswitch.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          # /etc/nsswitch.conf:\n\n          passwd:      files usrfiles\n          shadow:      files usrfiles\n          group:       files usrfiles\n\n          hosts:       files usrfiles resolve dns\n          networks:    files usrfiles dns\n\n          services:    files usrfiles\n          protocols:   files usrfiles\n          rpc:         files usrfiles\n\n          ethers:      files\n          netmasks:    files\n          netgroup:    files\n          bootparams:  files\n          automount:   files\n          aliases:     files\n\n\n\nOnly nss-aware applications can take advantage of the \nsystemd-resolved\n cache. Notably, this means that statically linked Go programs and programs running within Docker/rkt will use \n/etc/resolv.conf\n only, and will not use the \nsystemd-resolve\n cache.",
            "title": "DNS Configuration"
        },
        {
            "location": "/os/configuring-dns/#dns-configuration",
            "text": "By default, DNS resolution on Flatcar Container Linux is handled through  /etc/resolv.conf , which is a symlink to  /run/systemd/resolve/resolv.conf . This file is managed by  systemd-resolved . Normally,  systemd-resolved  gets DNS IP addresses from  systemd-networkd , either via DHCP or static configuration. DNS IP addresses can also be set via  systemd-resolved 's  resolved.conf . See  Network configuration with networkd  for more information on  systemd-networkd .",
            "title": "DNS Configuration"
        },
        {
            "location": "/os/configuring-dns/#using-a-local-dns-cache",
            "text": "systemd-resolved  includes a caching DNS resolver. To use it for DNS resolution and caching, you must enable it via  nsswitch.conf  by adding  resolve  to the  hosts  section.  Here is an example  Container Linux Config  snippet to do that:  storage:\n  files:\n    - path: /etc/nsswitch.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          # /etc/nsswitch.conf:\n\n          passwd:      files usrfiles\n          shadow:      files usrfiles\n          group:       files usrfiles\n\n          hosts:       files usrfiles resolve dns\n          networks:    files usrfiles dns\n\n          services:    files usrfiles\n          protocols:   files usrfiles\n          rpc:         files usrfiles\n\n          ethers:      files\n          netmasks:    files\n          netgroup:    files\n          bootparams:  files\n          automount:   files\n          aliases:     files  Only nss-aware applications can take advantage of the  systemd-resolved  cache. Notably, this means that statically linked Go programs and programs running within Docker/rkt will use  /etc/resolv.conf  only, and will not use the  systemd-resolve  cache.",
            "title": "Using a local DNS cache"
        },
        {
            "location": "/os/customize-etcd-unit/",
            "text": "Customizing the etcd unit\n\u00b6\n\n\nThe etcd systemd unit can be customized by overriding the unit that ships with the default Flatcar Container Linux settings. Common use-cases for doing this are covered below.\n\n\nUse client certificates\n\u00b6\n\n\netcd supports client certificates as a way to provide secure communication between clients \u2194 leader and internal traffic between etcd peers in the cluster. Configuring certificates for both scenarios is done through the etcd section in a Container Linux Config. Options provided here will augment the unit that ships with Flatcar Container Linux.\n\n\nPlease follow the \ninstruction\n to know how to create self-signed certificates and private keys.\n\n\netcd:\n  # More settings are needed here for a functioning etcd daemon\n  ca_file:        /path/to/CA.pem\n  cert_file:      /path/to/server.crt\n  key_file:       /path/to/server.key\n  peer_ca_file:   /path/to/CA.pem\n  peer_cert_file: /path/to/peers.crt\n  peer_key_file:  /path/to/peers.key\nstorage:\n  files:\n    - path: /path/to/CA.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFNDCCAx6gAwIBAgIBATALBgkqhkiG9w0BAQUwLTEMMAoGA1UEBhMDVVNBMRAw\n          ...snip...\n          EtHaxYQRy72yZrte6Ypw57xPRB8sw1DIYjr821Lw05DrLuBYcbyclg==\n          -----END CERTIFICATE-----\n    - path: /path/to/server.crt\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFWTCCA0OgAwIBAgIBAjALBgkqhkiG9w0BAQUwLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNDA1MjEyMTQ0MjhaFw0y\n          ...snip...\n          rdmtCVLOyo2wz/UTzvo7UpuxRrnizBHpytE4u0KgifGp1OOKY+1Lx8XSH7jJIaZB\n          a3m12FMs3AsSt7mzyZk+bH2WjZLrlUXyrvprI40=\n          -----END CERTIFICATE-----\n    - path: /path/to/server.key\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          Proc-Type: 4,ENCRYPTED\n          DEK-Info: DES-EDE3-CBC,069abc493cd8bda6\n\n          TBX9mCqvzNMWZN6YQKR2cFxYISFreNk5Q938s5YClnCWz3B6KfwCZtjMlbdqAakj\n          ...snip...\n          mgVh2LBerGMbsdsTQ268sDvHKTdD9MDAunZlQIgO2zotARY02MLV/Q5erASYdCxk\n          -----END RSA PRIVATE KEY-----\n    - path: /path/to/peers.crt\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          VQQLEwJDQTAeFw0xNDA1MjEyMTQ0MjhaFw0yMIIFWTCCA0OgAwIBAgIBAjALBgkq\n          DgYDVQQKEwdldGNkLWNhMQswCQYDhkiG9w0BAQUwLTEMMAoGA1UEBhMDVVNBMRAw\n          ...snip...\n          BHpytE4u0KgifGp1OOKY+1Lx8XSH7jJIaZBrdmtCVLOyo2wz/UTzvo7UpuxRrniz\n          St7mza3m12FMs3AsyZk+bH2WjZLrlUXyrvprI90=\n          -----END CERTIFICATE-----\n    - path: /path/to/peers.key\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          Proc-Type: 4,ENCRYPTED\n          DEK-Info: DES-EDE3-CBC,069abc493cd8bda6\n\n          SFreNk5Q938s5YTBX9mCqvzNMWZN6YQKR2cFxYIClnCWz3B6KfwCZtjMlbdqAakj\n          ...snip...\n          DvHKTdD9MDAunZlQIgO2zotmgVh2LBerGMbsdsTQ268sARY02MLV/Q5erASYdCxk\n          -----END RSA PRIVATE KEY-----",
            "title": "Customizing the etcd unit"
        },
        {
            "location": "/os/customize-etcd-unit/#customizing-the-etcd-unit",
            "text": "The etcd systemd unit can be customized by overriding the unit that ships with the default Flatcar Container Linux settings. Common use-cases for doing this are covered below.",
            "title": "Customizing the etcd unit"
        },
        {
            "location": "/os/customize-etcd-unit/#use-client-certificates",
            "text": "etcd supports client certificates as a way to provide secure communication between clients \u2194 leader and internal traffic between etcd peers in the cluster. Configuring certificates for both scenarios is done through the etcd section in a Container Linux Config. Options provided here will augment the unit that ships with Flatcar Container Linux.  Please follow the  instruction  to know how to create self-signed certificates and private keys.  etcd:\n  # More settings are needed here for a functioning etcd daemon\n  ca_file:        /path/to/CA.pem\n  cert_file:      /path/to/server.crt\n  key_file:       /path/to/server.key\n  peer_ca_file:   /path/to/CA.pem\n  peer_cert_file: /path/to/peers.crt\n  peer_key_file:  /path/to/peers.key\nstorage:\n  files:\n    - path: /path/to/CA.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFNDCCAx6gAwIBAgIBATALBgkqhkiG9w0BAQUwLTEMMAoGA1UEBhMDVVNBMRAw\n          ...snip...\n          EtHaxYQRy72yZrte6Ypw57xPRB8sw1DIYjr821Lw05DrLuBYcbyclg==\n          -----END CERTIFICATE-----\n    - path: /path/to/server.crt\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFWTCCA0OgAwIBAgIBAjALBgkqhkiG9w0BAQUwLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNDA1MjEyMTQ0MjhaFw0y\n          ...snip...\n          rdmtCVLOyo2wz/UTzvo7UpuxRrnizBHpytE4u0KgifGp1OOKY+1Lx8XSH7jJIaZB\n          a3m12FMs3AsSt7mzyZk+bH2WjZLrlUXyrvprI40=\n          -----END CERTIFICATE-----\n    - path: /path/to/server.key\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          Proc-Type: 4,ENCRYPTED\n          DEK-Info: DES-EDE3-CBC,069abc493cd8bda6\n\n          TBX9mCqvzNMWZN6YQKR2cFxYISFreNk5Q938s5YClnCWz3B6KfwCZtjMlbdqAakj\n          ...snip...\n          mgVh2LBerGMbsdsTQ268sDvHKTdD9MDAunZlQIgO2zotARY02MLV/Q5erASYdCxk\n          -----END RSA PRIVATE KEY-----\n    - path: /path/to/peers.crt\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          VQQLEwJDQTAeFw0xNDA1MjEyMTQ0MjhaFw0yMIIFWTCCA0OgAwIBAgIBAjALBgkq\n          DgYDVQQKEwdldGNkLWNhMQswCQYDhkiG9w0BAQUwLTEMMAoGA1UEBhMDVVNBMRAw\n          ...snip...\n          BHpytE4u0KgifGp1OOKY+1Lx8XSH7jJIaZBrdmtCVLOyo2wz/UTzvo7UpuxRrniz\n          St7mza3m12FMs3AsyZk+bH2WjZLrlUXyrvprI90=\n          -----END CERTIFICATE-----\n    - path: /path/to/peers.key\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          Proc-Type: 4,ENCRYPTED\n          DEK-Info: DES-EDE3-CBC,069abc493cd8bda6\n\n          SFreNk5Q938s5YTBX9mCqvzNMWZN6YQKR2cFxYIClnCWz3B6KfwCZtjMlbdqAakj\n          ...snip...\n          DvHKTdD9MDAunZlQIgO2zotmgVh2LBerGMbsdsTQ268sARY02MLV/Q5erASYdCxk\n          -----END RSA PRIVATE KEY-----",
            "title": "Use client certificates"
        },
        {
            "location": "/os/customizing-docker/",
            "text": "Customizing docker\n\u00b6\n\n\nThe Docker systemd unit can be customized by overriding the unit that ships with the default Flatcar Container Linux settings. Common use-cases for doing this are covered below.\n\n\nEnable the remote API on a new socket\n\u00b6\n\n\nCreate a file called \n/etc/systemd/system/docker-tcp.socket\n to make Docker available on a TCP socket on port 2375.\n\n\n[Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=2375\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target\n\n\n\nThen enable this new socket:\n\n\nsystemctl enable docker-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tcp.socket\nsystemctl start docker\n\n\n\nTest that it's working:\n\n\ndocker -H tcp://127.0.0.1:2375 ps\n\n\n\nContainer Linux Config\n\u00b6\n\n\nTo enable the remote API on every Flatcar Container Linux machine in a cluster, use a \nContainer Linux Config\n. We need to provide the new socket file and Docker's socket activation support will automatically start using the socket:\n\n\nsystemd:\n  units:\n    - name: docker-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\n\n\n\nTo keep access to the port local, replace the \nListenStream\n configuration above with:\n\n\n        [Socket]\n        ListenStream=127.0.0.1:2375\n\n\n\nEnable the remote API with TLS authentication\n\u00b6\n\n\nDocker TLS configuration consists of three parts: keys creation, configuring new \nsystemd socket\n unit and systemd \ndrop-in\n configuration.\n\n\nTLS keys creation\n\u00b6\n\n\nPlease follow the \ninstruction\n to know how to create self-signed certificates and private keys. Then copy with following files into \n/etc/docker\n Flatcar Container Linux's directory and fix their permissions:\n\n\nscp ~/cfssl/{server.pem,server-key.pem,ca.pem} coreos.example.com:\nssh core@coreos.example.com\nsudo mv {server.pem,server-key.pem,ca.pem} /etc/docker/\nsudo chown root:root /etc/docker/{server-key.pem,server.pem,ca.pem}\nsudo chmod 0600 /etc/docker/server-key.pem\n\n\n\nOn your local host copy certificates into \n~/.docker\n:\n\n\nmkdir ~/.docker\nchmod 700 ~/.docker\ncd ~/.docker\ncp -p ~/cfssl/ca.pem ca.pem\ncp -p ~/cfssl/client.pem cert.pem\ncp -p ~/cfssl/client-key.pem key.pem\n\n\n\nEnable the secure remote API on a new socket\n\u00b6\n\n\nCreate a file called \n/etc/systemd/system/docker-tls-tcp.socket\n to make Docker available on a secured TCP socket on port 2376.\n\n\n[Unit]\nDescription=Docker Secured Socket for the API\n\n[Socket]\nListenStream=2376\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target\n\n\n\nThen enable this new socket:\n\n\nsystemctl enable docker-tls-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tls-tcp.socket\n\n\n\nDrop-in configuration\n\u00b6\n\n\nCreate \n/etc/systemd/system/docker.service.d/10-tls-verify.conf\n \ndrop-in\n for systemd Docker service:\n\n\n[Service]\nEnvironment=\"DOCKER_OPTS=--tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server.pem --tlskey=/etc/docker/server-key.pem\"\n\n\n\nReload systemd config files and restart docker service:\n\n\nsudo systemctl daemon-reload\nsudo systemctl restart docker.service\n\n\n\nNow you can access your Docker's API through TLS secured connection:\n\n\ndocker --tlsverify -H tcp://server:2376 images\n# or\ndocker --tlsverify -H tcp://server.example.com:2376 images\n\n\n\nIf you've experienceed problems connection to remote Docker API using TLS connection, you can debug it with \ncurl\n:\n\n\ncurl -v --cacert ~/.docker/ca.pem --cert ~/.docker/cert.pem --key ~/.docker/key.pem https://server:2376\n\n\n\nOr on your Flatcar Container Linux host:\n\n\njournalctl -f -u docker.service\n\n\n\nIn addition you can export environment variables and use docker client without additional options:\n\n\nexport DOCKER_HOST=tcp://server.example.com:2376 DOCKER_TLS_VERIFY=1\ndocker images\n\n\n\nContainer Linux Config\n\u00b6\n\n\nA Container Linux Config for Docker TLS authentication will look like:\n\n\nstorage:\n  files:\n    - path: /etc/docker/ca.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFNDCCAx6gAwIBAgIBATALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDExMDhaFw0y\n          NTA5MDIxMDExMThaMC0xDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEL\n          ... ... ...\n    - path: /etc/docker/server.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFajCCA1SgAwIBAgIBBTALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDM3MDFaFw0y\n          NTA5MDIxMDM3MDNaMEQxDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEQ\n          ... ... ...\n    - path: /etc/docker/server-key.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          MIIJKAIBAAKCAgEA23Q4yELhNEywScrHl6+MUtbonCu59LIjpxDMAGxAHvWhWpEY\n          P5vfas8KgxxNyR+U8VpIjEXvwnhwCx/CSCJc3/VtU9v011Ir0WtTrNDocb90fIr3\n          YeRWq744UJpBeDHPV9opf8xFE7F74zWeTVMwtiMPKcQDzZ7XoNyJMxg1wmiMbdCj\n          ... ... ...\nsystemd:\n  units:\n    - name: docker-tls-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Secured Socket for the API\n\n        [Socket]\n        ListenStream=2376\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\ndocker:\n  flags:\n    - --tlsverify\n    - --tlscacert=/etc/docker/ca.pem\n    - --tlscert=/etc/docker/server.pem\n    - --tlskey=/etc/docker/server-key.pem\n\n\n\nUse attached storage for Docker images\n\u00b6\n\n\nDocker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Check out the guide to \nmounting storage to your Flatcar Container Linux machine\n for an example of how to bind mount storage into \n/var/lib/docker\n.\n\n\nEnabling the Docker debug flag\n\u00b6\n\n\nSet the \n--debug\n (\n-D\n) flag in the \nDOCKER_OPTS\n environment variable by using a drop-in file. For example, the following could be written to \n/etc/systemd/system/docker.service.d/10-debug.conf\n:\n\n\n[Service]\nEnvironment=DOCKER_OPTS=--debug\n\n\n\nNow tell systemd about the new configuration and restart Docker:\n\n\nsystemctl daemon-reload\nsystemctl restart docker\n\n\n\nTo test our debugging stream, run a Docker command and then read the systemd journal, which should contain the output:\n\n\ndocker ps\njournalctl -u docker\n\n\n\nContainer Linux Config\n\u00b6\n\n\nIf you need to modify a flag across many machines, you can add the flag with a Container Linux Config:\n\n\ndocker:\n  flags:\n    - --debug\n\n\n\nUse an HTTP proxy\n\u00b6\n\n\nIf you're operating in a locked down networking environment, you can specify an HTTP proxy for Docker to use via an environment variable. First, create a directory for drop-in configuration for Docker:\n\n\nmkdir /etc/systemd/system/docker.service.d\n\n\n\nNow, create a file called \n/etc/systemd/system/docker.service.d/http-proxy.conf\n that adds the environment variable:\n\n\n[Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:8080\"\n\n\n\nTo apply the change, reload the unit and restart Docker:\n\n\nsystemctl daemon-reload\nsystemctl restart docker\n\n\n\nProxy environment variables can also be set \nsystem-wide\n.\n\n\nContainer Linux Config\n\u00b6\n\n\nThe easiest way to use this proxy on all of your machines is via a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 20-http-proxy.conf\n          contents: |\n            [Service]\n            Environment=\"HTTP_PROXY=http://proxy.example.com:8080\"\n\n\n\nIncrease ulimits\n\u00b6\n\n\nIf you need to increase certain ulimits that are too low for your application by default, like memlock, you will need to modify the Docker service to increase the limit. First, create a directory for drop-in configuration for Docker:\n\n\nmkdir /etc/systemd/system/docker.service.d\n\n\n\nNow, create a file called \n/etc/systemd/system/docker.service.d/increase-ulimit.conf\n that adds increased limit:\n\n\n[Service]\nLimitMEMLOCK=infinity\n\n\n\nTo apply the change, reload the unit and restart Docker:\n\n\nsystemctl daemon-reload\nsystemctl restart docker\n\n\n\nContainer Linux Config\n\u00b6\n\n\nThe easiest way to use these new ulimits on all of your machines is via a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 30-increase-ulimit.conf\n          contents: |\n            [Service]\n            LimitMEMLOCK=infinity\n\n\n\nUsing a dockercfg file for authentication\n\u00b6\n\n\nA json file \n.dockercfg\n can be created in your home directory that holds authentication information for a public or private Docker registry.",
            "title": "Customizing docker"
        },
        {
            "location": "/os/customizing-docker/#customizing-docker",
            "text": "The Docker systemd unit can be customized by overriding the unit that ships with the default Flatcar Container Linux settings. Common use-cases for doing this are covered below.",
            "title": "Customizing docker"
        },
        {
            "location": "/os/customizing-docker/#enable-the-remote-api-on-a-new-socket",
            "text": "Create a file called  /etc/systemd/system/docker-tcp.socket  to make Docker available on a TCP socket on port 2375.  [Unit]\nDescription=Docker Socket for the API\n\n[Socket]\nListenStream=2375\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target  Then enable this new socket:  systemctl enable docker-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tcp.socket\nsystemctl start docker  Test that it's working:  docker -H tcp://127.0.0.1:2375 ps",
            "title": "Enable the remote API on a new socket"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config",
            "text": "To enable the remote API on every Flatcar Container Linux machine in a cluster, use a  Container Linux Config . We need to provide the new socket file and Docker's socket activation support will automatically start using the socket:  systemd:\n  units:\n    - name: docker-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Socket for the API\n\n        [Socket]\n        ListenStream=2375\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target  To keep access to the port local, replace the  ListenStream  configuration above with:          [Socket]\n        ListenStream=127.0.0.1:2375",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#enable-the-remote-api-with-tls-authentication",
            "text": "Docker TLS configuration consists of three parts: keys creation, configuring new  systemd socket  unit and systemd  drop-in  configuration.",
            "title": "Enable the remote API with TLS authentication"
        },
        {
            "location": "/os/customizing-docker/#tls-keys-creation",
            "text": "Please follow the  instruction  to know how to create self-signed certificates and private keys. Then copy with following files into  /etc/docker  Flatcar Container Linux's directory and fix their permissions:  scp ~/cfssl/{server.pem,server-key.pem,ca.pem} coreos.example.com:\nssh core@coreos.example.com\nsudo mv {server.pem,server-key.pem,ca.pem} /etc/docker/\nsudo chown root:root /etc/docker/{server-key.pem,server.pem,ca.pem}\nsudo chmod 0600 /etc/docker/server-key.pem  On your local host copy certificates into  ~/.docker :  mkdir ~/.docker\nchmod 700 ~/.docker\ncd ~/.docker\ncp -p ~/cfssl/ca.pem ca.pem\ncp -p ~/cfssl/client.pem cert.pem\ncp -p ~/cfssl/client-key.pem key.pem",
            "title": "TLS keys creation"
        },
        {
            "location": "/os/customizing-docker/#enable-the-secure-remote-api-on-a-new-socket",
            "text": "Create a file called  /etc/systemd/system/docker-tls-tcp.socket  to make Docker available on a secured TCP socket on port 2376.  [Unit]\nDescription=Docker Secured Socket for the API\n\n[Socket]\nListenStream=2376\nBindIPv6Only=both\nService=docker.service\n\n[Install]\nWantedBy=sockets.target  Then enable this new socket:  systemctl enable docker-tls-tcp.socket\nsystemctl stop docker\nsystemctl start docker-tls-tcp.socket",
            "title": "Enable the secure remote API on a new socket"
        },
        {
            "location": "/os/customizing-docker/#drop-in-configuration",
            "text": "Create  /etc/systemd/system/docker.service.d/10-tls-verify.conf   drop-in  for systemd Docker service:  [Service]\nEnvironment=\"DOCKER_OPTS=--tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server.pem --tlskey=/etc/docker/server-key.pem\"  Reload systemd config files and restart docker service:  sudo systemctl daemon-reload\nsudo systemctl restart docker.service  Now you can access your Docker's API through TLS secured connection:  docker --tlsverify -H tcp://server:2376 images\n# or\ndocker --tlsverify -H tcp://server.example.com:2376 images  If you've experienceed problems connection to remote Docker API using TLS connection, you can debug it with  curl :  curl -v --cacert ~/.docker/ca.pem --cert ~/.docker/cert.pem --key ~/.docker/key.pem https://server:2376  Or on your Flatcar Container Linux host:  journalctl -f -u docker.service  In addition you can export environment variables and use docker client without additional options:  export DOCKER_HOST=tcp://server.example.com:2376 DOCKER_TLS_VERIFY=1\ndocker images",
            "title": "Drop-in configuration"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_1",
            "text": "A Container Linux Config for Docker TLS authentication will look like:  storage:\n  files:\n    - path: /etc/docker/ca.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFNDCCAx6gAwIBAgIBATALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDExMDhaFw0y\n          NTA5MDIxMDExMThaMC0xDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEL\n          ... ... ...\n    - path: /etc/docker/server.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN CERTIFICATE-----\n          MIIFajCCA1SgAwIBAgIBBTALBgkqhkiG9w0BAQswLTEMMAoGA1UEBhMDVVNBMRAw\n          DgYDVQQKEwdldGNkLWNhMQswCQYDVQQLEwJDQTAeFw0xNTA5MDIxMDM3MDFaFw0y\n          NTA5MDIxMDM3MDNaMEQxDDAKBgNVBAYTA1VTQTEQMA4GA1UEChMHZXRjZC1jYTEQ\n          ... ... ...\n    - path: /etc/docker/server-key.pem\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          -----BEGIN RSA PRIVATE KEY-----\n          MIIJKAIBAAKCAgEA23Q4yELhNEywScrHl6+MUtbonCu59LIjpxDMAGxAHvWhWpEY\n          P5vfas8KgxxNyR+U8VpIjEXvwnhwCx/CSCJc3/VtU9v011Ir0WtTrNDocb90fIr3\n          YeRWq744UJpBeDHPV9opf8xFE7F74zWeTVMwtiMPKcQDzZ7XoNyJMxg1wmiMbdCj\n          ... ... ...\nsystemd:\n  units:\n    - name: docker-tls-tcp.socket\n      enable: true\n      contents: |\n        [Unit]\n        Description=Docker Secured Socket for the API\n\n        [Socket]\n        ListenStream=2376\n        BindIPv6Only=both\n        Service=docker.service\n\n        [Install]\n        WantedBy=sockets.target\ndocker:\n  flags:\n    - --tlsverify\n    - --tlscacert=/etc/docker/ca.pem\n    - --tlscert=/etc/docker/server.pem\n    - --tlskey=/etc/docker/server-key.pem",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#use-attached-storage-for-docker-images",
            "text": "Docker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Check out the guide to  mounting storage to your Flatcar Container Linux machine  for an example of how to bind mount storage into  /var/lib/docker .",
            "title": "Use attached storage for Docker images"
        },
        {
            "location": "/os/customizing-docker/#enabling-the-docker-debug-flag",
            "text": "Set the  --debug  ( -D ) flag in the  DOCKER_OPTS  environment variable by using a drop-in file. For example, the following could be written to  /etc/systemd/system/docker.service.d/10-debug.conf :  [Service]\nEnvironment=DOCKER_OPTS=--debug  Now tell systemd about the new configuration and restart Docker:  systemctl daemon-reload\nsystemctl restart docker  To test our debugging stream, run a Docker command and then read the systemd journal, which should contain the output:  docker ps\njournalctl -u docker",
            "title": "Enabling the Docker debug flag"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_2",
            "text": "If you need to modify a flag across many machines, you can add the flag with a Container Linux Config:  docker:\n  flags:\n    - --debug",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#use-an-http-proxy",
            "text": "If you're operating in a locked down networking environment, you can specify an HTTP proxy for Docker to use via an environment variable. First, create a directory for drop-in configuration for Docker:  mkdir /etc/systemd/system/docker.service.d  Now, create a file called  /etc/systemd/system/docker.service.d/http-proxy.conf  that adds the environment variable:  [Service]\nEnvironment=\"HTTP_PROXY=http://proxy.example.com:8080\"  To apply the change, reload the unit and restart Docker:  systemctl daemon-reload\nsystemctl restart docker  Proxy environment variables can also be set  system-wide .",
            "title": "Use an HTTP proxy"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_3",
            "text": "The easiest way to use this proxy on all of your machines is via a Container Linux Config:  systemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 20-http-proxy.conf\n          contents: |\n            [Service]\n            Environment=\"HTTP_PROXY=http://proxy.example.com:8080\"",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#increase-ulimits",
            "text": "If you need to increase certain ulimits that are too low for your application by default, like memlock, you will need to modify the Docker service to increase the limit. First, create a directory for drop-in configuration for Docker:  mkdir /etc/systemd/system/docker.service.d  Now, create a file called  /etc/systemd/system/docker.service.d/increase-ulimit.conf  that adds increased limit:  [Service]\nLimitMEMLOCK=infinity  To apply the change, reload the unit and restart Docker:  systemctl daemon-reload\nsystemctl restart docker",
            "title": "Increase ulimits"
        },
        {
            "location": "/os/customizing-docker/#container-linux-config_4",
            "text": "The easiest way to use these new ulimits on all of your machines is via a Container Linux Config:  systemd:\n  units:\n    - name: docker.service\n      enable: true\n      dropins:\n        - name: 30-increase-ulimit.conf\n          contents: |\n            [Service]\n            LimitMEMLOCK=infinity",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/customizing-docker/#using-a-dockercfg-file-for-authentication",
            "text": "A json file  .dockercfg  can be created in your home directory that holds authentication information for a public or private Docker registry.",
            "title": "Using a dockercfg file for authentication"
        },
        {
            "location": "/os/customizing-sshd/",
            "text": "Customizing the SSH daemon\n\u00b6\n\n\nFlatcar Container Linux defaults to running an OpenSSH daemon using \nsystemd\n socket activation -- when a client connects to the port configured for SSH, \nsshd\n is started on the fly for that client using a \nsystemd\n unit derived automatically from a template. In some cases you may want to customize this daemon's authentication methods or other configuration. This guide will show you how to do that at boot time using a \nContainer Linux Config\n, and after building by modifying the \nsystemd\n unit file.\n\n\nAs a practical example, when a client fails to connect by not completing the TCP connection (e.g. because the \"client\" is actually a TCP port scanner), the MOTD may report failures of \nsystemd\n units (which will be named by the source IP that failed to connect) next time you log in to the Flatcar Container Linux host. These failures are not themselves harmful, but it is a good general practice to change how SSH listens, either by changing the IP address \nsshd\n listens to from the default setting (which listens on all configured interfaces), changing the default port, or both.\n\n\nCustomizing sshd with a Container Linux Config\n\u00b6\n\n\nIn this example we will disable logins for the \nroot\n user, only allow login for the \ncore\n user and disable password based authentication. For more details on what sections can be added to \n/etc/ssh/sshd_config\n see the \nOpenSSH manual\n.\nIf you're interested in additional security options, Mozilla provides a well-commented example of a \nhardened configuration\n.\n\n\nstorage:\n  files:\n    - path: /etc/ssh/sshd_config\n      filesystem: root\n      mode: 0600\n      contents:\n        inline: |\n          # Use most defaults for sshd configuration.\n          UsePrivilegeSeparation sandbox\n          Subsystem sftp internal-sftp\n          UseDNS no\n\n          PermitRootLogin no\n          AllowUsers core\n          AuthenticationMethods publickey\n\n\n\nChanging the sshd port\n\u00b6\n\n\nFlatcar Container Linux ships with socket-activated SSH daemon by default. The configuration for this can be found at \n/usr/lib/systemd/system/sshd.socket\n. We're going to override some of the default settings for this in the Container Linux Config provided at boot:\n\n\nsystemd:\n  units:\n    - name: sshd.socket\n      dropins:\n      - name: 10-sshd-port.conf\n        contents: |\n          [Socket]\n          ListenStream=\n          ListenStream=222\n\n\n\nsshd\n will now listen only on port 222 on all interfaces when the system is built.\n\n\nDisabling socket activation for sshd\n\u00b6\n\n\nIt may be desirable to disable socket-activation for sshd to ensure it will reliably accept connections even when systemd or dbus aren't operating correctly.\n\n\nTo configure sshd on Flatcar Container Linux without socket activation, a Container Linux Config file similar to the following may be used:\n\n\nsystemd:\n  units:\n  - name: sshd.service\n    enable: true\n  - name: sshd.socket\n    mask: true\n\n\n\nNote that in this configuration the port will be configured by updating the \n/etc/ssh/sshd_config\n file with the \nPort\n directive rather than via \nsshd.socket\n.\n\n\nFurther reading\n\u00b6\n\n\nRead the \nfull Container Linux Config\n guide for more details on working with Container Linux Configs, including setting user's ssh keys.\n\n\nCustomizing sshd after first boot\n\u00b6\n\n\nSince \nContainer Linux Configs\n are only applied on first boot, existing machines will have to be configured in a different way.\n\n\nThe following sections walk through applying the same changes documented above on a running machine.\n\n\nNote\n: To avoid incidentally locking yourself out of the machine, it's a good idea to double-check you're able to directly login to the machine's console, if applicable.\n\n\nCustomizing sshd_config\n\u00b6\n\n\nSince \n/etc/ssh/sshd_config\n is a symlink to a read only file in \n/usr\n, it\nneeds to be replaced with a regular file before it may be edited.\n\n\nThis, for example, can be done by running \nsudo sed -i '' /etc/ssh/sshd_config\n.\n\n\nAt this point, any configuration changes can easily be applied by editing the file \n/etc/ssh/sshd_config\n.\n\n\nChanging the sshd port\n\u00b6\n\n\nThe sshd.socket unit may be configured via systemd \ndropins\n.\n\n\nTo change how sshd listens, update the list of \nListenStream\ns in the \n[Socket]\n section of the dropin.\n\n\nNote\n: \nListenStream\n is a list of values with each line adding to the list. An empty value clears the list, which is why \nListenStream=\n is necessary to prevent it from \nalso\n listening on the default port \n22\n.\n\n\nTo change just the listened-to port (in this example, port 222), create a dropin at \n/etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n\n\n# /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222\n\n\n\nTo change the listened-to IP address (in this example, 10.20.30.40):\n\n\n# /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=10.20.30.40:22\nFreeBind=true\n\n\n\nYou can specify both an IP and an alternate port in a single \nListenStream\n line. IPv6 address bindings would be specified using the format \n[2001:db8::7]:22\n.\n\n\nNote\n: While specifying an IP address is optional, you must always specify the port, even if it is the default SSH port. The \nFreeBind\n option is used to allow the socket to be bound on addresses that are not yet configured on an interface, to avoid issues caused by delays in IP configuration at boot. (This option is required only if you are specifying an address.)\n\n\nMultiple ListenStream lines can be specified, in which case \nsshd\n will listen on all the specified sockets:\n\n\n# /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222\nListenStream=10.20.30.40:223\nFreeBind=true\n\n\n\nActivating changes\n\u00b6\n\n\nAfter creating the dropin file, the changes can be activated by doing a daemon-reload and restarting \nsshd.socket\n\n\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart sshd.socket\n\n\n\nWe now see that systemd is listening on the new sockets:\n\n\n$ systemctl status sshd.socket\n\u25cf sshd.socket - OpenSSH Server Socket\n   Loaded: loaded (/etc/systemd/system/sshd.socket; disabled; vendor preset: disabled)\n   Active: active (listening) since Wed 2015-10-14 21:04:31 UTC; 2min 45s ago\n   Listen: [::]:222 (Stream)\n           10.20.30.40:223 (Stream)\n Accepted: 1; Connected: 0\n...\n\n\n\nAnd if we attempt to connect to port 22 on our public IP, the connection is rejected, but port 222 works:\n\n\n$ ssh core@[public IP]\nssh: connect to host [public IP] port 22: Connection refused\n$ ssh -p 222 core@[public IP]\nFlatcar Container Linux by Kinvolk stable (1353.8.0)\ncore@machine $\n\n\n\nDisabling socket-activation for sshd\n\u00b6\n\n\nSimply mask the systemd.socket unit:\n\n\n# systemctl mask --now sshd.socket\n\n\n\nFinally, restart the sshd.service unit:\n\n\n# systemctl restart sshd.service\n\n\n\nFurther reading on systemd units\n\u00b6\n\n\nFor more information about configuring Flatcar Container Linux hosts with \nsystemd\n, see \nGetting Started with systemd\n.",
            "title": "Customizing the SSH daemon"
        },
        {
            "location": "/os/customizing-sshd/#customizing-the-ssh-daemon",
            "text": "Flatcar Container Linux defaults to running an OpenSSH daemon using  systemd  socket activation -- when a client connects to the port configured for SSH,  sshd  is started on the fly for that client using a  systemd  unit derived automatically from a template. In some cases you may want to customize this daemon's authentication methods or other configuration. This guide will show you how to do that at boot time using a  Container Linux Config , and after building by modifying the  systemd  unit file.  As a practical example, when a client fails to connect by not completing the TCP connection (e.g. because the \"client\" is actually a TCP port scanner), the MOTD may report failures of  systemd  units (which will be named by the source IP that failed to connect) next time you log in to the Flatcar Container Linux host. These failures are not themselves harmful, but it is a good general practice to change how SSH listens, either by changing the IP address  sshd  listens to from the default setting (which listens on all configured interfaces), changing the default port, or both.",
            "title": "Customizing the SSH daemon"
        },
        {
            "location": "/os/customizing-sshd/#customizing-sshd-with-a-container-linux-config",
            "text": "In this example we will disable logins for the  root  user, only allow login for the  core  user and disable password based authentication. For more details on what sections can be added to  /etc/ssh/sshd_config  see the  OpenSSH manual .\nIf you're interested in additional security options, Mozilla provides a well-commented example of a  hardened configuration .  storage:\n  files:\n    - path: /etc/ssh/sshd_config\n      filesystem: root\n      mode: 0600\n      contents:\n        inline: |\n          # Use most defaults for sshd configuration.\n          UsePrivilegeSeparation sandbox\n          Subsystem sftp internal-sftp\n          UseDNS no\n\n          PermitRootLogin no\n          AllowUsers core\n          AuthenticationMethods publickey",
            "title": "Customizing sshd with a Container Linux Config"
        },
        {
            "location": "/os/customizing-sshd/#changing-the-sshd-port",
            "text": "Flatcar Container Linux ships with socket-activated SSH daemon by default. The configuration for this can be found at  /usr/lib/systemd/system/sshd.socket . We're going to override some of the default settings for this in the Container Linux Config provided at boot:  systemd:\n  units:\n    - name: sshd.socket\n      dropins:\n      - name: 10-sshd-port.conf\n        contents: |\n          [Socket]\n          ListenStream=\n          ListenStream=222  sshd  will now listen only on port 222 on all interfaces when the system is built.",
            "title": "Changing the sshd port"
        },
        {
            "location": "/os/customizing-sshd/#disabling-socket-activation-for-sshd",
            "text": "It may be desirable to disable socket-activation for sshd to ensure it will reliably accept connections even when systemd or dbus aren't operating correctly.  To configure sshd on Flatcar Container Linux without socket activation, a Container Linux Config file similar to the following may be used:  systemd:\n  units:\n  - name: sshd.service\n    enable: true\n  - name: sshd.socket\n    mask: true  Note that in this configuration the port will be configured by updating the  /etc/ssh/sshd_config  file with the  Port  directive rather than via  sshd.socket .",
            "title": "Disabling socket activation for sshd"
        },
        {
            "location": "/os/customizing-sshd/#further-reading",
            "text": "Read the  full Container Linux Config  guide for more details on working with Container Linux Configs, including setting user's ssh keys.",
            "title": "Further reading"
        },
        {
            "location": "/os/customizing-sshd/#customizing-sshd-after-first-boot",
            "text": "Since  Container Linux Configs  are only applied on first boot, existing machines will have to be configured in a different way.  The following sections walk through applying the same changes documented above on a running machine.  Note : To avoid incidentally locking yourself out of the machine, it's a good idea to double-check you're able to directly login to the machine's console, if applicable.",
            "title": "Customizing sshd after first boot"
        },
        {
            "location": "/os/customizing-sshd/#customizing-sshd95config",
            "text": "Since  /etc/ssh/sshd_config  is a symlink to a read only file in  /usr , it\nneeds to be replaced with a regular file before it may be edited.  This, for example, can be done by running  sudo sed -i '' /etc/ssh/sshd_config .  At this point, any configuration changes can easily be applied by editing the file  /etc/ssh/sshd_config .",
            "title": "Customizing sshd_config"
        },
        {
            "location": "/os/customizing-sshd/#changing-the-sshd-port_1",
            "text": "The sshd.socket unit may be configured via systemd  dropins .  To change how sshd listens, update the list of  ListenStream s in the  [Socket]  section of the dropin.  Note :  ListenStream  is a list of values with each line adding to the list. An empty value clears the list, which is why  ListenStream=  is necessary to prevent it from  also  listening on the default port  22 .  To change just the listened-to port (in this example, port 222), create a dropin at  /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf  # /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222  To change the listened-to IP address (in this example, 10.20.30.40):  # /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=10.20.30.40:22\nFreeBind=true  You can specify both an IP and an alternate port in a single  ListenStream  line. IPv6 address bindings would be specified using the format  [2001:db8::7]:22 .  Note : While specifying an IP address is optional, you must always specify the port, even if it is the default SSH port. The  FreeBind  option is used to allow the socket to be bound on addresses that are not yet configured on an interface, to avoid issues caused by delays in IP configuration at boot. (This option is required only if you are specifying an address.)  Multiple ListenStream lines can be specified, in which case  sshd  will listen on all the specified sockets:  # /etc/systemd/system/sshd.socket.d/10-sshd-listen-ports.conf\n[Socket]\nListenStream=\nListenStream=222\nListenStream=10.20.30.40:223\nFreeBind=true",
            "title": "Changing the sshd port"
        },
        {
            "location": "/os/customizing-sshd/#activating-changes",
            "text": "After creating the dropin file, the changes can be activated by doing a daemon-reload and restarting  sshd.socket  $ sudo systemctl daemon-reload\n$ sudo systemctl restart sshd.socket  We now see that systemd is listening on the new sockets:  $ systemctl status sshd.socket\n\u25cf sshd.socket - OpenSSH Server Socket\n   Loaded: loaded (/etc/systemd/system/sshd.socket; disabled; vendor preset: disabled)\n   Active: active (listening) since Wed 2015-10-14 21:04:31 UTC; 2min 45s ago\n   Listen: [::]:222 (Stream)\n           10.20.30.40:223 (Stream)\n Accepted: 1; Connected: 0\n...  And if we attempt to connect to port 22 on our public IP, the connection is rejected, but port 222 works:  $ ssh core@[public IP]\nssh: connect to host [public IP] port 22: Connection refused\n$ ssh -p 222 core@[public IP]\nFlatcar Container Linux by Kinvolk stable (1353.8.0)\ncore@machine $",
            "title": "Activating changes"
        },
        {
            "location": "/os/customizing-sshd/#disabling-socket-activation-for-sshd_1",
            "text": "Simply mask the systemd.socket unit:  # systemctl mask --now sshd.socket  Finally, restart the sshd.service unit:  # systemctl restart sshd.service",
            "title": "Disabling socket-activation for sshd"
        },
        {
            "location": "/os/customizing-sshd/#further-reading-on-systemd-units",
            "text": "For more information about configuring Flatcar Container Linux hosts with  systemd , see  Getting Started with systemd .",
            "title": "Further reading on systemd units"
        },
        {
            "location": "/os/developer-guides/",
            "text": "Developer Guides\n\u00b6\n\n\nMost users will never have to build Flatcar Container Linux from source or modify it in any way. If you have a need to modify Flatcar Container Linux, we provide an SDK that allows you to build your own developer images. We also provide OEM functionality for cloud providers and other companies that must customize Flatcar Container Linux to run within their environment.\n\n\n\n\nModifying Flatcar Container Linux\n\n\nBuilding production images\n\n\nBuilding custom kernel modules\n\n\nSDK tips and tricks\n\n\nDisk layout\n\n\nKola integration testing framework",
            "title": "Developer Guides"
        },
        {
            "location": "/os/developer-guides/#developer-guides",
            "text": "Most users will never have to build Flatcar Container Linux from source or modify it in any way. If you have a need to modify Flatcar Container Linux, we provide an SDK that allows you to build your own developer images. We also provide OEM functionality for cloud providers and other companies that must customize Flatcar Container Linux to run within their environment.   Modifying Flatcar Container Linux  Building production images  Building custom kernel modules  SDK tips and tricks  Disk layout  Kola integration testing framework",
            "title": "Developer Guides"
        },
        {
            "location": "/os/generate-self-signed-certificates/",
            "text": "Generate self-signed certificates\n\u00b6\n\n\nIf you build Flatcar Container Linux cluster on top of public networks it is recommended to enable encryption for Flatcar Container Linux services to prevent traffic interception and man-in-the-middle attacks. For these purposes you have to use Certificate Authority (CA), private keys and certificates signed by CA. Let's use \ncfssl\n and walk through the whole process to create all these components.\n\n\nNOTE:\n We will use basic procedure here. If your configuration requires advanced security options, please refer to official \ncfssl\n documentation.\n\n\nDownload cfssl\n\u00b6\n\n\nCloudFlare's distributes \ncfssl\n source code on github page and binaries on \ncfssl website\n.\n\n\nOur documentation assumes that you will run \ncfssl\n on your local x86_64 Linux host.\n\n\nmkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin\n\n\n\nInitialize a certificate authority\n\u00b6\n\n\nFirst of all we have to save default \ncfssl\n options for future substitutions:\n\n\nmkdir ~/cfssl\ncd ~/cfssl\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json\n\n\n\nCertificate types which are used inside Flatcar Container Linux\n\u00b6\n\n\n\n\nclient certificate\n is used to authenticate client by server. For example \netcdctl\n, \netcd proxy\n, or \ndocker\n clients.\n\n\nserver certificate\n is used by server and verified by client for server identity. For example \ndocker\n server or \nkube-apiserver\n.\n\n\npeer certificate\n is used by etcd cluster members as they communicate with each other in both ways.\n\n\n\n\nConfigure CA options\n\u00b6\n\n\nNow we can configure signing options inside \nca-config.json\n config file. Default options contain following preconfigured fields:\n\n\n\n\nprofiles: \nwww\n with \nserver auth\n (TLS Web Server Authentication) X509 V3 extension and \nclient\n with \nclient auth\n (TLS Web Client Authentication) X509 V3 extension.\n\n\nexpiry: with \n8760h\n default value (or 365 days)\n\n\n\n\nFor compliance let's rename \nwww\n profile into \nserver\n, create additional \npeer\n profile with both \nserver auth\n and \nclient auth\n extensions, and set expiry to 43800h (5 years):\n\n\n{\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"43800h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}\n\n\n\nYou can also modify \nca-csr.json\n Certificate Signing Request (CSR):\n\n\n{\n    \"CN\": \"My own CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"O\": \"My Company Name\",\n            \"ST\": \"San Francisco\",\n            \"OU\": \"Org Unit 1\",\n            \"OU\": \"Org Unit 2\"\n        }\n    ]\n}\n\n\n\nAnd generate CA with defined options:\n\n\ncfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\n\n\nYou'll get following files:\n\n\nca-key.pem\nca.csr\nca.pem\n\n\n\n\n\nPlease keep \nca-key.pem\n file in safe. This key allows to create any kind of certificates within your CA.\n\n\n*.csr\n files are not used in our example.\n\n\n\n\nGenerate server certificate\n\u00b6\n\n\ncfssl print-defaults csr > server.json\n\n\n\nMost important values for server certificate are \nCommon Name (CN)\n and \nhosts\n. We have to substitute them, for example:\n\n\n...\n    \"CN\": \"coreos1\",\n    \"hosts\": [\n        \"192.168.122.68\",\n        \"ext.example.com\",\n        \"coreos1.local\",\n        \"coreos1\"\n    ],\n...\n\n\n\nNow we are ready to generate server certificate and private key:\n\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server\n\n\n\nOr without CSR json file:\n\n\necho '{\"CN\":\"coreos1\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=\"192.168.122.68,ext.example.com,coreos1.local,coreos1\" - | cfssljson -bare server\n\n\n\nYou'll get following files:\n\n\nserver-key.pem\nserver.csr\nserver.pem\n\n\n\nGenerate peer certificate\n\u00b6\n\n\ncfssl print-defaults csr > member1.json\n\n\n\nSubstitute CN and hosts values, for example:\n\n\n...\n    \"CN\": \"member1\",\n    \"hosts\": [\n        \"192.168.122.101\",\n        \"ext.example.com\",\n        \"member1.local\",\n        \"member1\"\n    ],\n...\n\n\n\nNow we are ready to generate member1 certificate and private key:\n\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1\n\n\n\nOr without CSR json file:\n\n\necho '{\"CN\":\"member1\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=\"192.168.122.101,ext.example.com,member1.local,member1\" - | cfssljson -bare member1\n\n\n\nYou'll get following files:\n\n\nmember1-key.pem\nmember1.csr\nmember1.pem\n\n\n\nRepeat these steps for each \netcd\n member hostname.\n\n\nGenerate client certificate\n\u00b6\n\n\ncfssl print-defaults csr > client.json\n\n\n\nFor client certificate we can ignore \nhosts\n values and set only \nCommon Name (CN)\n to \nclient\n value:\n\n\n...\n    \"CN\": \"client\",\n    \"hosts\": [\"\"],\n...\n\n\n\nGenerate client certificate:\n\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client\n\n\n\nOr without CSR json file:\n\n\necho '{\"CN\":\"client\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client\n\n\n\nYou'll get following files:\n\n\nclient-key.pem\nclient.csr\nclient.pem\n\n\n\nTLDR\n\u00b6\n\n\nDownload binaries\n\u00b6\n\n\nmkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin\n\n\n\nCreate directory to store certificates:\n\u00b6\n\n\nmkdir ~/cfssl\ncd ~/cfssl\n\n\n\nGenerate CA and certificates\n\u00b6\n\n\necho '{\"CN\":\"CA\",\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -initca - | cfssljson -bare ca -\necho '{\"signing\":{\"default\":{\"expiry\":\"43800h\",\"usages\":[\"signing\",\"key encipherment\",\"server auth\",\"client auth\"]}}}' > ca-config.json\nexport ADDRESS=192.168.122.68,ext1.example.com,coreos1.local,coreos1\nexport NAME=server\necho '{\"CN\":\"'$NAME'\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -config=ca-config.json -ca=ca.pem -ca-key=ca-key.pem -hostname=\"$ADDRESS\" - | cfssljson -bare $NAME\nexport ADDRESS=\nexport NAME=client\necho '{\"CN\":\"'$NAME'\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -config=ca-config.json -ca=ca.pem -ca-key=ca-key.pem -hostname=\"$ADDRESS\" - | cfssljson -bare $NAME\n\n\n\nVerify data\n\u00b6\n\n\nopenssl x509 -in ca.pem -text -noout\nopenssl x509 -in server.pem -text -noout\nopenssl x509 -in client.pem -text -noout\n\n\n\nThings to know\n\u00b6\n\n\n\n\nDon't put your \nca-key.pem\n into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.\n\n\nKeep \nkey\n files in safe. Don't forget to set proper file permissions, i.e. \nchmod 0600 server-key.pem\n.\n\n\nCertificates in this \nTLDR\n example have both \nserver auth\n and \nclient auth\n X509 V3 extensions and you can use them with servers and clients' authentication.\n\n\nYou are free to generate keys and certificates for wildcard \n*\n address as well. They will work on any machine. It will simplify certificates routine but increase security risks.\n\n\n\n\nMore information\n\u00b6\n\n\nFor another examples, check out these documents:\n\n\nCustom Certificate Authorities\n\n\netcd Security Model",
            "title": "Generate self-signed certificates"
        },
        {
            "location": "/os/generate-self-signed-certificates/#generate-self-signed-certificates",
            "text": "If you build Flatcar Container Linux cluster on top of public networks it is recommended to enable encryption for Flatcar Container Linux services to prevent traffic interception and man-in-the-middle attacks. For these purposes you have to use Certificate Authority (CA), private keys and certificates signed by CA. Let's use  cfssl  and walk through the whole process to create all these components.  NOTE:  We will use basic procedure here. If your configuration requires advanced security options, please refer to official  cfssl  documentation.",
            "title": "Generate self-signed certificates"
        },
        {
            "location": "/os/generate-self-signed-certificates/#download-cfssl",
            "text": "CloudFlare's distributes  cfssl  source code on github page and binaries on  cfssl website .  Our documentation assumes that you will run  cfssl  on your local x86_64 Linux host.  mkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin",
            "title": "Download cfssl"
        },
        {
            "location": "/os/generate-self-signed-certificates/#initialize-a-certificate-authority",
            "text": "First of all we have to save default  cfssl  options for future substitutions:  mkdir ~/cfssl\ncd ~/cfssl\ncfssl print-defaults config > ca-config.json\ncfssl print-defaults csr > ca-csr.json",
            "title": "Initialize a certificate authority"
        },
        {
            "location": "/os/generate-self-signed-certificates/#certificate-types-which-are-used-inside-flatcar-container-linux",
            "text": "client certificate  is used to authenticate client by server. For example  etcdctl ,  etcd proxy , or  docker  clients.  server certificate  is used by server and verified by client for server identity. For example  docker  server or  kube-apiserver .  peer certificate  is used by etcd cluster members as they communicate with each other in both ways.",
            "title": "Certificate types which are used inside Flatcar Container Linux"
        },
        {
            "location": "/os/generate-self-signed-certificates/#configure-ca-options",
            "text": "Now we can configure signing options inside  ca-config.json  config file. Default options contain following preconfigured fields:   profiles:  www  with  server auth  (TLS Web Server Authentication) X509 V3 extension and  client  with  client auth  (TLS Web Client Authentication) X509 V3 extension.  expiry: with  8760h  default value (or 365 days)   For compliance let's rename  www  profile into  server , create additional  peer  profile with both  server auth  and  client auth  extensions, and set expiry to 43800h (5 years):  {\n    \"signing\": {\n        \"default\": {\n            \"expiry\": \"43800h\"\n        },\n        \"profiles\": {\n            \"server\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\"\n                ]\n            },\n            \"client\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"client auth\"\n                ]\n            },\n            \"peer\": {\n                \"expiry\": \"43800h\",\n                \"usages\": [\n                    \"signing\",\n                    \"key encipherment\",\n                    \"server auth\",\n                    \"client auth\"\n                ]\n            }\n        }\n    }\n}  You can also modify  ca-csr.json  Certificate Signing Request (CSR):  {\n    \"CN\": \"My own CA\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n        {\n            \"C\": \"US\",\n            \"L\": \"CA\",\n            \"O\": \"My Company Name\",\n            \"ST\": \"San Francisco\",\n            \"OU\": \"Org Unit 1\",\n            \"OU\": \"Org Unit 2\"\n        }\n    ]\n}  And generate CA with defined options:  cfssl gencert -initca ca-csr.json | cfssljson -bare ca -  You'll get following files:  ca-key.pem\nca.csr\nca.pem   Please keep  ca-key.pem  file in safe. This key allows to create any kind of certificates within your CA.  *.csr  files are not used in our example.",
            "title": "Configure CA options"
        },
        {
            "location": "/os/generate-self-signed-certificates/#generate-server-certificate",
            "text": "cfssl print-defaults csr > server.json  Most important values for server certificate are  Common Name (CN)  and  hosts . We have to substitute them, for example:  ...\n    \"CN\": \"coreos1\",\n    \"hosts\": [\n        \"192.168.122.68\",\n        \"ext.example.com\",\n        \"coreos1.local\",\n        \"coreos1\"\n    ],\n...  Now we are ready to generate server certificate and private key:  cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server  Or without CSR json file:  echo '{\"CN\":\"coreos1\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server -hostname=\"192.168.122.68,ext.example.com,coreos1.local,coreos1\" - | cfssljson -bare server  You'll get following files:  server-key.pem\nserver.csr\nserver.pem",
            "title": "Generate server certificate"
        },
        {
            "location": "/os/generate-self-signed-certificates/#generate-peer-certificate",
            "text": "cfssl print-defaults csr > member1.json  Substitute CN and hosts values, for example:  ...\n    \"CN\": \"member1\",\n    \"hosts\": [\n        \"192.168.122.101\",\n        \"ext.example.com\",\n        \"member1.local\",\n        \"member1\"\n    ],\n...  Now we are ready to generate member1 certificate and private key:  cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer member1.json | cfssljson -bare member1  Or without CSR json file:  echo '{\"CN\":\"member1\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer -hostname=\"192.168.122.101,ext.example.com,member1.local,member1\" - | cfssljson -bare member1  You'll get following files:  member1-key.pem\nmember1.csr\nmember1.pem  Repeat these steps for each  etcd  member hostname.",
            "title": "Generate peer certificate"
        },
        {
            "location": "/os/generate-self-signed-certificates/#generate-client-certificate",
            "text": "cfssl print-defaults csr > client.json  For client certificate we can ignore  hosts  values and set only  Common Name (CN)  to  client  value:  ...\n    \"CN\": \"client\",\n    \"hosts\": [\"\"],\n...  Generate client certificate:  cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client  Or without CSR json file:  echo '{\"CN\":\"client\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client - | cfssljson -bare client  You'll get following files:  client-key.pem\nclient.csr\nclient.pem",
            "title": "Generate client certificate"
        },
        {
            "location": "/os/generate-self-signed-certificates/#tldr",
            "text": "",
            "title": "TLDR"
        },
        {
            "location": "/os/generate-self-signed-certificates/#download-binaries",
            "text": "mkdir ~/bin\ncurl -s -L -o ~/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\ncurl -s -L -o ~/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\nchmod +x ~/bin/{cfssl,cfssljson}\nexport PATH=$PATH:~/bin",
            "title": "Download binaries"
        },
        {
            "location": "/os/generate-self-signed-certificates/#create-directory-to-store-certificates",
            "text": "mkdir ~/cfssl\ncd ~/cfssl",
            "title": "Create directory to store certificates:"
        },
        {
            "location": "/os/generate-self-signed-certificates/#generate-ca-and-certificates",
            "text": "echo '{\"CN\":\"CA\",\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -initca - | cfssljson -bare ca -\necho '{\"signing\":{\"default\":{\"expiry\":\"43800h\",\"usages\":[\"signing\",\"key encipherment\",\"server auth\",\"client auth\"]}}}' > ca-config.json\nexport ADDRESS=192.168.122.68,ext1.example.com,coreos1.local,coreos1\nexport NAME=server\necho '{\"CN\":\"'$NAME'\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -config=ca-config.json -ca=ca.pem -ca-key=ca-key.pem -hostname=\"$ADDRESS\" - | cfssljson -bare $NAME\nexport ADDRESS=\nexport NAME=client\necho '{\"CN\":\"'$NAME'\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -config=ca-config.json -ca=ca.pem -ca-key=ca-key.pem -hostname=\"$ADDRESS\" - | cfssljson -bare $NAME",
            "title": "Generate CA and certificates"
        },
        {
            "location": "/os/generate-self-signed-certificates/#verify-data",
            "text": "openssl x509 -in ca.pem -text -noout\nopenssl x509 -in server.pem -text -noout\nopenssl x509 -in client.pem -text -noout",
            "title": "Verify data"
        },
        {
            "location": "/os/generate-self-signed-certificates/#things-to-know",
            "text": "Don't put your  ca-key.pem  into a Container Linux Config, it is recommended to store it in safe place. This key allows to generate as much certificates as possible.  Keep  key  files in safe. Don't forget to set proper file permissions, i.e.  chmod 0600 server-key.pem .  Certificates in this  TLDR  example have both  server auth  and  client auth  X509 V3 extensions and you can use them with servers and clients' authentication.  You are free to generate keys and certificates for wildcard  *  address as well. They will work on any machine. It will simplify certificates routine but increase security risks.",
            "title": "Things to know"
        },
        {
            "location": "/os/generate-self-signed-certificates/#more-information",
            "text": "For another examples, check out these documents:  Custom Certificate Authorities  etcd Security Model",
            "title": "More information"
        },
        {
            "location": "/os/getting-started-with-docker/",
            "text": "Getting started with Docker\n\u00b6\n\n\nDocker is an open-source project that makes creating and managing Linux containers really easy. Containers are like extremely lightweight VMs \u2013 they allow code to run in isolation from other containers but safely share the machine\u2019s resources, all without the overhead of a hypervisor.\n\n\nDocker containers can boot extremely fast (in milliseconds!) which gives you unprecedented flexibility in managing load across your cluster. For example, instead of running chef on each of your VMs, it\u2019s faster and more reliable to have your build system create a container and launch it on the appropriate number of Flatcar Container Linux hosts. This guide will show you how to launch a container, install some software on it, commit that container, and optionally launch it on another Flatcar Container Linux machine. Before starting, make sure you've got at least one Flatcar Container Linux machine up and running \u2014 try it on \nAmazon EC2\n or locally with \nVagrant\n.\n\n\nDocker CLI basics\n\u00b6\n\n\nDocker has a \nstraightforward CLI\n that allows you to do almost everything you could want to a container. All of these commands use the image id (ex. be29975e0098), the image name (ex. myusername/webapp) and the container id (ex. 72d468f455ea) interchangeably depending on the operation you are trying to do. This is confusing at first, so pay special attention to what you're using.\n\n\nLaunching a container\n\u00b6\n\n\nLaunching a container is simple as \ndocker run\n + the image name you would like to run + the command to run within the container. If the image doesn't exist on your local machine, Docker will attempt to fetch it from the public image registry. Later we'll explore how to use Docker with a private registry. It's important to note that containers are designed to stop once the command executed within them has exited. For example, if you ran \n/bin/echo hello world\n as your command, the container will start, print hello world and then stop:\n\n\ndocker run ubuntu /bin/echo hello world\n\n\n\nLet's launch an Ubuntu container and install Apache inside of it using the bash prompt:\n\n\ndocker run -t -i ubuntu /bin/bash\n\n\n\nThe \n-t\n and \n-i\n flags allocate a pseudo-tty and keep stdin open even if not attached. This will allow you to use the container like a traditional VM as long as the bash prompt is running. Install Apache with \napt-get update && apt-get install apache2\n. You're probably wondering what address you can connect to in order to test that Apache was correctly installed...we'll get to that after we commit the container.\n\n\nCommitting a container\n\u00b6\n\n\nAfter that completes, we need to \ncommit\n these changes to our container with the container ID and the image name.\n\n\nTo find the container ID, open another shell (so the container is still running) and read the ID using \ndocker ps\n.\n\n\nThe image name is in the format of \nusername/name\n. We're going to use \ncoreos\n as our username in this example but you should \nsign up for a Docker.IO user account\n and use that instead.\n\n\nIt's important to note that you can commit using any username and image name locally, but to push an image to the public registry, the username must be a valid \nDocker.IO user account\n.\n\n\nCommit the container with the container ID, your username, and the name \napache\n:\n\n\ndocker commit 72d468f455ea coreos/apache\n\n\n\nThe overlay filesystem works similar to git: our image now builds off of the \nubuntu\n base and adds another layer with Apache on top. These layers get cached separately so that you won't have to pull down the ubuntu base more than once.\n\n\nKeeping the Apache container running\n\u00b6\n\n\nNow we have our Ubuntu container with Apache running in one shell and an image of that container sitting on disk. Let's launch a new container based on that image but set it up to keep running indefinitely. The basic syntax looks like this, but we need to configure a few additional options that we'll fill in as we go:\n\n\ndocker run [options] [image] [process]\n\n\n\nThe first step is to tell Docker that we want to run our \ncoreos/apache\n image:\n\n\ndocker run [options] coreos/apache [process]\n\n\n\nRun container detached\n\u00b6\n\n\nWhen running Docker containers manually, the most important option is to run the container in detached mode with the \n-d\n flag. This will output the container ID to show that the command was successful, but nothing else. At any time you can run \ndocker ps\n in the other shell to view a list of the running containers. Our command now looks like:\n\n\ndocker run -d coreos/apache [process]\n\n\n\nAfter you are comfortable with the mechanics of running containers by hand, it's recommended to use \nsystemd units\n to run your containers on a cluster of Flatcar Container Linux machines.\n\n\nDo not run containers with detached mode inside of systemd unit files. Detached mode prevents your init system, in our case systemd, from monitoring the process that owns the container because detached mode forks it into the background. To prevent this issue, just omit the \n-d\n flag if you aren't running something manually.\n\n\nRun Apache in foreground\n\u00b6\n\n\nWe need to run the apache process in the foreground, since our container will stop when the process specified in the \ndocker run\n command stops. We can do this with a flag \n-D\n when starting the apache2 process:\n\n\n/usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nLet's add that to our command:\n\n\ndocker run -d coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nPermanently running a container\n\u00b6\n\n\nWhile the sections above explained how to run a container when configuring it, for a production setup, you should not manually start and babysit containers.\n\n\nInstead, create a systemd unit file to make systemd keep that container running. See the \nGetting Started with systemd\n for details.\n\n\nNetwork access to 80\n\u00b6\n\n\nThe default apache install will be running on port 80. To give our container access to traffic over port 80, we use the \n-p\n flag and specify the port on the host that maps to the port inside the container. In our case we want 80 for each, so we include \n-p 80:80\n in our command:\n\n\ndocker run -d -p 80:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nYou can now run this command on your Flatcar Container Linux host to create the container. You should see the default apache webpage when you load either \nlocalhost:80\n or the IP of your remote server. Be sure that any firewall or EC2 Security Group allows traffic to port 80.\n\n\nUsing the Docker registry\n\u00b6\n\n\nEarlier we downloaded the ubuntu image remotely from the Docker public registry because it didn't exist on our local machine. We can also push local images to the public registry (or a private registry) very easily with the \npush\n command:\n\n\ndocker push coreos/apache\n\n\n\nTo push to a private repository the syntax is very similar. First, we must prefix our image with the host running our private registry instead of our username. List images by running \ndocker images\n and insert the correct ID into the \ntag\n command:\n\n\ndocker tag f455ea72d468 registry.example.com:5000/apache\n\n\n\nAfter tagging, the image needs to be pushed to the registry:\n\n\ndocker push registry.example.com:5000/apache\n\n\n\nOnce the image is done uploading, you should be able to start the exact same container on a different Flatcar Container Linux host by running:\n\n\ndocker run -d -p 80:80 registry.example.com:5000/apache /usr/sbin/apache2ctl -D FOREGROUND\n\n\n\nMore information\n\u00b6\n\n\nDocker Overview\n\n\nDocker Website\n\n\ndocker's Getting Started Guide",
            "title": "Getting started with Docker"
        },
        {
            "location": "/os/getting-started-with-docker/#getting-started-with-docker",
            "text": "Docker is an open-source project that makes creating and managing Linux containers really easy. Containers are like extremely lightweight VMs \u2013 they allow code to run in isolation from other containers but safely share the machine\u2019s resources, all without the overhead of a hypervisor.  Docker containers can boot extremely fast (in milliseconds!) which gives you unprecedented flexibility in managing load across your cluster. For example, instead of running chef on each of your VMs, it\u2019s faster and more reliable to have your build system create a container and launch it on the appropriate number of Flatcar Container Linux hosts. This guide will show you how to launch a container, install some software on it, commit that container, and optionally launch it on another Flatcar Container Linux machine. Before starting, make sure you've got at least one Flatcar Container Linux machine up and running \u2014 try it on  Amazon EC2  or locally with  Vagrant .",
            "title": "Getting started with Docker"
        },
        {
            "location": "/os/getting-started-with-docker/#docker-cli-basics",
            "text": "Docker has a  straightforward CLI  that allows you to do almost everything you could want to a container. All of these commands use the image id (ex. be29975e0098), the image name (ex. myusername/webapp) and the container id (ex. 72d468f455ea) interchangeably depending on the operation you are trying to do. This is confusing at first, so pay special attention to what you're using.",
            "title": "Docker CLI basics"
        },
        {
            "location": "/os/getting-started-with-docker/#launching-a-container",
            "text": "Launching a container is simple as  docker run  + the image name you would like to run + the command to run within the container. If the image doesn't exist on your local machine, Docker will attempt to fetch it from the public image registry. Later we'll explore how to use Docker with a private registry. It's important to note that containers are designed to stop once the command executed within them has exited. For example, if you ran  /bin/echo hello world  as your command, the container will start, print hello world and then stop:  docker run ubuntu /bin/echo hello world  Let's launch an Ubuntu container and install Apache inside of it using the bash prompt:  docker run -t -i ubuntu /bin/bash  The  -t  and  -i  flags allocate a pseudo-tty and keep stdin open even if not attached. This will allow you to use the container like a traditional VM as long as the bash prompt is running. Install Apache with  apt-get update && apt-get install apache2 . You're probably wondering what address you can connect to in order to test that Apache was correctly installed...we'll get to that after we commit the container.",
            "title": "Launching a container"
        },
        {
            "location": "/os/getting-started-with-docker/#committing-a-container",
            "text": "After that completes, we need to  commit  these changes to our container with the container ID and the image name.  To find the container ID, open another shell (so the container is still running) and read the ID using  docker ps .  The image name is in the format of  username/name . We're going to use  coreos  as our username in this example but you should  sign up for a Docker.IO user account  and use that instead.  It's important to note that you can commit using any username and image name locally, but to push an image to the public registry, the username must be a valid  Docker.IO user account .  Commit the container with the container ID, your username, and the name  apache :  docker commit 72d468f455ea coreos/apache  The overlay filesystem works similar to git: our image now builds off of the  ubuntu  base and adds another layer with Apache on top. These layers get cached separately so that you won't have to pull down the ubuntu base more than once.",
            "title": "Committing a container"
        },
        {
            "location": "/os/getting-started-with-docker/#keeping-the-apache-container-running",
            "text": "Now we have our Ubuntu container with Apache running in one shell and an image of that container sitting on disk. Let's launch a new container based on that image but set it up to keep running indefinitely. The basic syntax looks like this, but we need to configure a few additional options that we'll fill in as we go:  docker run [options] [image] [process]  The first step is to tell Docker that we want to run our  coreos/apache  image:  docker run [options] coreos/apache [process]",
            "title": "Keeping the Apache container running"
        },
        {
            "location": "/os/getting-started-with-docker/#run-container-detached",
            "text": "When running Docker containers manually, the most important option is to run the container in detached mode with the  -d  flag. This will output the container ID to show that the command was successful, but nothing else. At any time you can run  docker ps  in the other shell to view a list of the running containers. Our command now looks like:  docker run -d coreos/apache [process]  After you are comfortable with the mechanics of running containers by hand, it's recommended to use  systemd units  to run your containers on a cluster of Flatcar Container Linux machines.  Do not run containers with detached mode inside of systemd unit files. Detached mode prevents your init system, in our case systemd, from monitoring the process that owns the container because detached mode forks it into the background. To prevent this issue, just omit the  -d  flag if you aren't running something manually.",
            "title": "Run container detached"
        },
        {
            "location": "/os/getting-started-with-docker/#run-apache-in-foreground",
            "text": "We need to run the apache process in the foreground, since our container will stop when the process specified in the  docker run  command stops. We can do this with a flag  -D  when starting the apache2 process:  /usr/sbin/apache2ctl -D FOREGROUND  Let's add that to our command:  docker run -d coreos/apache /usr/sbin/apache2ctl -D FOREGROUND",
            "title": "Run Apache in foreground"
        },
        {
            "location": "/os/getting-started-with-docker/#permanently-running-a-container",
            "text": "While the sections above explained how to run a container when configuring it, for a production setup, you should not manually start and babysit containers.  Instead, create a systemd unit file to make systemd keep that container running. See the  Getting Started with systemd  for details.",
            "title": "Permanently running a container"
        },
        {
            "location": "/os/getting-started-with-docker/#network-access-to-80",
            "text": "The default apache install will be running on port 80. To give our container access to traffic over port 80, we use the  -p  flag and specify the port on the host that maps to the port inside the container. In our case we want 80 for each, so we include  -p 80:80  in our command:  docker run -d -p 80:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND  You can now run this command on your Flatcar Container Linux host to create the container. You should see the default apache webpage when you load either  localhost:80  or the IP of your remote server. Be sure that any firewall or EC2 Security Group allows traffic to port 80.",
            "title": "Network access to 80"
        },
        {
            "location": "/os/getting-started-with-docker/#using-the-docker-registry",
            "text": "Earlier we downloaded the ubuntu image remotely from the Docker public registry because it didn't exist on our local machine. We can also push local images to the public registry (or a private registry) very easily with the  push  command:  docker push coreos/apache  To push to a private repository the syntax is very similar. First, we must prefix our image with the host running our private registry instead of our username. List images by running  docker images  and insert the correct ID into the  tag  command:  docker tag f455ea72d468 registry.example.com:5000/apache  After tagging, the image needs to be pushed to the registry:  docker push registry.example.com:5000/apache  Once the image is done uploading, you should be able to start the exact same container on a different Flatcar Container Linux host by running:  docker run -d -p 80:80 registry.example.com:5000/apache /usr/sbin/apache2ctl -D FOREGROUND",
            "title": "Using the Docker registry"
        },
        {
            "location": "/os/getting-started-with-docker/#more-information",
            "text": "Docker Overview  Docker Website  docker's Getting Started Guide",
            "title": "More information"
        },
        {
            "location": "/os/getting-started-with-systemd/",
            "text": "Getting started with systemd\n\u00b6\n\n\nsystemd is an init system that provides many powerful features for starting, stopping, and managing processes. Within Flatcar Container Linux, you will almost exclusively use systemd to manage the lifecycle of your Docker containers.\n\n\nTerminology\n\u00b6\n\n\nsystemd consists of two main concepts: a unit and a target. A unit is a configuration file that describes the properties of the process that you'd like to run. This is normally a \ndocker run\n command or something similar. A target is a grouping mechanism that allows systemd to start up groups of processes at the same time. This happens at every boot as processes are started at different run levels.\n\n\nsystemd is the first process started on Flatcar Container Linux and it reads different targets and starts the processes specified which allows the operating system to start. The target that you'll interact with is the \nmulti-user.target\n which holds all of the general use unit files for our containers.\n\n\nEach target is actually a collection of symlinks to our unit files. This is specified in the unit file by \nWantedBy=multi-user.target\n. Running \nsystemctl enable foo.service\n creates symlinks to the unit inside \nmulti-user.target.wants\n.\n\n\nUnit file\n\u00b6\n\n\nOn Flatcar Container Linux, unit files are located at \n/etc/systemd/system\n. Let's create a simple unit named \nhello.service\n:\n\n\n[Unit]\nDescription=MyApp\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill busybox1\nExecStartPre=-/usr/bin/docker rm busybox1\nExecStartPre=/usr/bin/docker pull busybox\nExecStart=/usr/bin/docker run --name busybox1 busybox /bin/sh -c \"trap 'exit 0' INT TERM; while true; do echo Hello World; sleep 1; done\"\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nThe \nDescription\n shows up in the systemd log and a few other places. Write something that will help you understand exactly what this does later on.\n\n\nAfter=docker.service\n and \nRequires=docker.service\n means this unit will only start after \ndocker.service\n is active. You can define as many of these as you want.\n\n\nExecStart=\n allows you to specify any command that you'd like to run when this unit is started. The pid assigned to this process is what systemd will monitor to determine whether the process has crashed or not. Do not run docker containers with \n-d\n as this will prevent the container from starting as a child of this pid. systemd will think the process has exited and the unit will be stopped.\n\n\nWantedBy=\n is the target that this unit is a part of.\n\n\nTo start a new unit, we need to tell systemd to create the symlink and then start the file:\n\n\n$ sudo systemctl enable /etc/systemd/system/hello.service\n$ sudo systemctl start hello.service\n\n\n\nTo verify the unit started, you can see the list of containers running with \ndocker ps\n and read the unit's output with \njournalctl\n:\n\n\n$ journalctl -f -u hello.service\n-- Logs begin at Fri 2014-02-07 00:05:55 UTC. --\nFeb 11 17:46:26 localhost docker[23470]: Hello World\nFeb 11 17:46:27 localhost docker[23470]: Hello World\nFeb 11 17:46:28 localhost docker[23470]: Hello World\n...\n\n\n\nOverview of systemctl\n\n\nReading the System Log\n\n\nAdvanced unit files\n\u00b6\n\n\nsystemd provides a high degree of functionality in your unit files. Here's a curated list of useful features listed in the order they'll occur in the lifecycle of a unit:\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nExecStartPre\n\n\nCommands that will run before \nExecStart\n.\n\n\n\n\n\n\nExecStart\n\n\nMain commands to run for this unit.\n\n\n\n\n\n\nExecStartPost\n\n\nCommands that will run after all \nExecStart\n commands have completed.\n\n\n\n\n\n\nExecReload\n\n\nCommands that will run when this unit is reloaded via \nsystemctl reload foo.service\n\n\n\n\n\n\nExecStop\n\n\nCommands that will run when this unit is considered failed or if it is stopped via \nsystemctl stop foo.service\n\n\n\n\n\n\nExecStopPost\n\n\nCommands that will run after \nExecStop\n has completed.\n\n\n\n\n\n\nRestartSec\n\n\nThe amount of time to sleep before restarting a service. Useful to prevent your failed service from attempting to restart itself every 100ms.\n\n\n\n\n\n\n\n\nThe full list is located on the \nsystemd man page\n.\n\n\nLet's put a few of these concepts together to register new units within etcd. Imagine we had another container running that would read these values from etcd and act upon them.\n\n\nWe can use \nExecStartPre\n to scrub existing container state. The \ndocker kill\n will force any previous copy of this container to stop, which is useful if we restarted the unit but Docker didn't stop the container for some reason. The \n=-\n is systemd syntax to ignore errors for this command. We need to do this because Docker will return a non-zero exit code if we try to stop a container that doesn't exist. We don't consider this an error (because we want the container stopped) so we tell systemd to ignore the possible failure.\n\n\ndocker rm\n will remove the container and \ndocker pull\n will pull down the latest version. You can optionally pull down a specific version as a Docker tag: \ncoreos/apache:1.2.3\n\n\nExecStart\n is where the container is started from the container image that we pulled above.\n\n\nSince our container will be started in \nExecStart\n, it makes sense for our etcd command to run as \nExecStartPost\n to ensure that our container is started and functioning.\n\n\nWhen the service is told to stop, we need to stop the Docker container using its \n--name\n from the run command. We also need to clean up our etcd key when the container exits or the unit is failed by using \nExecStopPost\n.\n\n\n[Unit]\nDescription=My Advanced Service\nAfter=etcd2.service\nAfter=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill apache1\nExecStartPre=-/usr/bin/docker rm apache1\nExecStartPre=/usr/bin/docker pull coreos/apache\nExecStart=/usr/bin/docker run --name apache1 -p 8081:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running\nExecStop=/usr/bin/docker stop apache1\nExecStopPost=/usr/bin/etcdctl rm /domains/example.com/10.10.10.123:8081\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nWhile it's possible to manage the starting, stopping, and removal of the container in a single \nExecStart\n command by using \ndocker run --rm\n, it's a good idea to separate the container's lifecycle into \nExecStartPre\n, \nExecStart\n, and \nExecStop\n options as we've done above. This gives you a chance to inspect the container's state after it stops or fails.\n\n\nUnit specifiers\n\u00b6\n\n\nIn our last example we had to hardcode our IP address when we announced our container in etcd. That's not scalable and systemd has a few variables built in to help us out. Here's a few of the most useful:\n\n\n\n\n\n\n\n\nVariable\n\n\nMeaning\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%n\n\n\nFull unit name\n\n\nUseful if the name of your unit is unique enough to be used as an argument on a command.\n\n\n\n\n\n\n%m\n\n\nMachine ID\n\n\nUseful for namespacing etcd keys by machine. Example: \n/machines/%m/units\n\n\n\n\n\n\n%b\n\n\nBootID\n\n\nSimilar to the machine ID, but this value is random and changes on each boot\n\n\n\n\n\n\n%H\n\n\nHostname\n\n\nAllows you to run the same unit file across many machines. Useful for service discovery. Example: \n/domains/example.com/%H:8081\n\n\n\n\n\n\n\n\nA full list of specifiers can be found on the \nsystemd man page\n.\n\n\nInstantiated units\n\u00b6\n\n\nSince systemd is based on symlinks, there are a few interesting tricks you can leverage that are very powerful when used with containers. If you create multiple symlinks to the same unit file, the following variables become available to you:\n\n\n\n\n\n\n\n\nVariable\n\n\nMeaning\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n%p\n\n\nPrefix name\n\n\nRefers to any string before \n@\n in your unit name.\n\n\n\n\n\n\n%i\n\n\nInstance name\n\n\nRefers to the string between the \n@\n and the suffix.\n\n\n\n\n\n\n\n\nIn our earlier example we had to hardcode our IP address when registering within etcd:\n\n\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running\n\n\n\nWe can enhance this by using \n%H\n and \n%i\n to dynamically announce the hostname and port. Specify the port after the \n@\n by using two unit files named \nfoo@123.service\n and \nfoo@456.service\n:\n\n\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running\n\n\n\nThis gives us the flexibility to use a single unit file to announce multiple copies of the same container on a single machine (no port overlap) and on multiple machines (no hostname overlap).\n\n\nMore information\n\u00b6\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs\n\n\nsystemd.target Docs",
            "title": "Getting started with systemd"
        },
        {
            "location": "/os/getting-started-with-systemd/#getting-started-with-systemd",
            "text": "systemd is an init system that provides many powerful features for starting, stopping, and managing processes. Within Flatcar Container Linux, you will almost exclusively use systemd to manage the lifecycle of your Docker containers.",
            "title": "Getting started with systemd"
        },
        {
            "location": "/os/getting-started-with-systemd/#terminology",
            "text": "systemd consists of two main concepts: a unit and a target. A unit is a configuration file that describes the properties of the process that you'd like to run. This is normally a  docker run  command or something similar. A target is a grouping mechanism that allows systemd to start up groups of processes at the same time. This happens at every boot as processes are started at different run levels.  systemd is the first process started on Flatcar Container Linux and it reads different targets and starts the processes specified which allows the operating system to start. The target that you'll interact with is the  multi-user.target  which holds all of the general use unit files for our containers.  Each target is actually a collection of symlinks to our unit files. This is specified in the unit file by  WantedBy=multi-user.target . Running  systemctl enable foo.service  creates symlinks to the unit inside  multi-user.target.wants .",
            "title": "Terminology"
        },
        {
            "location": "/os/getting-started-with-systemd/#unit-file",
            "text": "On Flatcar Container Linux, unit files are located at  /etc/systemd/system . Let's create a simple unit named  hello.service :  [Unit]\nDescription=MyApp\nAfter=docker.service\nRequires=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill busybox1\nExecStartPre=-/usr/bin/docker rm busybox1\nExecStartPre=/usr/bin/docker pull busybox\nExecStart=/usr/bin/docker run --name busybox1 busybox /bin/sh -c \"trap 'exit 0' INT TERM; while true; do echo Hello World; sleep 1; done\"\n\n[Install]\nWantedBy=multi-user.target  The  Description  shows up in the systemd log and a few other places. Write something that will help you understand exactly what this does later on.  After=docker.service  and  Requires=docker.service  means this unit will only start after  docker.service  is active. You can define as many of these as you want.  ExecStart=  allows you to specify any command that you'd like to run when this unit is started. The pid assigned to this process is what systemd will monitor to determine whether the process has crashed or not. Do not run docker containers with  -d  as this will prevent the container from starting as a child of this pid. systemd will think the process has exited and the unit will be stopped.  WantedBy=  is the target that this unit is a part of.  To start a new unit, we need to tell systemd to create the symlink and then start the file:  $ sudo systemctl enable /etc/systemd/system/hello.service\n$ sudo systemctl start hello.service  To verify the unit started, you can see the list of containers running with  docker ps  and read the unit's output with  journalctl :  $ journalctl -f -u hello.service\n-- Logs begin at Fri 2014-02-07 00:05:55 UTC. --\nFeb 11 17:46:26 localhost docker[23470]: Hello World\nFeb 11 17:46:27 localhost docker[23470]: Hello World\nFeb 11 17:46:28 localhost docker[23470]: Hello World\n...  Overview of systemctl  Reading the System Log",
            "title": "Unit file"
        },
        {
            "location": "/os/getting-started-with-systemd/#advanced-unit-files",
            "text": "systemd provides a high degree of functionality in your unit files. Here's a curated list of useful features listed in the order they'll occur in the lifecycle of a unit:     Name  Description      ExecStartPre  Commands that will run before  ExecStart .    ExecStart  Main commands to run for this unit.    ExecStartPost  Commands that will run after all  ExecStart  commands have completed.    ExecReload  Commands that will run when this unit is reloaded via  systemctl reload foo.service    ExecStop  Commands that will run when this unit is considered failed or if it is stopped via  systemctl stop foo.service    ExecStopPost  Commands that will run after  ExecStop  has completed.    RestartSec  The amount of time to sleep before restarting a service. Useful to prevent your failed service from attempting to restart itself every 100ms.     The full list is located on the  systemd man page .  Let's put a few of these concepts together to register new units within etcd. Imagine we had another container running that would read these values from etcd and act upon them.  We can use  ExecStartPre  to scrub existing container state. The  docker kill  will force any previous copy of this container to stop, which is useful if we restarted the unit but Docker didn't stop the container for some reason. The  =-  is systemd syntax to ignore errors for this command. We need to do this because Docker will return a non-zero exit code if we try to stop a container that doesn't exist. We don't consider this an error (because we want the container stopped) so we tell systemd to ignore the possible failure.  docker rm  will remove the container and  docker pull  will pull down the latest version. You can optionally pull down a specific version as a Docker tag:  coreos/apache:1.2.3  ExecStart  is where the container is started from the container image that we pulled above.  Since our container will be started in  ExecStart , it makes sense for our etcd command to run as  ExecStartPost  to ensure that our container is started and functioning.  When the service is told to stop, we need to stop the Docker container using its  --name  from the run command. We also need to clean up our etcd key when the container exits or the unit is failed by using  ExecStopPost .  [Unit]\nDescription=My Advanced Service\nAfter=etcd2.service\nAfter=docker.service\n\n[Service]\nTimeoutStartSec=0\nExecStartPre=-/usr/bin/docker kill apache1\nExecStartPre=-/usr/bin/docker rm apache1\nExecStartPre=/usr/bin/docker pull coreos/apache\nExecStart=/usr/bin/docker run --name apache1 -p 8081:80 coreos/apache /usr/sbin/apache2ctl -D FOREGROUND\nExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running\nExecStop=/usr/bin/docker stop apache1\nExecStopPost=/usr/bin/etcdctl rm /domains/example.com/10.10.10.123:8081\n\n[Install]\nWantedBy=multi-user.target  While it's possible to manage the starting, stopping, and removal of the container in a single  ExecStart  command by using  docker run --rm , it's a good idea to separate the container's lifecycle into  ExecStartPre ,  ExecStart , and  ExecStop  options as we've done above. This gives you a chance to inspect the container's state after it stops or fails.",
            "title": "Advanced unit files"
        },
        {
            "location": "/os/getting-started-with-systemd/#unit-specifiers",
            "text": "In our last example we had to hardcode our IP address when we announced our container in etcd. That's not scalable and systemd has a few variables built in to help us out. Here's a few of the most useful:     Variable  Meaning  Description      %n  Full unit name  Useful if the name of your unit is unique enough to be used as an argument on a command.    %m  Machine ID  Useful for namespacing etcd keys by machine. Example:  /machines/%m/units    %b  BootID  Similar to the machine ID, but this value is random and changes on each boot    %H  Hostname  Allows you to run the same unit file across many machines. Useful for service discovery. Example:  /domains/example.com/%H:8081     A full list of specifiers can be found on the  systemd man page .",
            "title": "Unit specifiers"
        },
        {
            "location": "/os/getting-started-with-systemd/#instantiated-units",
            "text": "Since systemd is based on symlinks, there are a few interesting tricks you can leverage that are very powerful when used with containers. If you create multiple symlinks to the same unit file, the following variables become available to you:     Variable  Meaning  Description      %p  Prefix name  Refers to any string before  @  in your unit name.    %i  Instance name  Refers to the string between the  @  and the suffix.     In our earlier example we had to hardcode our IP address when registering within etcd:  ExecStartPost=/usr/bin/etcdctl set /domains/example.com/10.10.10.123:8081 running  We can enhance this by using  %H  and  %i  to dynamically announce the hostname and port. Specify the port after the  @  by using two unit files named  foo@123.service  and  foo@456.service :  ExecStartPost=/usr/bin/etcdctl set /domains/example.com/%H:%i running  This gives us the flexibility to use a single unit file to announce multiple copies of the same container on a single machine (no port overlap) and on multiple machines (no hostname overlap).",
            "title": "Instantiated units"
        },
        {
            "location": "/os/getting-started-with-systemd/#more-information",
            "text": "systemd.service Docs  systemd.unit Docs  systemd.target Docs",
            "title": "More information"
        },
        {
            "location": "/os/hardening-guide/",
            "text": "Flatcar Container Linux hardening guide\n\u00b6\n\n\nThis guide covers the basics of securing a Flatcar Container Linux instance. Flatcar Container Linux has a very slim network profile and the only service that listens by default on Flatcar Container Linux is sshd on port 22 on all interfaces. There are also some defaults for local users and services that should be considered.\n\n\nRemote listening services\n\u00b6\n\n\nDisabling sshd\n\u00b6\n\n\nTo disable sshd from listening you can stop the socket:\n\n\nsystemctl mask sshd.socket --now\n\n\n\nIf you wish to make further customizations see our \ncustomize sshd guide\n.\n\n\nRemote non-listening services\n\u00b6\n\n\netcd and Locksmith\n\u00b6\n\n\netcd and Locksmith should be secured and authenticated using TLS if you are using these services. Please see the relevant guides for details.\n\n\n\n\netcd security guide\n\n\n\n\nLocal services\n\u00b6\n\n\nLocal users\n\u00b6\n\n\nFlatcar Container Linux has a single default user account called \"core\". Generally this user is the one that gets ssh keys added to it via a Container Linux Config for administrators to login. The core user, by default, has access to the wheel group which grants sudo access. You can change this by removing the core user from wheel by running this command: \ngpasswd -d core wheel\n.\n\n\nDocker daemon\n\u00b6\n\n\nThe docker daemon is accessible via a unix domain socket at \n/run/docker.sock\n. Users in the \"docker\" group have access to this service and access to the docker socket grants similar capabilities to sudo. The core user, by default, has access to the docker group. You can change this by removing the core user from docker by running this command: \ngpasswd -d core docker\n.\n\n\nrkt fetch\n\u00b6\n\n\nUsers in the \"rkt\" group have access to the rkt container image store. A user may download new images and place them in the store if they belong to this group. This could be used as an attack vector to insert images that are later executed as root by the rkt container runtime. The core user, by default, has access to the rkt group. You can change this by removing the core user from rkt by running this command: \ngpasswd -d core rkt\n.\n\n\nAdditional hardening\n\u00b6\n\n\nDisabling Simultaneous Multi-Threading\n\u00b6\n\n\nRecent Intel CPU vulnerabilities cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.\n\n\nThe \nSMT on Container Linux guide\n provides guidance and instructions for disabling SMT.\n\n\nSELinux\n\u00b6\n\n\nSELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.\n\n\nFlatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. The \nSELinux on Flatcar Container Linux guide\n covers the process of checking containers for SELinux policy compatibility and switching SELinux into enforcing mode.",
            "title": "Flatcar Container Linux hardening guide"
        },
        {
            "location": "/os/hardening-guide/#flatcar-container-linux-hardening-guide",
            "text": "This guide covers the basics of securing a Flatcar Container Linux instance. Flatcar Container Linux has a very slim network profile and the only service that listens by default on Flatcar Container Linux is sshd on port 22 on all interfaces. There are also some defaults for local users and services that should be considered.",
            "title": "Flatcar Container Linux hardening guide"
        },
        {
            "location": "/os/hardening-guide/#remote-listening-services",
            "text": "",
            "title": "Remote listening services"
        },
        {
            "location": "/os/hardening-guide/#disabling-sshd",
            "text": "To disable sshd from listening you can stop the socket:  systemctl mask sshd.socket --now  If you wish to make further customizations see our  customize sshd guide .",
            "title": "Disabling sshd"
        },
        {
            "location": "/os/hardening-guide/#remote-non-listening-services",
            "text": "",
            "title": "Remote non-listening services"
        },
        {
            "location": "/os/hardening-guide/#etcd-and-locksmith",
            "text": "etcd and Locksmith should be secured and authenticated using TLS if you are using these services. Please see the relevant guides for details.   etcd security guide",
            "title": "etcd and Locksmith"
        },
        {
            "location": "/os/hardening-guide/#local-services",
            "text": "",
            "title": "Local services"
        },
        {
            "location": "/os/hardening-guide/#local-users",
            "text": "Flatcar Container Linux has a single default user account called \"core\". Generally this user is the one that gets ssh keys added to it via a Container Linux Config for administrators to login. The core user, by default, has access to the wheel group which grants sudo access. You can change this by removing the core user from wheel by running this command:  gpasswd -d core wheel .",
            "title": "Local users"
        },
        {
            "location": "/os/hardening-guide/#docker-daemon",
            "text": "The docker daemon is accessible via a unix domain socket at  /run/docker.sock . Users in the \"docker\" group have access to this service and access to the docker socket grants similar capabilities to sudo. The core user, by default, has access to the docker group. You can change this by removing the core user from docker by running this command:  gpasswd -d core docker .",
            "title": "Docker daemon"
        },
        {
            "location": "/os/hardening-guide/#rkt-fetch",
            "text": "Users in the \"rkt\" group have access to the rkt container image store. A user may download new images and place them in the store if they belong to this group. This could be used as an attack vector to insert images that are later executed as root by the rkt container runtime. The core user, by default, has access to the rkt group. You can change this by removing the core user from rkt by running this command:  gpasswd -d core rkt .",
            "title": "rkt fetch"
        },
        {
            "location": "/os/hardening-guide/#additional-hardening",
            "text": "",
            "title": "Additional hardening"
        },
        {
            "location": "/os/hardening-guide/#disabling-simultaneous-multi-threading",
            "text": "Recent Intel CPU vulnerabilities cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.  The  SMT on Container Linux guide  provides guidance and instructions for disabling SMT.",
            "title": "Disabling Simultaneous Multi-Threading"
        },
        {
            "location": "/os/hardening-guide/#selinux",
            "text": "SELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.  Flatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. The  SELinux on Flatcar Container Linux guide  covers the process of checking containers for SELinux policy compatibility and switching SELinux into enforcing mode.",
            "title": "SELinux"
        },
        {
            "location": "/os/install-debugging-tools/",
            "text": "Install debugging tools\n\u00b6\n\n\nYou can use common debugging tools like tcpdump or strace with Toolbox. Using the filesystem of a specified Docker container Toolbox will launch a container with full system privileges including access to system PIDs, network interfaces and other global information. Inside of the toolbox, the machine's filesystem is mounted to \n/media/root\n.\n\n\nQuick debugging\n\u00b6\n\n\nBy default, Toolbox uses the stock Fedora Docker container. To start using it, simply run:\n\n\n/usr/bin/toolbox\n\n\n\nYou're now in the namespace of Fedora and can install any software you'd like via \ndnf\n. For example, if you'd like to use \ntcpdump\n:\n\n\n[root@srv-3qy0p ~]# dnf -y install tcpdump\n[root@srv-3qy0p ~]# tcpdump -i ens3\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on ens3, link-type EN10MB (Ethernet), capture size 65535 bytes\n\n\n\nSpecify a custom Docker image\n\u00b6\n\n\nCreate a \n.toolboxrc\n in the user's home folder to use a specific Docker image:\n\n\n$ cat .toolboxrc\nTOOLBOX_DOCKER_IMAGE=index.example.com/debug\nTOOLBOX_USER=root\n$ /usr/bin/toolbox\nPulling repository index.example.com/debug\n...\n\n\n\nYou can also specify this in a Container Linux Config:\n\n\nstorage:\n  files:\n    - path: /home/core/.toolboxrc\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          TOOLBOX_DOCKER_IMAGE=index.example.com/debug\n          TOOLBOX_DOCKER_TAG=v1\n          TOOLBOX_USER=root\n\n\n\nUnder the hood\n\u00b6\n\n\nBehind the scenes, \ntoolbox\n downloads, prepares and exports the container\nimage you specify (or the default \nfedora\n image), then creates a container\nfrom that extracted image by calling \nsystemd-nspawn\n.  The exported\nimage is retained in\n\n/var/lib/toolbox/[username]-[image name]-[image tag]\n, e.g. the default\nimage run by the \ncore\n user is at \n/var/lib/toolbox/core-fedora-latest\n.  \n\n\nThis means two important things:\n\n\n\n\nChanges made inside the container will persist between sessions\n\n\nThe container filesystem will take up space on disk (a few hundred MiB\nfor the default \nfedora\n container)\n\n\n\n\nSSH directly into a toolbox\n\u00b6\n\n\nAdvanced users can SSH directly into a toolbox by setting up an \n/etc/passwd\n entry:\n\n\nuseradd bob -m -p '*' -s /usr/bin/toolbox -U -G sudo,docker,rkt\n\n\n\nTo test, SSH as bob:\n\n\nssh bob@hostname.example.com\n\n   ______                ____  _____\n  / ____/___  ________  / __ \\/ ___/\n / /   / __ \\/ ___/ _ \\/ / / /\\__ \\\n/ /___/ /_/ / /  /  __/ /_/ /___/ /\n\\____/\\____/_/   \\___/\\____//____/\n[root@srv-3qy0p ~]# dnf -y install emacs-nox\n[root@srv-3qy0p ~]# emacs /media/root/etc/systemd/system/newapp.service",
            "title": "Install debugging tools"
        },
        {
            "location": "/os/install-debugging-tools/#install-debugging-tools",
            "text": "You can use common debugging tools like tcpdump or strace with Toolbox. Using the filesystem of a specified Docker container Toolbox will launch a container with full system privileges including access to system PIDs, network interfaces and other global information. Inside of the toolbox, the machine's filesystem is mounted to  /media/root .",
            "title": "Install debugging tools"
        },
        {
            "location": "/os/install-debugging-tools/#quick-debugging",
            "text": "By default, Toolbox uses the stock Fedora Docker container. To start using it, simply run:  /usr/bin/toolbox  You're now in the namespace of Fedora and can install any software you'd like via  dnf . For example, if you'd like to use  tcpdump :  [root@srv-3qy0p ~]# dnf -y install tcpdump\n[root@srv-3qy0p ~]# tcpdump -i ens3\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on ens3, link-type EN10MB (Ethernet), capture size 65535 bytes",
            "title": "Quick debugging"
        },
        {
            "location": "/os/install-debugging-tools/#specify-a-custom-docker-image",
            "text": "Create a  .toolboxrc  in the user's home folder to use a specific Docker image:  $ cat .toolboxrc\nTOOLBOX_DOCKER_IMAGE=index.example.com/debug\nTOOLBOX_USER=root\n$ /usr/bin/toolbox\nPulling repository index.example.com/debug\n...  You can also specify this in a Container Linux Config:  storage:\n  files:\n    - path: /home/core/.toolboxrc\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          TOOLBOX_DOCKER_IMAGE=index.example.com/debug\n          TOOLBOX_DOCKER_TAG=v1\n          TOOLBOX_USER=root",
            "title": "Specify a custom Docker image"
        },
        {
            "location": "/os/install-debugging-tools/#under-the-hood",
            "text": "Behind the scenes,  toolbox  downloads, prepares and exports the container\nimage you specify (or the default  fedora  image), then creates a container\nfrom that extracted image by calling  systemd-nspawn .  The exported\nimage is retained in /var/lib/toolbox/[username]-[image name]-[image tag] , e.g. the default\nimage run by the  core  user is at  /var/lib/toolbox/core-fedora-latest .    This means two important things:   Changes made inside the container will persist between sessions  The container filesystem will take up space on disk (a few hundred MiB\nfor the default  fedora  container)",
            "title": "Under the hood"
        },
        {
            "location": "/os/install-debugging-tools/#ssh-directly-into-a-toolbox",
            "text": "Advanced users can SSH directly into a toolbox by setting up an  /etc/passwd  entry:  useradd bob -m -p '*' -s /usr/bin/toolbox -U -G sudo,docker,rkt  To test, SSH as bob:  ssh bob@hostname.example.com\n\n   ______                ____  _____\n  / ____/___  ________  / __ \\/ ___/\n / /   / __ \\/ ___/ _ \\/ / / /\\__ \\\n/ /___/ /_/ / /  /  __/ /_/ /___/ /\n\\____/\\____/_/   \\___/\\____//____/\n[root@srv-3qy0p ~]# dnf -y install emacs-nox\n[root@srv-3qy0p ~]# emacs /media/root/etc/systemd/system/newapp.service",
            "title": "SSH directly into a toolbox"
        },
        {
            "location": "/os/installing-to-disk/",
            "text": "Installing Flatcar Container Linux to disk\n\u00b6\n\n\nInstall script\n\u00b6\n\n\nThere is a simple installer that will destroy everything on the given target disk and install Flatcar Container Linux. Essentially it downloads an image, verifies it with gpg, and then copies it bit for bit to disk. An installation requires at least 8 GB of usable space on the device.\n\n\nThe script is self-contained and located \non GitHub here\n and can be run from any Linux distribution. You cannot normally install Flatcar Container Linux to the same device that is currently booted. However, the \nFlatcar Container Linux ISO\n or any Linux liveCD will allow Flatcar Container Linux to install to a non-active device.\n\n\nIf you boot Flatcar Container Linux via PXE, the install script is already installed. By default the install script will attempt to install the same version and channel that was PXE-booted:\n\n\nflatcar-install -d /dev/sda -i ignition.json\n\n\n\nignition.json\n should include user information (especially an SSH key) generated from a \nContainer Linux Config\n, or you will not be able to log into your Flatcar Container Linux instance.\n\n\nIf you are installing on VMware, pass \n-o vmware_raw\n to install the VMware-specific image:\n\n\nflatcar-install -d /dev/sda -i ignition.json -o vmware_raw\n\n\n\nChoose a channel\n\u00b6\n\n\nFlatcar Container Linux is designed to be \nupdated automatically\n with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\n\n  \n\n    \nStable Channel\n\n    \nBeta Channel\n\n    \nAlpha Channel\n\n    \nEdge Channel\n\n  \n\n  \n\n    \n\n      \nThe Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}.\n\n      \nIf you want to ensure you are installing the latest alpha version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C alpha\n\n    \n\n    \n\n      \nThe Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}.\n\n      \nIf you want to ensure you are installing the latest beta version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C beta\n\n    \n\n    \n\n      \nThe Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}.\n\n      \nIf you want to ensure you are installing the latest edge version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C edge\n\n    \n\n    \n\n      \nThe Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}.\n\n      \nIf you want to ensure you are installing the latest stable version, use the \n-C\n option:\n\n      \nflatcar-install -d /dev/sda -C stable\n\n    \n\n  \n\n\n\n\n\nFor reference here are the rest of the \nflatcar-install\n options:\n\n\n-d DEVICE   Install Flatcar Container Linux to the given device.\n-V VERSION  Version to install (e.g. current)\n-B BOARD    Flatcar Container Linux board to use\n-C CHANNEL  Release channel to use (e.g. beta)\n-o OEM      OEM type to install (e.g. ami)\n-c CLOUD    Insert a cloud-init config to be executed on boot.\n-i IGNITION Insert an Ignition config to be executed on boot.\n-b BASEURL  URL to the image mirror (overrides BOARD)\n-k KEYFILE  Override default GPG key for verifying image signature\n-f IMAGE    Install unverified local image file to disk instead of fetching\n-n          Copy generated network units to the root partition.\n-v          Super verbose, for debugging.\n\n\n\nContainer Linux Configs\n\u00b6\n\n\nBy default there isn't a password or any other way to log into a fresh Flatcar Container Linux system. The easiest way to configure accounts, add systemd units, and more is via Container Linux Configs. Jump over to the \ndocs to learn about the supported features\n.\n\n\nAfter using the \nContainer Linux Config Transpiler\n to produce an Ignition config, the installation script will process your \nignition.json\n file specified with the \n-i\n flag and use it when the installation is booted.\n\n\nA Container Linux Config that specifies an SSH key for the \ncore\n user but doesn't use any other parameters looks like:\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\n\n\n\nNote: The \n{PRIVATE_IPV4}\n and \n{PUBLIC_IPV4}\n substitution variables referenced in other documents are not supported on libvirt.\n\n\nTo start the installation script with a reference to our Ignition config, run:\n\n\nflatcar-install -d /dev/sda -C stable -i ~/ignition.json\n\n\n\nAdvanced Container Linux Config example\n\u00b6\n\n\nThis example will configure Flatcar Container Linux components: etcd and flannel. You have to substitute \n<PEER_ADDRESS>\n to your host's IP or DNS address.\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\netcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  advertise_client_urls: http://<PEER_ADDRESS>:2379,http://<PEER_ADDRESS>:4001\n  initial_advertise_peer_urls: http://<PEER_ADDRESS>:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://<PEER_ADDRESS>:2380\nsystemd:\n  units:\n    - name: flanneld.service\n      enable: true\n      dropins:\n      - name: 50-network-config.conf\n        contents: |\n          [Service]\n          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{\"Network\":\"10.1.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}'\n\n\n\nUsing Flatcar Container Linux\n\u00b6\n\n\nNow that you have a machine booted it is time to play around. Check out the \nFlatcar Container Linux Quickstart\n guide or dig into \nmore specific topics\n.",
            "title": "Installing Flatcar Container Linux to disk"
        },
        {
            "location": "/os/installing-to-disk/#installing-flatcar-container-linux-to-disk",
            "text": "",
            "title": "Installing Flatcar Container Linux to disk"
        },
        {
            "location": "/os/installing-to-disk/#install-script",
            "text": "There is a simple installer that will destroy everything on the given target disk and install Flatcar Container Linux. Essentially it downloads an image, verifies it with gpg, and then copies it bit for bit to disk. An installation requires at least 8 GB of usable space on the device.  The script is self-contained and located  on GitHub here  and can be run from any Linux distribution. You cannot normally install Flatcar Container Linux to the same device that is currently booted. However, the  Flatcar Container Linux ISO  or any Linux liveCD will allow Flatcar Container Linux to install to a non-active device.  If you boot Flatcar Container Linux via PXE, the install script is already installed. By default the install script will attempt to install the same version and channel that was PXE-booted:  flatcar-install -d /dev/sda -i ignition.json  ignition.json  should include user information (especially an SSH key) generated from a  Container Linux Config , or you will not be able to log into your Flatcar Container Linux instance.  If you are installing on VMware, pass  -o vmware_raw  to install the VMware-specific image:  flatcar-install -d /dev/sda -i ignition.json -o vmware_raw",
            "title": "Install script"
        },
        {
            "location": "/os/installing-to-disk/#choose-a-channel",
            "text": "Flatcar Container Linux is designed to be  updated automatically  with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  \n   \n     Stable Channel \n     Beta Channel \n     Alpha Channel \n     Edge Channel \n   \n   \n     \n       The Alpha channel closely tracks master and is released frequently. The newest versions of system libraries and utilities will be available for testing. The current version is Flatcar Container Linux {{site.alpha-channel}}. \n       If you want to ensure you are installing the latest alpha version, use the  -C  option: \n       flatcar-install -d /dev/sda -C alpha \n     \n     \n       The Beta channel consists of promoted Alpha releases. The current version is Flatcar Container Linux {{site.beta-channel}}. \n       If you want to ensure you are installing the latest beta version, use the  -C  option: \n       flatcar-install -d /dev/sda -C beta \n     \n     \n       The Edge channel includes bleeding-edge features with the newest versions of the Linux kernel, systemd and other core packages. Can be highly unstable. The current version is Flatcar Container Linux {{site.edge-channel}}. \n       If you want to ensure you are installing the latest edge version, use the  -C  option: \n       flatcar-install -d /dev/sda -C edge \n     \n     \n       The Stable channel should be used by production clusters. Versions of Flatcar Container Linux are battle-tested within the Beta and Alpha channels before being promoted. The current version is Flatcar Container Linux {{site.stable-channel}}. \n       If you want to ensure you are installing the latest stable version, use the  -C  option: \n       flatcar-install -d /dev/sda -C stable \n     \n     For reference here are the rest of the  flatcar-install  options:  -d DEVICE   Install Flatcar Container Linux to the given device.\n-V VERSION  Version to install (e.g. current)\n-B BOARD    Flatcar Container Linux board to use\n-C CHANNEL  Release channel to use (e.g. beta)\n-o OEM      OEM type to install (e.g. ami)\n-c CLOUD    Insert a cloud-init config to be executed on boot.\n-i IGNITION Insert an Ignition config to be executed on boot.\n-b BASEURL  URL to the image mirror (overrides BOARD)\n-k KEYFILE  Override default GPG key for verifying image signature\n-f IMAGE    Install unverified local image file to disk instead of fetching\n-n          Copy generated network units to the root partition.\n-v          Super verbose, for debugging.",
            "title": "Choose a channel"
        },
        {
            "location": "/os/installing-to-disk/#container-linux-configs",
            "text": "By default there isn't a password or any other way to log into a fresh Flatcar Container Linux system. The easiest way to configure accounts, add systemd units, and more is via Container Linux Configs. Jump over to the  docs to learn about the supported features .  After using the  Container Linux Config Transpiler  to produce an Ignition config, the installation script will process your  ignition.json  file specified with the  -i  flag and use it when the installation is booted.  A Container Linux Config that specifies an SSH key for the  core  user but doesn't use any other parameters looks like:  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......  Note: The  {PRIVATE_IPV4}  and  {PUBLIC_IPV4}  substitution variables referenced in other documents are not supported on libvirt.  To start the installation script with a reference to our Ignition config, run:  flatcar-install -d /dev/sda -C stable -i ~/ignition.json",
            "title": "Container Linux Configs"
        },
        {
            "location": "/os/installing-to-disk/#advanced-container-linux-config-example",
            "text": "This example will configure Flatcar Container Linux components: etcd and flannel. You have to substitute  <PEER_ADDRESS>  to your host's IP or DNS address.  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDGdByTgSVHq.......\netcd:\n  # generate a new token for each unique cluster from https://discovery.etcd.io/new?size=3\n  # specify the initial size of your cluster with ?size=X\n  discovery: https://discovery.etcd.io/<token>\n  advertise_client_urls: http://<PEER_ADDRESS>:2379,http://<PEER_ADDRESS>:4001\n  initial_advertise_peer_urls: http://<PEER_ADDRESS>:2380\n  # listen on both the official ports and the legacy ports\n  # legacy ports can be omitted if your application doesn't depend on them\n  listen_client_urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n  listen_peer_urls: http://<PEER_ADDRESS>:2380\nsystemd:\n  units:\n    - name: flanneld.service\n      enable: true\n      dropins:\n      - name: 50-network-config.conf\n        contents: |\n          [Service]\n          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{\"Network\":\"10.1.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}'",
            "title": "Advanced Container Linux Config example"
        },
        {
            "location": "/os/installing-to-disk/#using-flatcar-container-linux",
            "text": "Now that you have a machine booted it is time to play around. Check out the  Flatcar Container Linux Quickstart  guide or dig into  more specific topics .",
            "title": "Using Flatcar Container Linux"
        },
        {
            "location": "/os/integrations/",
            "text": "Integrations\n\u00b6\n\n\nThis document tracks projects that integrate with Flatcar Container Linux. \nJoin the community\n, and help us keep the list current.\n\n\nProjects\n\u00b6\n\n\nDeis Workflow\n: an open source PaaS for Kubernetes that runs on Flatcar Container Linux.\n\n\nAmazon Web Services\n: Amazon's cloud computing solution. Offers Flatcar Container Linux.\n\n\nGoogle Cloud Platform\n: Google's cloud computing solution. Offers Flatcar Container Linux.\n\n\nMicrosoft Azure\n: Microsoft's cloud computing solution. Offers Flatcar Container Linux.\n\n\nDigitalOcean\n: An independent cloud computing solution. Offers Flatcar Container Linux.\n\n\nPacket\n: A hosted bare metal solution. Offers Flatcar Container Linux.",
            "title": "Integrations"
        },
        {
            "location": "/os/integrations/#integrations",
            "text": "This document tracks projects that integrate with Flatcar Container Linux.  Join the community , and help us keep the list current.",
            "title": "Integrations"
        },
        {
            "location": "/os/integrations/#projects",
            "text": "Deis Workflow : an open source PaaS for Kubernetes that runs on Flatcar Container Linux.  Amazon Web Services : Amazon's cloud computing solution. Offers Flatcar Container Linux.  Google Cloud Platform : Google's cloud computing solution. Offers Flatcar Container Linux.  Microsoft Azure : Microsoft's cloud computing solution. Offers Flatcar Container Linux.  DigitalOcean : An independent cloud computing solution. Offers Flatcar Container Linux.  Packet : A hosted bare metal solution. Offers Flatcar Container Linux.",
            "title": "Projects"
        },
        {
            "location": "/os/iscsi/",
            "text": "iSCSI on Flatcar Container Linux\n\u00b6\n\n\niSCSI\n is a protocol which provides block-level access to storage devices over IP.\nThis allows applications to treat remote storage devices as if they were local disks.\niSCSI handles taking requests from clients and carrying them out on the remote SCSI devices.\n\n\nFlatcar Container Linux has integrated support for mounting devices.\nThis guide covers iSCSI configuration manually or automatically with \nContainer Linux Configs\n.\n\n\nManual iSCSI configuration\n\u00b6\n\n\nSet the Flatcar Container Linux iSCSI initiator name\n\u00b6\n\n\niSCSI clients each have a unique initiator name.\nFlatcar Container Linux generates a unique initiator name on each install and stores it in \n/etc/iscsi/initiatorname.iscsi\n.\nThis may be replaced if necessary.\n\n\nConfigure the global iSCSI credentials\n\u00b6\n\n\nIf all iSCSI mounts on a Flatcar Container Linux system use the same credentials, these may be configured locally by editing \n/etc/iscsi/iscsid.conf\n and setting the \nnode.session.auth.username\n and \nnode.session.auth.password\n fields.\nIf the iSCSI target is configured to support mutual authentication (allowing the initiator to verify that it is speaking to the correct client), these should be set in \nnode.session.auth.username_in\n and \nnode.session.auth.password_in\n.\n\n\nStart the iSCSI daemon\n\u00b6\n\n\nsystemctl start iscsid\n\n\n\nDiscover available iSCSI targets\n\u00b6\n\n\nTo discover targets, run:\n\n\n$ iscsiadm -m discovery -t sendtargets -p target_ip:target_port\n\n\n\nProvide target-specific credentials\n\u00b6\n\n\nFor each unique \n--targetname\n, first enter the username:\n\n\n$ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.username \\\n  --value=my_username\n\n\n\nAnd then the password:\n\n\n$ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.password \\\n  --value=my_secret_passphrase\n\n\n\nLog into an iSCSI target\n\u00b6\n\n\nThe following command will log into all discovered targets.\n\n\n$ iscsiadm -m node --login\n\n\n\nThen, to log into a specific target use:\n\n\n$ iscsiadm -m node --targetname=custom_target --login\n\n\n\nEnable automatic iSCSI login at boot\n\u00b6\n\n\nIf you want to connect to iSCSI targets automatically at boot you first need to enable the systemd service:\n\n\n$ systemctl enable iscsid\n\n\n\nAutomatic iSCSI configuration\n\u00b6\n\n\nTo configure and start iSCSI automatically after a machine is provisioned, credentials need to be written to disk and the iSCSI service started.\n\n\nA Container Linux Config will be used to write the file \n/etc/iscsi/iscsid.conf\n to disk:\n\n\n/etc/iscsi/iscsid.conf\n\u00b6\n\n\n\n\n\nisns.address = host_ip\nisns.port = host_port\nnode.session.auth.username = my_username\nnode.session.auth.password = my_secret_password\ndiscovery.sendtargets.auth.username = my_username\ndiscovery.sendtargets.auth.password = my_secret_password\n\n\n\nThe Container Linux Config\n\u00b6\n\n\nsystemd:\n  units:\n    - name: iscsid.service\n      enable: true\nstorage:\n  files:\n    - path: /etc/iscsi/iscsid.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          isns.address = host_ip\n          isns.port = host_port\n          node.session.auth.username = my_username\n          node.session.auth.password = my_secret_password\n          discovery.sendtargets.auth.username = my_username\n          discovery.sendtargets.auth.password = my_secret_password\n\n\n\nMounting iSCSI targets\n\u00b6\n\n\nSee the \nmounting storage docs\n for an example.",
            "title": "iSCSI on Flatcar Container Linux"
        },
        {
            "location": "/os/iscsi/#iscsi-on-flatcar-container-linux",
            "text": "iSCSI  is a protocol which provides block-level access to storage devices over IP.\nThis allows applications to treat remote storage devices as if they were local disks.\niSCSI handles taking requests from clients and carrying them out on the remote SCSI devices.  Flatcar Container Linux has integrated support for mounting devices.\nThis guide covers iSCSI configuration manually or automatically with  Container Linux Configs .",
            "title": "iSCSI on Flatcar Container Linux"
        },
        {
            "location": "/os/iscsi/#manual-iscsi-configuration",
            "text": "",
            "title": "Manual iSCSI configuration"
        },
        {
            "location": "/os/iscsi/#set-the-flatcar-container-linux-iscsi-initiator-name",
            "text": "iSCSI clients each have a unique initiator name.\nFlatcar Container Linux generates a unique initiator name on each install and stores it in  /etc/iscsi/initiatorname.iscsi .\nThis may be replaced if necessary.",
            "title": "Set the Flatcar Container Linux iSCSI initiator name"
        },
        {
            "location": "/os/iscsi/#configure-the-global-iscsi-credentials",
            "text": "If all iSCSI mounts on a Flatcar Container Linux system use the same credentials, these may be configured locally by editing  /etc/iscsi/iscsid.conf  and setting the  node.session.auth.username  and  node.session.auth.password  fields.\nIf the iSCSI target is configured to support mutual authentication (allowing the initiator to verify that it is speaking to the correct client), these should be set in  node.session.auth.username_in  and  node.session.auth.password_in .",
            "title": "Configure the global iSCSI credentials"
        },
        {
            "location": "/os/iscsi/#start-the-iscsi-daemon",
            "text": "systemctl start iscsid",
            "title": "Start the iSCSI daemon"
        },
        {
            "location": "/os/iscsi/#discover-available-iscsi-targets",
            "text": "To discover targets, run:  $ iscsiadm -m discovery -t sendtargets -p target_ip:target_port",
            "title": "Discover available iSCSI targets"
        },
        {
            "location": "/os/iscsi/#provide-target-specific-credentials",
            "text": "For each unique  --targetname , first enter the username:  $ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.username \\\n  --value=my_username  And then the password:  $ iscsiadm -m node \\\n  --targetname=custom_target \\\n  --op update \\\n  --name=node.session.auth.password \\\n  --value=my_secret_passphrase",
            "title": "Provide target-specific credentials"
        },
        {
            "location": "/os/iscsi/#log-into-an-iscsi-target",
            "text": "The following command will log into all discovered targets.  $ iscsiadm -m node --login  Then, to log into a specific target use:  $ iscsiadm -m node --targetname=custom_target --login",
            "title": "Log into an iSCSI target"
        },
        {
            "location": "/os/iscsi/#enable-automatic-iscsi-login-at-boot",
            "text": "If you want to connect to iSCSI targets automatically at boot you first need to enable the systemd service:  $ systemctl enable iscsid",
            "title": "Enable automatic iSCSI login at boot"
        },
        {
            "location": "/os/iscsi/#automatic-iscsi-configuration",
            "text": "To configure and start iSCSI automatically after a machine is provisioned, credentials need to be written to disk and the iSCSI service started.  A Container Linux Config will be used to write the file  /etc/iscsi/iscsid.conf  to disk:",
            "title": "Automatic iSCSI configuration"
        },
        {
            "location": "/os/iscsi/#etciscsiiscsidconf",
            "text": "isns.address = host_ip\nisns.port = host_port\nnode.session.auth.username = my_username\nnode.session.auth.password = my_secret_password\ndiscovery.sendtargets.auth.username = my_username\ndiscovery.sendtargets.auth.password = my_secret_password",
            "title": "/etc/iscsi/iscsid.conf"
        },
        {
            "location": "/os/iscsi/#the-container-linux-config",
            "text": "systemd:\n  units:\n    - name: iscsid.service\n      enable: true\nstorage:\n  files:\n    - path: /etc/iscsi/iscsid.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          isns.address = host_ip\n          isns.port = host_port\n          node.session.auth.username = my_username\n          node.session.auth.password = my_secret_password\n          discovery.sendtargets.auth.username = my_username\n          discovery.sendtargets.auth.password = my_secret_password",
            "title": "The Container Linux Config"
        },
        {
            "location": "/os/iscsi/#mounting-iscsi-targets",
            "text": "See the  mounting storage docs  for an example.",
            "title": "Mounting iSCSI targets"
        },
        {
            "location": "/os/kernel-modules/",
            "text": "Building custom kernel modules\n\u00b6\n\n\nCreate a writable overlay\n\u00b6\n\n\nThe kernel modules directory \n/usr/lib64/modules\n is read-only on Flatcar Container Linux. A writable overlay can be mounted over it to allow installing new modules.\n\n\nmodules=/opt/modules  # Adjust this writable storage location as needed.\nsudo mkdir -p \"$modules\" \"$modules.wd\"\nsudo mount \\\n    -o \"lowerdir=/usr/lib64/modules,upperdir=$modules,workdir=$modules.wd\" \\\n    -t overlay overlay /usr/lib64/modules\n\n\n\nThe following systemd unit can be written to \n/etc/systemd/system/usr-lib64-modules.mount\n.\n\n\n[Unit]\nDescription=Custom Kernel Modules\nBefore=local-fs.target\nConditionPathExists=/opt/modules\n\n[Mount]\nType=overlay\nWhat=overlay\nWhere=/usr/lib64/modules\nOptions=lowerdir=/usr/lib64/modules,upperdir=/opt/modules,workdir=/opt/modules.wd\n\n[Install]\nWantedBy=local-fs.target\n\n\n\nEnable the unit so this overlay is mounted automatically on boot.\n\n\nsudo systemctl enable usr-lib64-modules.mount\n\n\n\nAn alternative is to mount the overlay automatically when the system boots by adding the following line to \n/etc/fstab\n (creating it if necessary).\n\n\noverlay /lib/modules overlay lowerdir=/lib/modules,upperdir=/opt/modules,workdir=/opt/modules.wd,nofail 0 0\n\n\n\nPrepare a Flatcar Container Linux development container\n\u00b6\n\n\nRead system configuration files to determine the URL of the development container that corresponds to the current Flatcar Container Linux version.\n\n\n. /usr/share/coreos/release\n. /usr/share/coreos/update.conf\nurl=\"https://${GROUP:-stable}.release.flatcar-linux.net/$FLATCAR_RELEASE_BOARD/$FLATCAR_RELEASE_VERSION/flatcar_developer_container.bin.bz2\"\n\n\n\nDownload, decompress, and verify the development container image.\n\n\ngpg2 --keyserver pool.sks-keyservers.net --recv-keys F88CFEDEFF29A5B4D9523864E25D9AED0593B34A  # Fetch the buildbot key if necessary.\ncurl -L \"$url\" |\n    tee >(bzip2 -d > flatcar_developer_container.bin) |\n    gpg2 --verify <(curl -Ls \"$url.sig\") -\n\n\n\nStart the development container with the host's writable modules directory mounted into place.\n\n\nsudo systemd-nspawn \\\n    --bind=/usr/lib64/modules \\\n    --image=flatcar_developer_container.bin\n\n\n\nNow, inside the container, fetch the Flatcar Container Linux package definitions, then download and prepare the Linux kernel source for building external modules.\n\n\nemerge-gitclone\nemerge -gKv coreos-sources\ngzip -cd /proc/config.gz > /usr/src/linux/.config\nmake -C /usr/src/linux modules_prepare\n\n\n\nBuild and install kernel modules\n\u00b6\n\n\nAt this point, upstream projects' instructions for building their out-of-tree modules should work in the Flatcar Container Linux development container. New kernel modules should be installed into \n/usr/lib64/modules\n, which is bind-mounted from the host, so they will be available on future boots without using the container again.\n\n\nIn case the installation step didn't update the module dependency files automatically, running the following command will ensure commands like \nmodprobe\n function correctly with the new modules.\n\n\nsudo depmod",
            "title": "Building custom kernel modules"
        },
        {
            "location": "/os/kernel-modules/#building-custom-kernel-modules",
            "text": "",
            "title": "Building custom kernel modules"
        },
        {
            "location": "/os/kernel-modules/#create-a-writable-overlay",
            "text": "The kernel modules directory  /usr/lib64/modules  is read-only on Flatcar Container Linux. A writable overlay can be mounted over it to allow installing new modules.  modules=/opt/modules  # Adjust this writable storage location as needed.\nsudo mkdir -p \"$modules\" \"$modules.wd\"\nsudo mount \\\n    -o \"lowerdir=/usr/lib64/modules,upperdir=$modules,workdir=$modules.wd\" \\\n    -t overlay overlay /usr/lib64/modules  The following systemd unit can be written to  /etc/systemd/system/usr-lib64-modules.mount .  [Unit]\nDescription=Custom Kernel Modules\nBefore=local-fs.target\nConditionPathExists=/opt/modules\n\n[Mount]\nType=overlay\nWhat=overlay\nWhere=/usr/lib64/modules\nOptions=lowerdir=/usr/lib64/modules,upperdir=/opt/modules,workdir=/opt/modules.wd\n\n[Install]\nWantedBy=local-fs.target  Enable the unit so this overlay is mounted automatically on boot.  sudo systemctl enable usr-lib64-modules.mount  An alternative is to mount the overlay automatically when the system boots by adding the following line to  /etc/fstab  (creating it if necessary).  overlay /lib/modules overlay lowerdir=/lib/modules,upperdir=/opt/modules,workdir=/opt/modules.wd,nofail 0 0",
            "title": "Create a writable overlay"
        },
        {
            "location": "/os/kernel-modules/#prepare-a-flatcar-container-linux-development-container",
            "text": "Read system configuration files to determine the URL of the development container that corresponds to the current Flatcar Container Linux version.  . /usr/share/coreos/release\n. /usr/share/coreos/update.conf\nurl=\"https://${GROUP:-stable}.release.flatcar-linux.net/$FLATCAR_RELEASE_BOARD/$FLATCAR_RELEASE_VERSION/flatcar_developer_container.bin.bz2\"  Download, decompress, and verify the development container image.  gpg2 --keyserver pool.sks-keyservers.net --recv-keys F88CFEDEFF29A5B4D9523864E25D9AED0593B34A  # Fetch the buildbot key if necessary.\ncurl -L \"$url\" |\n    tee >(bzip2 -d > flatcar_developer_container.bin) |\n    gpg2 --verify <(curl -Ls \"$url.sig\") -  Start the development container with the host's writable modules directory mounted into place.  sudo systemd-nspawn \\\n    --bind=/usr/lib64/modules \\\n    --image=flatcar_developer_container.bin  Now, inside the container, fetch the Flatcar Container Linux package definitions, then download and prepare the Linux kernel source for building external modules.  emerge-gitclone\nemerge -gKv coreos-sources\ngzip -cd /proc/config.gz > /usr/src/linux/.config\nmake -C /usr/src/linux modules_prepare",
            "title": "Prepare a Flatcar Container Linux development container"
        },
        {
            "location": "/os/kernel-modules/#build-and-install-kernel-modules",
            "text": "At this point, upstream projects' instructions for building their out-of-tree modules should work in the Flatcar Container Linux development container. New kernel modules should be installed into  /usr/lib64/modules , which is bind-mounted from the host, so they will be available on future boots without using the container again.  In case the installation step didn't update the module dependency files automatically, running the following command will ensure commands like  modprobe  function correctly with the new modules.  sudo depmod",
            "title": "Build and install kernel modules"
        },
        {
            "location": "/os/manual-rollbacks/",
            "text": "Performing manual Flatcar Container Linux rollbacks\n\u00b6\n\n\nIn the event of an upgrade failure, Flatcar Container Linux will automatically boot with the version on the rollback partition. Immediately after an upgrade reboot, the active version of Flatcar Container Linux can be rolled back to the version installed on the rollback partition, or downgraded to the version current on any lower release channel. There is no method to downgrade to an arbitrary version number.\n\n\nThis section describes the automated upgrade process, performing a manual rollback, and forcing a channel downgrade.\n\n\nNote:\n Neither performing a manual rollback nor forcing a channel downgrade are recommended.\n\n\nHow do updates work?\n\u00b6\n\n\nThe system's GPT tables are used to encode which partition is currently active and which is passive. This can be seen using the \ncgpt\n command.\n\n\n$ cgpt show /dev/sda\n       start        size    part  contents\n           0           1          Hybrid MBR\n           1           1          Pri GPT header\n           2          32          Pri GPT table\n        4096      262144       1  Label: \"EFI-SYSTEM\"\n                                  Type: EFI System Partition\n                                  UUID: 596FF08E-5617-4497-B10B-27A23F658B73\n                                  Attr: Legacy BIOS Bootable\n      266240        4096       2  Label: \"BIOS-BOOT\"\n                                  Type: BIOS Boot Partition\n                                  UUID: EACCC3D5-E7E9-461D-A6E2-1DCDAE4671EC\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0\n     4464640      262144       6  Label: \"OEM\"\n                                  Type: Alias for linux-data\n                                  UUID: 726E33FA-DFE9-45B2-B215-FB35CD9C2388\n     4726784      131072       7  Label: \"OEM-CONFIG\"\n                                  Type: Flatcar Container Linux reserved\n                                  UUID: 8F39CE8B-1FB3-4E7E-A784-0C53C8F40442\n     4857856    37085151       9  Label: \"ROOT\"\n                                  Type: Flatcar Container Linux auto-resize\n                                  UUID: D9A972BB-8084-4AB5-BA55-F8A3AFFAD70D\n    41943007          32          Sec GPT table\n    41943039           1          Sec GPT header\n\n\n\nLooking specifically at \"USR-A\" and \"USR-B\", we see that \"USR-A\" is the active USR partition (this is what's actually mounted at /usr). Its priority is higher than that of \"USR-B\". When the system boots, GRUB (the bootloader) looks at the priorities, tries, and successful flags to determine which partition to use.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0\n\n\n\nYou'll notice that on this machine, \"USR-B\" hasn't actually successfully booted. Not to worry! This is a fresh machine that hasn't been through an update cycle yet. When the machine downloads an update, the partition table is updated to allow the newer image to boot.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=1 successful=0\n\n\n\nIn this case, we see that \"USR-B\" now has a higher priority and it has one try to successfully boot. Once the machine reboots, the partition table will again be updated.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=0\n\n\n\nNow we see that the number of tries for \"USR-B\" has been decremented to zero. The successful flag still hasn't been updated though. Once update-engine has had a chance to run, it marks the boot as being successful.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=1\n\n\n\nPerforming a manual rollback\n\u00b6\n\n\nSo, now that we understand what happens when the machine updates, we can tweak the process so that it boots an older image (assuming it's still intact on the passive partition). The first command we'll use is \ncgpt find -t flatcar-usr\n. This will give us a list of all of the USR partitions available on the disk.\n\n\n$ cgpt find -t flatcar-usr\n/dev/sda3\n/dev/sda4\n\n\n\nTo figure out which partition is currently active, we can use \nrootdev\n.\n\n\n$ rootdev -s /usr\n/dev/sda4\n\n\n\nSo now we know that \n/dev/sda3\n is the passive partition on our system. We can compose the previous two commands to dynamically figure out the passive partition.\n\n\n$ cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\"\n/dev/sda3\n\n\n\nIn order to rollback, we need to mark that partition as active using \ncgpt prioritize\n.\n\n\n$ cgpt prioritize /dev/sda3\n\n\n\nIf we take another look at the GPT tables, we'll see that the priorities have been updated.\n\n\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=1\n\n\n\n\nComposing the previous two commands produces the following command to set the currently passive partition to be active on the next boot:\n\n\n$ cgpt prioritize \"$(cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\")\"\n\n\n\nForcing a Channel Downgrade\n\u00b6\n\n\nThe procedure above restores the last known good Flatcar Container Linux version from immediately before an upgrade reboot. The system remains on the same \nFlatcar Container Linux channel\n after rebooting with the previous USR partition. It is also possible, though not recommended, to switch a Flatcar Container Linux installation to an older release channel, for example to make a system running an Alpha release downgrade to the Stable channel. Root privileges are required for this procedure, noted by \nsudo\n in the commands below.\n\n\nFirst, edit \n/etc/coreos/update.conf\n to set \nGROUP\n to the name of the target channel, one of \nstable\n or \nbeta\n:\n\n\nGROUP=stable\n\n\n\nNext, clear the current version number from the \nrelease\n file so that the target channel will be certain to have a higher version number, triggering the \"upgrade,\" in this case a downgrade to the lower channel. Since \nrelease\n is on a read-only file system, it is convenient to temporarily override it with a bind mount. To do this, copy the original to a writable location, then bind the copy over the system \nrelease\n file:\n\n\n$ cp /usr/share/coreos/release /tmp\n$ sudo mount -o bind /tmp/release /usr/share/coreos/release\n\n\n\nThe file is now writable, but the bind mount will not survive the reboot, so that the default read-only system \nrelease\n file will be restored after this procedure is complete. Edit \n/usr/share/coreos/release\n to replace the value of \nCOREOS_RELEASE_VERSION\n with \n0.0.0\n:\n\n\nCOREOS_RELEASE_VERSION=0.0.0\n\n\n\nRestart the update service so that it rescans the edited configuration, then initiate an update. The system will reboot into the selected lower channel after downloading the release:\n\n\n$ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Performing manual Flatcar Container Linux rollbacks"
        },
        {
            "location": "/os/manual-rollbacks/#performing-manual-flatcar-container-linux-rollbacks",
            "text": "In the event of an upgrade failure, Flatcar Container Linux will automatically boot with the version on the rollback partition. Immediately after an upgrade reboot, the active version of Flatcar Container Linux can be rolled back to the version installed on the rollback partition, or downgraded to the version current on any lower release channel. There is no method to downgrade to an arbitrary version number.  This section describes the automated upgrade process, performing a manual rollback, and forcing a channel downgrade.  Note:  Neither performing a manual rollback nor forcing a channel downgrade are recommended.",
            "title": "Performing manual Flatcar Container Linux rollbacks"
        },
        {
            "location": "/os/manual-rollbacks/#how-do-updates-work",
            "text": "The system's GPT tables are used to encode which partition is currently active and which is passive. This can be seen using the  cgpt  command.  $ cgpt show /dev/sda\n       start        size    part  contents\n           0           1          Hybrid MBR\n           1           1          Pri GPT header\n           2          32          Pri GPT table\n        4096      262144       1  Label: \"EFI-SYSTEM\"\n                                  Type: EFI System Partition\n                                  UUID: 596FF08E-5617-4497-B10B-27A23F658B73\n                                  Attr: Legacy BIOS Bootable\n      266240        4096       2  Label: \"BIOS-BOOT\"\n                                  Type: BIOS Boot Partition\n                                  UUID: EACCC3D5-E7E9-461D-A6E2-1DCDAE4671EC\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0\n     4464640      262144       6  Label: \"OEM\"\n                                  Type: Alias for linux-data\n                                  UUID: 726E33FA-DFE9-45B2-B215-FB35CD9C2388\n     4726784      131072       7  Label: \"OEM-CONFIG\"\n                                  Type: Flatcar Container Linux reserved\n                                  UUID: 8F39CE8B-1FB3-4E7E-A784-0C53C8F40442\n     4857856    37085151       9  Label: \"ROOT\"\n                                  Type: Flatcar Container Linux auto-resize\n                                  UUID: D9A972BB-8084-4AB5-BA55-F8A3AFFAD70D\n    41943007          32          Sec GPT table\n    41943039           1          Sec GPT header  Looking specifically at \"USR-A\" and \"USR-B\", we see that \"USR-A\" is the active USR partition (this is what's actually mounted at /usr). Its priority is higher than that of \"USR-B\". When the system boots, GRUB (the bootloader) looks at the priorities, tries, and successful flags to determine which partition to use.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=0  You'll notice that on this machine, \"USR-B\" hasn't actually successfully booted. Not to worry! This is a fresh machine that hasn't been through an update cycle yet. When the machine downloads an update, the partition table is updated to allow the newer image to boot.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=1 successful=0  In this case, we see that \"USR-B\" now has a higher priority and it has one try to successfully boot. Once the machine reboots, the partition table will again be updated.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=0  Now we see that the number of tries for \"USR-B\" has been decremented to zero. The successful flag still hasn't been updated though. Once update-engine has had a chance to run, it marks the boot as being successful.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=2 tries=0 successful=1",
            "title": "How do updates work?"
        },
        {
            "location": "/os/manual-rollbacks/#performing-a-manual-rollback",
            "text": "So, now that we understand what happens when the machine updates, we can tweak the process so that it boots an older image (assuming it's still intact on the passive partition). The first command we'll use is  cgpt find -t flatcar-usr . This will give us a list of all of the USR partitions available on the disk.  $ cgpt find -t flatcar-usr\n/dev/sda3\n/dev/sda4  To figure out which partition is currently active, we can use  rootdev .  $ rootdev -s /usr\n/dev/sda4  So now we know that  /dev/sda3  is the passive partition on our system. We can compose the previous two commands to dynamically figure out the passive partition.  $ cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\"\n/dev/sda3  In order to rollback, we need to mark that partition as active using  cgpt prioritize .  $ cgpt prioritize /dev/sda3  If we take another look at the GPT tables, we'll see that the priorities have been updated.        270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=2 tries=0 successful=1\n     2367488     2097152       4  Label: \"USR-B\"\n                                  Type: Alias for flatcar-rootfs\n                                  UUID: E03DD35C-7C2D-4A47-B3FE-27F15780A57C\n                                  Attr: priority=1 tries=0 successful=1  Composing the previous two commands produces the following command to set the currently passive partition to be active on the next boot:  $ cgpt prioritize \"$(cgpt find -t flatcar-usr | grep --invert-match \"$(rootdev -s /usr)\")\"",
            "title": "Performing a manual rollback"
        },
        {
            "location": "/os/manual-rollbacks/#forcing-a-channel-downgrade",
            "text": "The procedure above restores the last known good Flatcar Container Linux version from immediately before an upgrade reboot. The system remains on the same  Flatcar Container Linux channel  after rebooting with the previous USR partition. It is also possible, though not recommended, to switch a Flatcar Container Linux installation to an older release channel, for example to make a system running an Alpha release downgrade to the Stable channel. Root privileges are required for this procedure, noted by  sudo  in the commands below.  First, edit  /etc/coreos/update.conf  to set  GROUP  to the name of the target channel, one of  stable  or  beta :  GROUP=stable  Next, clear the current version number from the  release  file so that the target channel will be certain to have a higher version number, triggering the \"upgrade,\" in this case a downgrade to the lower channel. Since  release  is on a read-only file system, it is convenient to temporarily override it with a bind mount. To do this, copy the original to a writable location, then bind the copy over the system  release  file:  $ cp /usr/share/coreos/release /tmp\n$ sudo mount -o bind /tmp/release /usr/share/coreos/release  The file is now writable, but the bind mount will not survive the reboot, so that the default read-only system  release  file will be restored after this procedure is complete. Edit  /usr/share/coreos/release  to replace the value of  COREOS_RELEASE_VERSION  with  0.0.0 :  COREOS_RELEASE_VERSION=0.0.0  Restart the update service so that it rescans the edited configuration, then initiate an update. The system will reboot into the selected lower channel after downloading the release:  $ sudo systemctl restart update-engine\n$ update_engine_client -update",
            "title": "Forcing a Channel Downgrade"
        },
        {
            "location": "/os/migrating-to-clcs/",
            "text": "Migrating from Cloud-Config to Container Linux Config\n\u00b6\n\n\nHistorically, the recommended way to provision a Flatcar Container Linux machine was with a cloud-config. This was a YAML file specifying things like systemd units to run, users that should exist, and files that should be written. This file would be given to a Flatcar Container Linux machine, and saved on disk. Then a utility called coreos-cloudinit running in a systemd unit would read this file, look at the system state, and make necessary changes on every boot.\n\n\nGoing forward, a new method of provisioning with Container Linux Configs is now recommended.\n\n\nThis document details how to convert an existing cloud-config into a Container Linux Config. Once a Container Linux Config has been written, it is given to the Config Transpiler to be converted into an Ignition Config. This Ignition Config can then be provided to a booting machine. For more information on this process, take a look at the \nprovisioning guide\n.\n\n\nThe etcd and flannel examples shown in this document will use dynamic data in the Container Linux Config (anything looking like this: \n{PRIVATE_IPV4}\n). Not all types of dynamic data are supported on all cloud providers, and if the machine is not on a cloud provider this feature cannot be used. Please see \nhere\n for more information.\n\n\nTo see all supported options available in a Container Linux Config, please look at the \nContainer Linux Config schema\n.\n\n\netcd2\n\u00b6\n\n\nIn a cloud-config, etcd version 2 can be enabled and configured by using the \ncoreos.etcd2.*\n section. As an example of this:\n\n\n#cloud-config\n\ncoreos:\n  etcd2:\n    discovery:                   \"https://discovery.etcd.io/<token>\"\n    advertise-client-urls:       \"http://$public_ipv4:2379\"\n    initial-advertise-peer-urls: \"http://$private_ipv4:2380\"\n    listen-client-urls:          \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n    listen-peer-urls:            \"http://$private_ipv4:2380,http://$private_ipv4:7001\"\n\n\n\netcd can be configured in a more general way with a Container Linux Config. This CL Config will use the etcd-member.service systemd unit rather than the etcd2 service understood by cloud-config and coreos-cloudinit. The etcd-member service will download a version of etcd of the user's choosing and run it. This means that in a Container Linux Config both etcd v2 and v3 can be configured.\n\n\nThis is done under the etcd section:\n\n\netcd:\n    version: 3.1.6\n\n\n\nOmitting the version specification declares that the unit file should use the version of etcd matching the running version of Flatcar Container Linux.\n\n\nConfiguration options in this section can be provided the same way as they were in a cloud-config, with the exception of dashes (\n-\n) being replaced with underscores (\n_\n) in key names.\n\n\netcd:\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"%m=http://{PRIVATE_IPV4}:2380\"\n\n\n\nflannel\n\u00b6\n\n\nFlannel is easily configurable in a cloud-config the same way etcd is, by using the \ncoreos.flannel.*\n section.\n\n\n#cloud-config\n\ncoreos:\n  flannel:\n      etcd_prefix: \"/coreos.com/network2\"\n\n\n\nThe flannel section in a Container Linux Config is used the same way, and a version can optionally be specified for flannel as well.\n\n\nflannel:\n  version:     0.7.0\n  etcd_prefix: \"/coreos.com/network2\"\n\n\n\nlocksmith\n\u00b6\n\n\nThe \ncoreos.locksmith.*\n section in a cloud-config can be used to configure the locksmith daemon via environment variables.\n\n\n#cloud-config\n\ncoreos:\n  locksmith:\n      endpoint: \"http://example.com:2379\"\n\n\n\nLocksmith can be configured in the same way under the locksmith section of a Container Linux Config, but some of the accepted options are slightly different. Also the reboot strategy is set in the locksmith section, instead of the update section. Check out the \nContainer Linux Config schema\n to see what options are available.\n\n\nlocksmith:\n  reboot_strategy: \"reboot\"\n  etcd_endpoints:  \"http://example.com:2379\"\n\n\n\nupdate\n\u00b6\n\n\nThe \ncoreos.update.*\n section can be used to configure the reboot strategy, update group, and update server in a cloud-config.\n\n\n#cloud-config\ncoreos:\n  update:\n    reboot-strategy: \"etcd-lock\"\n    group:           \"stable\"\n    server:          \"https://public.update.flatcar-linux.net/v1/update/\"\n\n\n\nIn the update section in a Container Linux Config the group and server can be configured, but the reboot-strategy option has been moved under the locksmith section.\n\n\nupdate:\n  group:  \"stable\"\n  server: \"https://public.update.flatcar-linux.net/v1/update/\"\n\n\n\nunits\n\u00b6\n\n\nThe \ncoreos.units.*\n section in a cloud-config can define arbitrary systemd units that should be started after booting.\n\n\n#cloud-config\n\ncoreos:\n  units:\n    - name: \"docker-redis.service\"\n      command: \"start\"\n      content: |\n        [Unit]\n        Description=Redis container\n        Author=Me\n        After=docker.service\n\n        [Service]\n        Restart=always\n        ExecStart=/usr/bin/docker start -a redis_server\n        ExecStop=/usr/bin/docker stop -t 2 redis_server\n\n\n\nThis section could also be used to define systemd drop-in files for existing units.\n\n\n#cloud-config\n\ncoreos:\n  units:\n    - name: \"docker.service\"\n      drop-ins:\n        - name: \"50-insecure-registry.conf\"\n          content: |\n            [Service]\n            Environment=DOCKER_OPTS='--insecure-registry=\"10.0.1.0/24\"'\n\n\n\nAnd existing units could also be started without any further configuration.\n\n\n#cloud-config\n\ncoreos:\n  units:\n    - name: \"etcd2.service\"\n      command: \"start\"\n\n\n\nOne big difference in Container Linux Config compared to cloud-configs is that the configuration is applied via \nIgnition\n before the machine has fully booted, as opposed to coreos-cloudinit that runs after the machine has fully booted. As a result units cannot be directly started in a Container Linux Config, the unit is instead enabled so that systemd will begin the unit once systemd starts.\n\n\nNote: in this example an \n[Install]\n section has been added so that the unit can be enabled.\n\n\nsystemd:\n  units:\n    - name: \"docker-redis.service\"\n      enable: true\n      contents: |\n        [Unit]\n        Description=Redis container\n        Author=Me\n        After=docker.service\n\n        [Service]\n        Restart=always\n        ExecStart=/usr/bin/docker start -a redis_server\n        ExecStop=/usr/bin/docker stop -t 2 redis_server\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nDrop-in files can be provided for units in a Container Linux Config just like in a cloud-config.\n\n\nsystemd:\n  units:\n    - name: \"docker.service\"\n      dropins:\n        - name: \"50-insecure-registry.conf\"\n          contents: |\n            [Service]\n            Environment=DOCKER_OPTS='--insecure-registry=\"10.0.1.0/24\"'\n\n\n\nExisting units can also be enabled without configuration.\n\n\nsystemd:\n  units:\n    - name: \"etcd-member.service\"\n      enable: true\n\n\n\nssh_authorized_keys\n\u00b6\n\n\nIn a cloud-config the \nssh_authorized_keys\n section can be used to add ssh public keys to the \ncore\n user.\n\n\n#cloud-config\n\nssh_authorized_keys:\n  - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"\n\n\n\nIn a Container Linux Config there is no analogous section to \nssh_authorized_keys\n, but ssh keys for the core user can be set just as easily using the \npasswd.users.*\n section:\n\n\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"\n\n\n\nhostname\n\u00b6\n\n\nIn a cloud-config the \nhostname\n section can be used to set a machine's hostname.\n\n\n#cloud-config\n\nhostname: \"coreos1\"\n\n\n\nThe Container Linux Config is intentionally more generalized than a cloud-config, and there is no equivalent hostname section understood in a CL Config. Instead, set the hostname by writing it to \n/etc/hostname\n in a CL Config \nstorage.files.*\n section.\n\n\nstorage:\n  files:\n    - filesystem: \"root\"\n      path:       \"/etc/hostname\"\n      mode:       0644\n      contents:\n        inline: coreos1\n\n\n\nusers\n\u00b6\n\n\nThe \nusers\n section in a cloud-config can be used to add users and specify many properties about them, from groups the user should be in to what the user's shell should be.\n\n\n#cloud-config\n\nusers:\n  - name: \"elroy\"\n    passwd: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n    groups:\n      - \"sudo\"\n      - \"docker\"\n    ssh-authorized-keys:\n      - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"\n\n\n\nThis same information can be added to the Container Linux Config in the \npasswd.users.*\n section.\n\n\npasswd:\n  users:\n    - name:          \"elroy\"\n      password_hash: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"\n      groups:\n        - \"sudo\"\n        - \"docker\"\n\n\n\nwrite_files\n\u00b6\n\n\nThe \nwrite_files\n section in a cloud-config can be used to specify files and their contents that should be written to disk on the machine.\n\n\n#cloud-config\nwrite_files:\n  - path:        \"/etc/resolv.conf\"\n    permissions: \"0644\"\n    owner:       \"root\"\n    content: |\n      nameserver 8.8.8.8\n\n\n\nThis can be done in a Container Linux Config with the \nstorage.files.*\n section.\n\n\nstorage:\n  files:\n    - filesystem: \"root\"\n      path:       \"/etc/resolv.conf\"\n      mode:       0644\n      contents:\n        inline: |\n          nameserver 8.8.8.8\n\n\n\nFile specifications in this section of a CL Config must define the target filesystem and the file's path relative to the root of that filesystem. This allows files to be written to filesystems other than the root filesystem.\n\n\nUnder the \ncontents\n section, the file contents are under a sub-section called \ninline\n. This is because a file's contents can be remote by replacing the \ninline\n section with a \nremote\n section. To see what options are available under the \nremote\n section, look at the \nContainer Linux Config schema\n.\n\n\nmanage_etc_hosts\n\u00b6\n\n\nThe \nmanage_etcd_hosts\n section in a cloud-config can be used to configure the contents of the \n/etc/hosts\n file. Currently only one value is supported, \n\"localhost\"\n, which will cause your system's hostname to resolve to \n127.0.0.1\n.\n\n\n#cloud-config\n\nmanage_etc_hosts: \"localhost\"\n\n\n\nThere is no analogous section in a Container Linux Config, however the \n/etc/hosts\n file can be written in the \nstorage.files.*\n section.\n\n\nstorage:\n  files:\n    - filesystem: \"root\"\n      path:       \"/etc/hosts\"\n      mode:       0644\n      contents:\n        inline: |\n          127.0.0.1 localhost\n          ::1       localhost\n          127.0.0.1 example.com",
            "title": "Migrating from Cloud-Config to Container Linux Config"
        },
        {
            "location": "/os/migrating-to-clcs/#migrating-from-cloud-config-to-container-linux-config",
            "text": "Historically, the recommended way to provision a Flatcar Container Linux machine was with a cloud-config. This was a YAML file specifying things like systemd units to run, users that should exist, and files that should be written. This file would be given to a Flatcar Container Linux machine, and saved on disk. Then a utility called coreos-cloudinit running in a systemd unit would read this file, look at the system state, and make necessary changes on every boot.  Going forward, a new method of provisioning with Container Linux Configs is now recommended.  This document details how to convert an existing cloud-config into a Container Linux Config. Once a Container Linux Config has been written, it is given to the Config Transpiler to be converted into an Ignition Config. This Ignition Config can then be provided to a booting machine. For more information on this process, take a look at the  provisioning guide .  The etcd and flannel examples shown in this document will use dynamic data in the Container Linux Config (anything looking like this:  {PRIVATE_IPV4} ). Not all types of dynamic data are supported on all cloud providers, and if the machine is not on a cloud provider this feature cannot be used. Please see  here  for more information.  To see all supported options available in a Container Linux Config, please look at the  Container Linux Config schema .",
            "title": "Migrating from Cloud-Config to Container Linux Config"
        },
        {
            "location": "/os/migrating-to-clcs/#etcd2",
            "text": "In a cloud-config, etcd version 2 can be enabled and configured by using the  coreos.etcd2.*  section. As an example of this:  #cloud-config\n\ncoreos:\n  etcd2:\n    discovery:                   \"https://discovery.etcd.io/<token>\"\n    advertise-client-urls:       \"http://$public_ipv4:2379\"\n    initial-advertise-peer-urls: \"http://$private_ipv4:2380\"\n    listen-client-urls:          \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n    listen-peer-urls:            \"http://$private_ipv4:2380,http://$private_ipv4:7001\"  etcd can be configured in a more general way with a Container Linux Config. This CL Config will use the etcd-member.service systemd unit rather than the etcd2 service understood by cloud-config and coreos-cloudinit. The etcd-member service will download a version of etcd of the user's choosing and run it. This means that in a Container Linux Config both etcd v2 and v3 can be configured.  This is done under the etcd section:  etcd:\n    version: 3.1.6  Omitting the version specification declares that the unit file should use the version of etcd matching the running version of Flatcar Container Linux.  Configuration options in this section can be provided the same way as they were in a cloud-config, with the exception of dashes ( - ) being replaced with underscores ( _ ) in key names.  etcd:\n  name:                        \"{HOSTNAME}\"\n  advertise_client_urls:       \"{PRIVATE_IPV4}:2379\"\n  initial_advertise_peer_urls: \"{PRIVATE_IPV4}:2380\"\n  listen_client_urls:          \"http://0.0.0.0:2379\"\n  listen_peer_urls:            \"http://{PRIVATE_IPV4}:2380\"\n  initial_cluster:             \"%m=http://{PRIVATE_IPV4}:2380\"",
            "title": "etcd2"
        },
        {
            "location": "/os/migrating-to-clcs/#flannel",
            "text": "Flannel is easily configurable in a cloud-config the same way etcd is, by using the  coreos.flannel.*  section.  #cloud-config\n\ncoreos:\n  flannel:\n      etcd_prefix: \"/coreos.com/network2\"  The flannel section in a Container Linux Config is used the same way, and a version can optionally be specified for flannel as well.  flannel:\n  version:     0.7.0\n  etcd_prefix: \"/coreos.com/network2\"",
            "title": "flannel"
        },
        {
            "location": "/os/migrating-to-clcs/#locksmith",
            "text": "The  coreos.locksmith.*  section in a cloud-config can be used to configure the locksmith daemon via environment variables.  #cloud-config\n\ncoreos:\n  locksmith:\n      endpoint: \"http://example.com:2379\"  Locksmith can be configured in the same way under the locksmith section of a Container Linux Config, but some of the accepted options are slightly different. Also the reboot strategy is set in the locksmith section, instead of the update section. Check out the  Container Linux Config schema  to see what options are available.  locksmith:\n  reboot_strategy: \"reboot\"\n  etcd_endpoints:  \"http://example.com:2379\"",
            "title": "locksmith"
        },
        {
            "location": "/os/migrating-to-clcs/#update",
            "text": "The  coreos.update.*  section can be used to configure the reboot strategy, update group, and update server in a cloud-config.  #cloud-config\ncoreos:\n  update:\n    reboot-strategy: \"etcd-lock\"\n    group:           \"stable\"\n    server:          \"https://public.update.flatcar-linux.net/v1/update/\"  In the update section in a Container Linux Config the group and server can be configured, but the reboot-strategy option has been moved under the locksmith section.  update:\n  group:  \"stable\"\n  server: \"https://public.update.flatcar-linux.net/v1/update/\"",
            "title": "update"
        },
        {
            "location": "/os/migrating-to-clcs/#units",
            "text": "The  coreos.units.*  section in a cloud-config can define arbitrary systemd units that should be started after booting.  #cloud-config\n\ncoreos:\n  units:\n    - name: \"docker-redis.service\"\n      command: \"start\"\n      content: |\n        [Unit]\n        Description=Redis container\n        Author=Me\n        After=docker.service\n\n        [Service]\n        Restart=always\n        ExecStart=/usr/bin/docker start -a redis_server\n        ExecStop=/usr/bin/docker stop -t 2 redis_server  This section could also be used to define systemd drop-in files for existing units.  #cloud-config\n\ncoreos:\n  units:\n    - name: \"docker.service\"\n      drop-ins:\n        - name: \"50-insecure-registry.conf\"\n          content: |\n            [Service]\n            Environment=DOCKER_OPTS='--insecure-registry=\"10.0.1.0/24\"'  And existing units could also be started without any further configuration.  #cloud-config\n\ncoreos:\n  units:\n    - name: \"etcd2.service\"\n      command: \"start\"  One big difference in Container Linux Config compared to cloud-configs is that the configuration is applied via  Ignition  before the machine has fully booted, as opposed to coreos-cloudinit that runs after the machine has fully booted. As a result units cannot be directly started in a Container Linux Config, the unit is instead enabled so that systemd will begin the unit once systemd starts.  Note: in this example an  [Install]  section has been added so that the unit can be enabled.  systemd:\n  units:\n    - name: \"docker-redis.service\"\n      enable: true\n      contents: |\n        [Unit]\n        Description=Redis container\n        Author=Me\n        After=docker.service\n\n        [Service]\n        Restart=always\n        ExecStart=/usr/bin/docker start -a redis_server\n        ExecStop=/usr/bin/docker stop -t 2 redis_server\n\n        [Install]\n        WantedBy=multi-user.target  Drop-in files can be provided for units in a Container Linux Config just like in a cloud-config.  systemd:\n  units:\n    - name: \"docker.service\"\n      dropins:\n        - name: \"50-insecure-registry.conf\"\n          contents: |\n            [Service]\n            Environment=DOCKER_OPTS='--insecure-registry=\"10.0.1.0/24\"'  Existing units can also be enabled without configuration.  systemd:\n  units:\n    - name: \"etcd-member.service\"\n      enable: true",
            "title": "units"
        },
        {
            "location": "/os/migrating-to-clcs/#ssh_authorized_keys",
            "text": "In a cloud-config the  ssh_authorized_keys  section can be used to add ssh public keys to the  core  user.  #cloud-config\n\nssh_authorized_keys:\n  - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"  In a Container Linux Config there is no analogous section to  ssh_authorized_keys , but ssh keys for the core user can be set just as easily using the  passwd.users.*  section:  passwd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"",
            "title": "ssh_authorized_keys"
        },
        {
            "location": "/os/migrating-to-clcs/#hostname",
            "text": "In a cloud-config the  hostname  section can be used to set a machine's hostname.  #cloud-config\n\nhostname: \"coreos1\"  The Container Linux Config is intentionally more generalized than a cloud-config, and there is no equivalent hostname section understood in a CL Config. Instead, set the hostname by writing it to  /etc/hostname  in a CL Config  storage.files.*  section.  storage:\n  files:\n    - filesystem: \"root\"\n      path:       \"/etc/hostname\"\n      mode:       0644\n      contents:\n        inline: coreos1",
            "title": "hostname"
        },
        {
            "location": "/os/migrating-to-clcs/#users",
            "text": "The  users  section in a cloud-config can be used to add users and specify many properties about them, from groups the user should be in to what the user's shell should be.  #cloud-config\n\nusers:\n  - name: \"elroy\"\n    passwd: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n    groups:\n      - \"sudo\"\n      - \"docker\"\n    ssh-authorized-keys:\n      - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"  This same information can be added to the Container Linux Config in the  passwd.users.*  section.  passwd:\n  users:\n    - name:          \"elroy\"\n      password_hash: \"$6$5s2u6/jR$un0AvWnqilcgaNB3Mkxd5yYv6mTlWfOoCYHZmfi3LDKVltj.E8XNKEcwWm...\"\n      ssh_authorized_keys:\n        - \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0g+ZTxC7weoIJLUafOgrm+h...\"\n      groups:\n        - \"sudo\"\n        - \"docker\"",
            "title": "users"
        },
        {
            "location": "/os/migrating-to-clcs/#write_files",
            "text": "The  write_files  section in a cloud-config can be used to specify files and their contents that should be written to disk on the machine.  #cloud-config\nwrite_files:\n  - path:        \"/etc/resolv.conf\"\n    permissions: \"0644\"\n    owner:       \"root\"\n    content: |\n      nameserver 8.8.8.8  This can be done in a Container Linux Config with the  storage.files.*  section.  storage:\n  files:\n    - filesystem: \"root\"\n      path:       \"/etc/resolv.conf\"\n      mode:       0644\n      contents:\n        inline: |\n          nameserver 8.8.8.8  File specifications in this section of a CL Config must define the target filesystem and the file's path relative to the root of that filesystem. This allows files to be written to filesystems other than the root filesystem.  Under the  contents  section, the file contents are under a sub-section called  inline . This is because a file's contents can be remote by replacing the  inline  section with a  remote  section. To see what options are available under the  remote  section, look at the  Container Linux Config schema .",
            "title": "write_files"
        },
        {
            "location": "/os/migrating-to-clcs/#manage_etc_hosts",
            "text": "The  manage_etcd_hosts  section in a cloud-config can be used to configure the contents of the  /etc/hosts  file. Currently only one value is supported,  \"localhost\" , which will cause your system's hostname to resolve to  127.0.0.1 .  #cloud-config\n\nmanage_etc_hosts: \"localhost\"  There is no analogous section in a Container Linux Config, however the  /etc/hosts  file can be written in the  storage.files.*  section.  storage:\n  files:\n    - filesystem: \"root\"\n      path:       \"/etc/hosts\"\n      mode:       0644\n      contents:\n        inline: |\n          127.0.0.1 localhost\n          ::1       localhost\n          127.0.0.1 example.com",
            "title": "manage_etc_hosts"
        },
        {
            "location": "/os/mounting-storage/",
            "text": "Mounting storage\n\u00b6\n\n\nContainer Linux Configs can be used to format and attach additional filesystems to Flatcar Container Linux nodes, whether such storage is provided by an underlying cloud platform, physical disk, SAN, or NAS system. This is done by specifying how partitions should be mounted in the config, and then using a \nsystemd mount unit\n to mount the partition. By \nsystemd convention\n, mount unit names derive from the target mount point, with interior slashes replaced by dashes, and the \n.mount\n extension appended. A unit mounting onto \n/var/www\n is thus named \nvar-www.mount\n.\n\n\nMount units name the source filesystem and target mount point, and optionally the filesystem type. \nSystemd\n mounts filesystems defined in such units at boot time. The following example formats an \nEC2 ephemeral disk\n and then mounts it at the node's \n/media/ephemeral\n directory. The mount unit is therefore named \nmedia-ephemeral.mount\n.\n\n\nstorage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target\n\n\n\nUse attached storage for Docker\n\u00b6\n\n\nDocker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Be aware that some cloud providers treat certain disks as ephemeral and you will lose all Docker images contained on that disk.\n\n\nWe're going to format a device as ext4 and then mount it to \n/var/lib/docker\n, where Docker stores images. Be sure to hardcode the correct device or look for a device by label:\n\n\nstorage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Description=Mount ephemeral to /var/lib/docker\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/var/lib/docker\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target\n    - name: docker.service\n      dropins:\n        - name: 10-wait-docker.conf\n          contents: |\n            [Unit]\n            After=var-lib-docker.mount\n            Requires=var-lib-docker.mount\n\n\n\nCreating and mounting a btrfs volume file\n\u00b6\n\n\nFlatcar Container Linux uses ext4 + overlayfs to provide a layered filesystem for the root partition. If you'd like to use btrfs for your Docker containers, you can do so with two systemd units: one that creates and formats a btrfs volume file and another that mounts it.\n\n\nIn this example, we are going to mount a new 25GB btrfs volume file to \n/var/lib/docker\n. One can verify that Docker is using the btrfs storage driver once the Docker service has started by executing \nsudo docker info\n. We recommend allocating \nno more than 85%\n of the available disk space for a btrfs filesystem as journald will also require space on the host filesystem.\n\n\nsystemd:\n  units:\n    - name: format-var-lib-docker.service\n      contents: |\n        [Unit]\n        Before=docker.service var-lib-docker.mount\n        RequiresMountsFor=/var/lib\n        ConditionPathExists=!/var/lib/docker.btrfs\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/truncate --size=25G /var/lib/docker.btrfs\n        ExecStart=/usr/sbin/mkfs.btrfs /var/lib/docker.btrfs\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=docker.service\n        After=format-var-lib-docker.service\n        Requires=format-var-lib-docker.service\n        [Mount]\n        What=/var/lib/docker.btrfs\n        Where=/var/lib/docker\n        Type=btrfs\n        Options=loop,discard\n        [Install]\n        RequiredBy=docker.service\n\n\n\nNote the declaration of \nConditionPathExists=!/var/lib/docker.btrfs\n. Without this line, systemd would reformat the btrfs filesystem every time the machine starts.\n\n\nMounting NFS exports\n\u00b6\n\n\nThis Container Linux Config excerpt mounts an NFS export onto the Flatcar Container Linux node's \n/var/www\n.\n\n\nsystemd:\n  units:\n    - name: var-www.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=remote-fs.target\n        [Mount]\n        What=nfs.example.com:/var/www\n        Where=/var/www\n        Type=nfs\n        [Install]\n        WantedBy=remote-fs.target\n\n\n\nTo declare that another service depends on this mount, name the mount unit in the dependent unit's \nAfter\n and \nRequires\n properties:\n\n\n[Unit]\nAfter=var-www.mount\nRequires=var-www.mount\n\n\n\nIf the mount fails, dependent units will not start.\n\n\nFurther reading\n\u00b6\n\n\nCheck the \nsystemd mount\n docs\n to learn about the available options. Examples specific to \nEC2\n, \nGoogle Compute Engine\n can be used as a starting point.",
            "title": "Mounting storage"
        },
        {
            "location": "/os/mounting-storage/#mounting-storage",
            "text": "Container Linux Configs can be used to format and attach additional filesystems to Flatcar Container Linux nodes, whether such storage is provided by an underlying cloud platform, physical disk, SAN, or NAS system. This is done by specifying how partitions should be mounted in the config, and then using a  systemd mount unit  to mount the partition. By  systemd convention , mount unit names derive from the target mount point, with interior slashes replaced by dashes, and the  .mount  extension appended. A unit mounting onto  /var/www  is thus named  var-www.mount .  Mount units name the source filesystem and target mount point, and optionally the filesystem type.  Systemd  mounts filesystems defined in such units at boot time. The following example formats an  EC2 ephemeral disk  and then mounts it at the node's  /media/ephemeral  directory. The mount unit is therefore named  media-ephemeral.mount .  storage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: media-ephemeral.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/media/ephemeral\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target",
            "title": "Mounting storage"
        },
        {
            "location": "/os/mounting-storage/#use-attached-storage-for-docker",
            "text": "Docker containers can be very large and debugging a build process makes it easy to accumulate hundreds of containers. It's advantageous to use attached storage to expand your capacity for container images. Be aware that some cloud providers treat certain disks as ephemeral and you will lose all Docker images contained on that disk.  We're going to format a device as ext4 and then mount it to  /var/lib/docker , where Docker stores images. Be sure to hardcode the correct device or look for a device by label:  storage:\n  filesystems:\n    - name: ephemeral1\n      mount:\n        device: /dev/xvdb\n        format: ext4\n        wipe_filesystem: true\nsystemd:\n  units:\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Description=Mount ephemeral to /var/lib/docker\n        Before=local-fs.target\n        [Mount]\n        What=/dev/xvdb\n        Where=/var/lib/docker\n        Type=ext4\n        [Install]\n        WantedBy=local-fs.target\n    - name: docker.service\n      dropins:\n        - name: 10-wait-docker.conf\n          contents: |\n            [Unit]\n            After=var-lib-docker.mount\n            Requires=var-lib-docker.mount",
            "title": "Use attached storage for Docker"
        },
        {
            "location": "/os/mounting-storage/#creating-and-mounting-a-btrfs-volume-file",
            "text": "Flatcar Container Linux uses ext4 + overlayfs to provide a layered filesystem for the root partition. If you'd like to use btrfs for your Docker containers, you can do so with two systemd units: one that creates and formats a btrfs volume file and another that mounts it.  In this example, we are going to mount a new 25GB btrfs volume file to  /var/lib/docker . One can verify that Docker is using the btrfs storage driver once the Docker service has started by executing  sudo docker info . We recommend allocating  no more than 85%  of the available disk space for a btrfs filesystem as journald will also require space on the host filesystem.  systemd:\n  units:\n    - name: format-var-lib-docker.service\n      contents: |\n        [Unit]\n        Before=docker.service var-lib-docker.mount\n        RequiresMountsFor=/var/lib\n        ConditionPathExists=!/var/lib/docker.btrfs\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/truncate --size=25G /var/lib/docker.btrfs\n        ExecStart=/usr/sbin/mkfs.btrfs /var/lib/docker.btrfs\n    - name: var-lib-docker.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=docker.service\n        After=format-var-lib-docker.service\n        Requires=format-var-lib-docker.service\n        [Mount]\n        What=/var/lib/docker.btrfs\n        Where=/var/lib/docker\n        Type=btrfs\n        Options=loop,discard\n        [Install]\n        RequiredBy=docker.service  Note the declaration of  ConditionPathExists=!/var/lib/docker.btrfs . Without this line, systemd would reformat the btrfs filesystem every time the machine starts.",
            "title": "Creating and mounting a btrfs volume file"
        },
        {
            "location": "/os/mounting-storage/#mounting-nfs-exports",
            "text": "This Container Linux Config excerpt mounts an NFS export onto the Flatcar Container Linux node's  /var/www .  systemd:\n  units:\n    - name: var-www.mount\n      enable: true\n      contents: |\n        [Unit]\n        Before=remote-fs.target\n        [Mount]\n        What=nfs.example.com:/var/www\n        Where=/var/www\n        Type=nfs\n        [Install]\n        WantedBy=remote-fs.target  To declare that another service depends on this mount, name the mount unit in the dependent unit's  After  and  Requires  properties:  [Unit]\nAfter=var-www.mount\nRequires=var-www.mount  If the mount fails, dependent units will not start.",
            "title": "Mounting NFS exports"
        },
        {
            "location": "/os/mounting-storage/#further-reading",
            "text": "Check the  systemd mount  docs  to learn about the available options. Examples specific to  EC2 ,  Google Compute Engine  can be used as a starting point.",
            "title": "Further reading"
        },
        {
            "location": "/os/network-config-with-networkd/",
            "text": "Network configuration with networkd\n\u00b6\n\n\nFlatcar Container Linux machines are preconfigured with \nnetworking customized\n for each platform. You can write your own networkd units to replace or override the units created for each platform. This article covers a subset of networkd functionality. You can view the \nfull docs here\n.\n\n\nDrop a networkd unit in \n/etc/systemd/network/\n or inject a unit on boot via a Container Linux Config. Files placed manually on the filesystem will need to reload networkd afterwards with \nsudo systemctl restart systemd-networkd\n. Network units injected via a Container Linux Config will be written to the system before networkd is started, so there are no work-arounds needed.\n\n\nLet's take a look at two common situations: using a static IP and turning off DHCP.\n\n\nStatic networking\n\u00b6\n\n\nTo configure a static IP on \nenp2s0\n, create \nstatic.network\n:\n\n\n[Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4\n\n\n\nPlace the file in \n/etc/systemd/network/\n. To apply the configuration, run:\n\n\nsudo systemctl restart systemd-networkd\n\n\n\nContainer Linux Config\n\u00b6\n\n\nSetting up static networking in your Container Linux Config can be done by writing out the network unit. Be sure to modify the \n[Match]\n section with the name of your desired interface, and replace the IPs:\n\n\nnetworkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n\n\n\nTurn off DHCP on specific interface\n\u00b6\n\n\nIf you'd like to use DHCP on all interfaces except \nenp2s0\n, create two files. They'll be checked in lexical order, as described in the \nfull network docs\n. Any interfaces matching during earlier files will be ignored during later files.\n\n\n10-static.network\n\u00b6\n\n\n[Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4\n\n\n\nPut your settings-of-last-resort in \n20-dhcp.network\n. For example, any interfaces matching \nen*\n that weren't matched in \n10-static.network\n will be configured with DHCP:\n\n\n20-dhcp.network\n\u00b6\n\n\n[Match]\nName=en*\n\n[Network]\nDHCP=yes\n\n\n\nTo apply the configuration, run \nsudo systemctl restart systemd-networkd\n. Check the status with \nsystemctl status systemd-networkd\n and read the full log with \njournalctl -u systemd-networkd\n.\n\n\nTurn off IPv6 on specific interfaces\n\u00b6\n\n\nWhile IPv6 can be disabled globally at boot by appending \nipv6.disable=1\n to the kernel command line, networkd supports disabling IPv6 on a per-interface basis. When a network unit's \n[Network]\n section has either \nLinkLocalAddressing=ipv4\n or \nLinkLocalAddressing=no\n, networkd will not try to configure IPv6 on the matching interfaces.\n\n\nNote however that even when using the above option, networkd will still be expecting to receive router advertisements if IPv6 is not disabled globally. If IPv6 traffic is not being received by the interface (e.g. due to \nsysctl\n or \nip6tables\n settings), it will remain in the \nconfiguring\n state and potentially cause timeouts for services waiting for the network to be fully configured. To avoid this, the \nIPv6AcceptRA=no\n option should also be set in the \n[Network]\n section.\n\n\nA network unit file's \n[Network]\n section should therefore contain the following to disable IPv6 on its matching interfaces.\n\n\n[Network]\nLinkLocalAddressing=no\nIPv6AcceptRA=no\n\n\n\nConfigure static routes\n\u00b6\n\n\nSpecify static routes in a systemd network unit's \n[Route]\n section. In this example, we create a unit file, \n10-static.network\n, and define in it a static route to the \n172.16.0.0/24\n subnet:\n\n\n10-static.network\n\u00b6\n\n\n[Route]\nGateway=192.168.122.1\nDestination=172.16.0.0/24\n\n\n\nTo specify the same route in a Container Linux Config, create the systemd network unit there instead:\n\n\nnetworkd:\n  units:\n    - name: 10-static.network\n      contents: |\n        [Route]\n        Gateway=192.168.122.1\n        Destination=172.16.0.0/24\n\n\n\nConfigure multiple IP addresses\n\u00b6\n\n\nTo configure multiple IP addresses on one interface, we define multiple \nAddress\n keys in the network unit. In the example below, we've also defined a different gateway for each IP address.\n\n\n20-multi_ip.network\n\u00b6\n\n\n[Match]\nName=eth0\n\n[Network]\nDNS=8.8.8.8\nAddress=10.0.0.101/24\nGateway=10.0.0.1\nAddress=10.0.1.101/24\nGateway=10.0.1.1\n\n\n\nTo do the same thing through a Container Linux Config:\n\n\nnetworkd:\n  units:\n    - name: 20-multi_ip.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=8.8.8.8\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n        Address=10.0.1.101/24\n        Gateway=10.0.1.1\n\n\n\nDebugging networkd\n\u00b6\n\n\nIf you've faced some problems with networkd you can enable debug mode following the instructions below.\n\n\nEnable debugging manually\n\u00b6\n\n\nmkdir -p /etc/systemd/system/systemd-networkd.service.d/\n\n\n\nCreate \nDrop-In\n \n/etc/systemd/system/systemd-networkd.service.d/10-debug.conf\n with following content:\n\n\n[Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nAnd restart \nsystemd-networkd\n service:\n\n\nsystemctl daemon-reload\nsystemctl restart systemd-networkd\njournalctl -b -u systemd-networkd\n\n\n\nEnable debugging through a Container Linux Config\n\u00b6\n\n\nDefine a \nDrop-In\n in a \nContainer Linux Config\n:\n\n\nsystemd:\n  units:\n    - name: systemd-networkd.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nFurther reading\n\u00b6\n\n\nIf you're interested in more general networkd features, check out the \nfull documentation\n.\n\n\nGetting Started with systemd\n\n\nReading the System Log",
            "title": "Network configuration with networkd"
        },
        {
            "location": "/os/network-config-with-networkd/#network-configuration-with-networkd",
            "text": "Flatcar Container Linux machines are preconfigured with  networking customized  for each platform. You can write your own networkd units to replace or override the units created for each platform. This article covers a subset of networkd functionality. You can view the  full docs here .  Drop a networkd unit in  /etc/systemd/network/  or inject a unit on boot via a Container Linux Config. Files placed manually on the filesystem will need to reload networkd afterwards with  sudo systemctl restart systemd-networkd . Network units injected via a Container Linux Config will be written to the system before networkd is started, so there are no work-arounds needed.  Let's take a look at two common situations: using a static IP and turning off DHCP.",
            "title": "Network configuration with networkd"
        },
        {
            "location": "/os/network-config-with-networkd/#static-networking",
            "text": "To configure a static IP on  enp2s0 , create  static.network :  [Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4  Place the file in  /etc/systemd/network/ . To apply the configuration, run:  sudo systemctl restart systemd-networkd",
            "title": "Static networking"
        },
        {
            "location": "/os/network-config-with-networkd/#container-linux-config",
            "text": "Setting up static networking in your Container Linux Config can be done by writing out the network unit. Be sure to modify the  [Match]  section with the name of your desired interface, and replace the IPs:  networkd:\n  units:\n    - name: 00-eth0.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=1.2.3.4\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/network-config-with-networkd/#turn-off-dhcp-on-specific-interface",
            "text": "If you'd like to use DHCP on all interfaces except  enp2s0 , create two files. They'll be checked in lexical order, as described in the  full network docs . Any interfaces matching during earlier files will be ignored during later files.",
            "title": "Turn off DHCP on specific interface"
        },
        {
            "location": "/os/network-config-with-networkd/#10-staticnetwork",
            "text": "[Match]\nName=enp2s0\n\n[Network]\nAddress=192.168.0.15/24\nGateway=192.168.0.1\nDNS=1.2.3.4  Put your settings-of-last-resort in  20-dhcp.network . For example, any interfaces matching  en*  that weren't matched in  10-static.network  will be configured with DHCP:",
            "title": "10-static.network"
        },
        {
            "location": "/os/network-config-with-networkd/#20-dhcpnetwork",
            "text": "[Match]\nName=en*\n\n[Network]\nDHCP=yes  To apply the configuration, run  sudo systemctl restart systemd-networkd . Check the status with  systemctl status systemd-networkd  and read the full log with  journalctl -u systemd-networkd .",
            "title": "20-dhcp.network"
        },
        {
            "location": "/os/network-config-with-networkd/#turn-off-ipv6-on-specific-interfaces",
            "text": "While IPv6 can be disabled globally at boot by appending  ipv6.disable=1  to the kernel command line, networkd supports disabling IPv6 on a per-interface basis. When a network unit's  [Network]  section has either  LinkLocalAddressing=ipv4  or  LinkLocalAddressing=no , networkd will not try to configure IPv6 on the matching interfaces.  Note however that even when using the above option, networkd will still be expecting to receive router advertisements if IPv6 is not disabled globally. If IPv6 traffic is not being received by the interface (e.g. due to  sysctl  or  ip6tables  settings), it will remain in the  configuring  state and potentially cause timeouts for services waiting for the network to be fully configured. To avoid this, the  IPv6AcceptRA=no  option should also be set in the  [Network]  section.  A network unit file's  [Network]  section should therefore contain the following to disable IPv6 on its matching interfaces.  [Network]\nLinkLocalAddressing=no\nIPv6AcceptRA=no",
            "title": "Turn off IPv6 on specific interfaces"
        },
        {
            "location": "/os/network-config-with-networkd/#configure-static-routes",
            "text": "Specify static routes in a systemd network unit's  [Route]  section. In this example, we create a unit file,  10-static.network , and define in it a static route to the  172.16.0.0/24  subnet:",
            "title": "Configure static routes"
        },
        {
            "location": "/os/network-config-with-networkd/#10-staticnetwork_1",
            "text": "[Route]\nGateway=192.168.122.1\nDestination=172.16.0.0/24  To specify the same route in a Container Linux Config, create the systemd network unit there instead:  networkd:\n  units:\n    - name: 10-static.network\n      contents: |\n        [Route]\n        Gateway=192.168.122.1\n        Destination=172.16.0.0/24",
            "title": "10-static.network"
        },
        {
            "location": "/os/network-config-with-networkd/#configure-multiple-ip-addresses",
            "text": "To configure multiple IP addresses on one interface, we define multiple  Address  keys in the network unit. In the example below, we've also defined a different gateway for each IP address.",
            "title": "Configure multiple IP addresses"
        },
        {
            "location": "/os/network-config-with-networkd/#20-multi_ipnetwork",
            "text": "[Match]\nName=eth0\n\n[Network]\nDNS=8.8.8.8\nAddress=10.0.0.101/24\nGateway=10.0.0.1\nAddress=10.0.1.101/24\nGateway=10.0.1.1  To do the same thing through a Container Linux Config:  networkd:\n  units:\n    - name: 20-multi_ip.network\n      contents: |\n        [Match]\n        Name=eth0\n\n        [Network]\n        DNS=8.8.8.8\n        Address=10.0.0.101/24\n        Gateway=10.0.0.1\n        Address=10.0.1.101/24\n        Gateway=10.0.1.1",
            "title": "20-multi_ip.network"
        },
        {
            "location": "/os/network-config-with-networkd/#debugging-networkd",
            "text": "If you've faced some problems with networkd you can enable debug mode following the instructions below.",
            "title": "Debugging networkd"
        },
        {
            "location": "/os/network-config-with-networkd/#enable-debugging-manually",
            "text": "mkdir -p /etc/systemd/system/systemd-networkd.service.d/  Create  Drop-In   /etc/systemd/system/systemd-networkd.service.d/10-debug.conf  with following content:  [Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug  And restart  systemd-networkd  service:  systemctl daemon-reload\nsystemctl restart systemd-networkd\njournalctl -b -u systemd-networkd",
            "title": "Enable debugging manually"
        },
        {
            "location": "/os/network-config-with-networkd/#enable-debugging-through-a-container-linux-config",
            "text": "Define a  Drop-In  in a  Container Linux Config :  systemd:\n  units:\n    - name: systemd-networkd.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug",
            "title": "Enable debugging through a Container Linux Config"
        },
        {
            "location": "/os/network-config-with-networkd/#further-reading",
            "text": "If you're interested in more general networkd features, check out the  full documentation .  Getting Started with systemd  Reading the System Log",
            "title": "Further reading"
        },
        {
            "location": "/os/notes-for-distributors/",
            "text": "Notes for distributors\n\u00b6\n\n\nImporting images\n\u00b6\n\n\nImages of Flatcar Container Linux alpha releases are hosted at \nhttps://alpha.release.flatcar-linux.net/amd64-usr/\n. There are directories for releases by version as well as \ncurrent\n with a copy of the latest version. Similarly, beta releases can be found at \nhttps://beta.release.flatcar-linux.net/amd64-usr/\n, edge releases at \nhttps://edge.release.flatcar-linux.net/amd64-usr/\n, and stable releases at \nhttps://stable.release.flatcar-linux.net/amd64-usr/\n.\n\n\nEach directory has a \nversion.txt\n file containing version information for the files in that directory. If you are importing images for use inside your environment it is recommended that you fetch \nversion.txt\n from the \ncurrent\n directory and use its contents to compute the path to the other artifacts. For example, to download the alpha OpenStack version of Flatcar Container Linux:\n\n\n\n\nDownload \nhttps://alpha.release.flatcar-linux.net/amd64-usr/current/version.txt\n.\n\n\nParse \nversion.txt\n to obtain the value of \nCOREOS_VERSION_ID\n, for example \n1576.1.0\n.\n\n\nDownload \nhttps://alpha.release.flatcar-linux.net/amd64-usr/1576.1.0/flatcar_production_openstack_image.img.bz2\n.\n\n\n\n\nIt is recommended that you also verify files using the \nFlatcar Container Linux Image Signing Key\n. The GPG signature for each image is a detached \n.sig\n file that must be passed to \ngpg --verify\n. For example:\n\n\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_openstack_image.img.bz2\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_openstack_image.img.bz2.sig\ngpg --verify flatcar_production_openstack_image.img.bz2.sig\n\n\n\nThe signing key is rotated annually. We will announce upcoming rotations of the signing key on the \nuser mailing list\n.\n\n\nImage customization\n\u00b6\n\n\nThere are two predominant ways that a Flatcar Container Linux image can be easily customized for a specific operating environment: through Ignition, a first-boot provisioning tool that runs during a machine's boot process, and through \ncloud-config\n, an older tool that runs every time a machine boots.\n\n\nIgnition\n\u00b6\n\n\nIgnition\n is a tool that acquires a JSON config file when a machine first boots, and uses this config to perform tasks such as formatting disks, creating files, modifying and creating users, and adding systemd units. How Ignition acquires this config file varies per-platform, and it is highly recommended that providers ensure Ignition has \nsupport for their platform\n.\n\n\nUse Ignition to handle platform specific configuration such as custom networking, running an agent on the machine, or injecting files onto disk. To do this, place an Ignition config at \n/usr/share/oem/base/base.ign\n and it will be prepended to the user provided config. In addition, any config placed at \n/usr/share/oem/base/default.ign\n will be executed if a user config is not found. On platforms that support cloud-config, use this feature to run coreos-cloudinit when no Ignition config is provided.\n\n\nAdditionally, it is recommended that providers ensure that \ncoreos-metadata\n and \nct\n have support for their platform. This will allow a nicer user experience, as coreos-metadata will be able to install users' ssh keys and users will be able to reference dynamic data in their Container Linux Configs.\n\n\nCloud config\n\u00b6\n\n\nA Flatcar Container Linux image can also be customized using \ncloud-config\n. Users are recommended to instead use Container Linux Configs (that are converted into Ignition configs with \nct\n), for reasons \noutlined in the blog post that introduced Ignition\n.\n\n\nProviders that previously supported cloud-config should continue to do so, as not all users have switched over to Container Linux Configs. New platforms do not need to support cloud-config.\n\n\nFlatcar Container Linux will automatically parse and execute \n/usr/share/oem/cloud-config.yml\n if it exists.\n\n\nHandling end-user Ignition files\n\u00b6\n\n\nEnd-users should be able to provide an Ignition file to your platform while specifying their VM's parameters. This file should be made available to Flatcar Container Linux at the time of boot (e.g. at known network address, injected directly onto disk). Examples of these data sources can be found in the \nIgnition documentation\n.",
            "title": "Notes for distributors"
        },
        {
            "location": "/os/notes-for-distributors/#notes-for-distributors",
            "text": "",
            "title": "Notes for distributors"
        },
        {
            "location": "/os/notes-for-distributors/#importing-images",
            "text": "Images of Flatcar Container Linux alpha releases are hosted at  https://alpha.release.flatcar-linux.net/amd64-usr/ . There are directories for releases by version as well as  current  with a copy of the latest version. Similarly, beta releases can be found at  https://beta.release.flatcar-linux.net/amd64-usr/ , edge releases at  https://edge.release.flatcar-linux.net/amd64-usr/ , and stable releases at  https://stable.release.flatcar-linux.net/amd64-usr/ .  Each directory has a  version.txt  file containing version information for the files in that directory. If you are importing images for use inside your environment it is recommended that you fetch  version.txt  from the  current  directory and use its contents to compute the path to the other artifacts. For example, to download the alpha OpenStack version of Flatcar Container Linux:   Download  https://alpha.release.flatcar-linux.net/amd64-usr/current/version.txt .  Parse  version.txt  to obtain the value of  COREOS_VERSION_ID , for example  1576.1.0 .  Download  https://alpha.release.flatcar-linux.net/amd64-usr/1576.1.0/flatcar_production_openstack_image.img.bz2 .   It is recommended that you also verify files using the  Flatcar Container Linux Image Signing Key . The GPG signature for each image is a detached  .sig  file that must be passed to  gpg --verify . For example:  wget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_openstack_image.img.bz2\nwget https://alpha.release.flatcar-linux.net/amd64-usr/current/flatcar_production_openstack_image.img.bz2.sig\ngpg --verify flatcar_production_openstack_image.img.bz2.sig  The signing key is rotated annually. We will announce upcoming rotations of the signing key on the  user mailing list .",
            "title": "Importing images"
        },
        {
            "location": "/os/notes-for-distributors/#image-customization",
            "text": "There are two predominant ways that a Flatcar Container Linux image can be easily customized for a specific operating environment: through Ignition, a first-boot provisioning tool that runs during a machine's boot process, and through  cloud-config , an older tool that runs every time a machine boots.",
            "title": "Image customization"
        },
        {
            "location": "/os/notes-for-distributors/#ignition",
            "text": "Ignition  is a tool that acquires a JSON config file when a machine first boots, and uses this config to perform tasks such as formatting disks, creating files, modifying and creating users, and adding systemd units. How Ignition acquires this config file varies per-platform, and it is highly recommended that providers ensure Ignition has  support for their platform .  Use Ignition to handle platform specific configuration such as custom networking, running an agent on the machine, or injecting files onto disk. To do this, place an Ignition config at  /usr/share/oem/base/base.ign  and it will be prepended to the user provided config. In addition, any config placed at  /usr/share/oem/base/default.ign  will be executed if a user config is not found. On platforms that support cloud-config, use this feature to run coreos-cloudinit when no Ignition config is provided.  Additionally, it is recommended that providers ensure that  coreos-metadata  and  ct  have support for their platform. This will allow a nicer user experience, as coreos-metadata will be able to install users' ssh keys and users will be able to reference dynamic data in their Container Linux Configs.",
            "title": "Ignition"
        },
        {
            "location": "/os/notes-for-distributors/#cloud-config",
            "text": "A Flatcar Container Linux image can also be customized using  cloud-config . Users are recommended to instead use Container Linux Configs (that are converted into Ignition configs with  ct ), for reasons  outlined in the blog post that introduced Ignition .  Providers that previously supported cloud-config should continue to do so, as not all users have switched over to Container Linux Configs. New platforms do not need to support cloud-config.  Flatcar Container Linux will automatically parse and execute  /usr/share/oem/cloud-config.yml  if it exists.",
            "title": "Cloud config"
        },
        {
            "location": "/os/notes-for-distributors/#handling-end-user-ignition-files",
            "text": "End-users should be able to provide an Ignition file to your platform while specifying their VM's parameters. This file should be made available to Flatcar Container Linux at the time of boot (e.g. at known network address, injected directly onto disk). Examples of these data sources can be found in the  Ignition documentation .",
            "title": "Handling end-user Ignition files"
        },
        {
            "location": "/os/other-settings/",
            "text": "Tips and other settings\n\u00b6\n\n\nLoading kernel modules\n\u00b6\n\n\nMost Linux kernel modules get automatically loaded as-needed but there are a some situations where this doesn't work. Problems can arise if there is boot-time dependencies are sensitive to exactly when the module gets loaded. Module auto-loading can be broken all-together if the operation requiring the module happens inside of a container. \niptables\n and other netfilter features can easily encounter both of these issues. To force a module to be loaded early during boot simply list them in a file under \n/etc/modules-load.d\n. The file name must end in \n.conf\n.\n\n\necho nf_conntrack > /etc/modules-load.d/nf.conf\n\n\n\nOr, using a Container Linux Config:\n\n\nstorage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: nf_conntrack\n\n\n\nLoading kernel modules with options\n\u00b6\n\n\nThe following section demonstrates how to provide module options when loading. After these configs are processed, the dummy module is loaded into the kernel, and five dummy interfaces are added to the network stack.\n\n\nFurther details can be found in the systemd man pages:\n\nmodules-load.d(5)\n\n\nsystemd-modules-load.service(8)\n\n\nmodprobe.d(5)\n\n\nThis example Container Linux Config loads the \ndummy\n network interface module with an option specifying the number of interfaces the module should create when loaded (\nnumdummies=5\n):\n\n\nstorage:\n  files:\n    - path: /etc/modprobe.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: options dummy numdummies=5\n    - path: /etc/modules-load.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: dummy\n\n\n\nTuning sysctl parameters\n\u00b6\n\n\nThe Linux kernel offers a plethora of knobs under \n/proc/sys\n to control the availability of different features and tune performance parameters. For one-shot changes values can be written directly to the files under \n/proc/sys\n but persistent settings must be written to \n/etc/sysctl.d\n:\n\n\necho net.netfilter.nf_conntrack_max=131072 > /etc/sysctl.d/nf.conf\nsysctl --system\n\n\n\nSome parameters, such as the conntrack one above, are only available after the module they control has been loaded. To ensure any modules are loaded in advance use \nmodules-load.d\n as described above. A complete Container Linux Config using both would look like:\n\n\nstorage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          nf_conntrack\n    - path: /etc/sysctl.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.netfilter.nf_conntrack_max=131072\n\n\n\nFurther details can be found in the systemd man pages:\n\nsysctl.d(5)\n\n\nsystemd-sysctl.service(8)\n\n\nAdding custom kernel boot options\n\u00b6\n\n\nThe Flatcar Container Linux bootloader parses the configuration file \n/usr/share/oem/grub.cfg\n, where custom kernel boot options may be set.\n\n\nThe \n/usr/share/oem/grub.cfg\n file can be configured with Ignition. Note that Ignition runs after GRUB. Therefore, the GRUB configuration won't take effect until the next reboot of the node. \n\n\nHere's an example configuration:\n\n\nstorage:\n  filesystems:\n    - name: \"OEM\"\n      mount:\n        device: \"/dev/disk/by-label/OEM\"\n        format: \"ext4\"\n  files:\n    - filesystem: \"OEM\"\n      path: \"/grub.cfg\"\n      mode: 0644\n      append: true\n      contents:\n        inline: |\n          set linux_append=\"$linux_append flatcar.autologin=tty1\"\n\n\n\nEnable Flatcar Container Linux autologin\n\u00b6\n\n\nTo login without a password on every boot, edit \n/usr/share/oem/grub.cfg\n to add the line:\n\n\nset linux_append=\"$linux_append flatcar.autologin=tty1\"\n\n\n\nEnable systemd debug logging\n\u00b6\n\n\nEdit \n/usr/share/oem/grub.cfg\n to add the following line, enabling systemd's most verbose \ndebug\n-level logging:\n\n\nset linux_append=\"$linux_append systemd.log_level=debug\"\n\n\n\nMask a systemd unit\n\u00b6\n\n\nCompletely disable the \nsystemd-networkd.service\n unit by adding this line to \n/usr/share/oem/grub.cfg\n:\n\n\nset linux_append=\"$linux_append systemd.mask=systemd-networkd.service\"\n\n\n\nAdding custom messages to MOTD\n\u00b6\n\n\nWhen logging in interactively, a brief message (the \"Message of the Day (MOTD)\") reports the Flatcar Container Linux release channel, version, and a list of any services or systemd units that have failed. Additional text can be added by dropping text files into \n/etc/motd.d\n. The directory may need to be created first, and the drop-in file name must end in \n.conf\n. Flatcar Container Linux versions 555.0.0 and greater support customization of the MOTD.\n\n\nmkdir -p /etc/motd.d\necho \"This machine is dedicated to computing Pi\" > /etc/motd.d/pi.conf\n\n\n\nOr via a Container Linux Config:\n\n\nstorage:\n  files:\n    - path: /etc/motd.d/pi.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: This machine is dedicated to computing Pi\n\n\n\nPrevent login prompts from clearing the console\n\u00b6\n\n\nThe system boot messages that are printed to the console will be cleared when systemd starts a login prompt. In order to preserve these messages, the \ngetty\n services will need to have their \nTTYVTDisallocate\n setting disabled. This can be achieved with a drop-in for the template unit, \ngetty@.service\n. Note that the console will still scroll so the login prompt is at the top of the screen, but the boot messages will be available by scrolling.\n\n\nmkdir -p '/etc/systemd/system/getty@.service.d'\necho -e '[Service]\\nTTYVTDisallocate=no' > '/etc/systemd/system/getty@.service.d/no-disallocate.conf'\n\n\n\nOr via a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: getty@.service\n      dropins:\n        - name: no-disallocate.conf\n          contents: |\n            [Service]\n            TTYVTDisallocate=no\n\n\n\nWhen the \nTTYVTDisallocate\n setting is disabled, the console scrollback is not cleared on logout, not even by the \nclear\n command in the default \n.bash_logout\n file. Scrollback must be cleared explicitly, e.g. by running \necho -en '\\033[3J' > /dev/console\n as the root user.",
            "title": "Tips and other settings"
        },
        {
            "location": "/os/other-settings/#tips-and-other-settings",
            "text": "",
            "title": "Tips and other settings"
        },
        {
            "location": "/os/other-settings/#loading-kernel-modules",
            "text": "Most Linux kernel modules get automatically loaded as-needed but there are a some situations where this doesn't work. Problems can arise if there is boot-time dependencies are sensitive to exactly when the module gets loaded. Module auto-loading can be broken all-together if the operation requiring the module happens inside of a container.  iptables  and other netfilter features can easily encounter both of these issues. To force a module to be loaded early during boot simply list them in a file under  /etc/modules-load.d . The file name must end in  .conf .  echo nf_conntrack > /etc/modules-load.d/nf.conf  Or, using a Container Linux Config:  storage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: nf_conntrack",
            "title": "Loading kernel modules"
        },
        {
            "location": "/os/other-settings/#loading-kernel-modules-with-options",
            "text": "The following section demonstrates how to provide module options when loading. After these configs are processed, the dummy module is loaded into the kernel, and five dummy interfaces are added to the network stack.  Further details can be found in the systemd man pages: modules-load.d(5)  systemd-modules-load.service(8)  modprobe.d(5)  This example Container Linux Config loads the  dummy  network interface module with an option specifying the number of interfaces the module should create when loaded ( numdummies=5 ):  storage:\n  files:\n    - path: /etc/modprobe.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: options dummy numdummies=5\n    - path: /etc/modules-load.d/dummy.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: dummy",
            "title": "Loading kernel modules with options"
        },
        {
            "location": "/os/other-settings/#tuning-sysctl-parameters",
            "text": "The Linux kernel offers a plethora of knobs under  /proc/sys  to control the availability of different features and tune performance parameters. For one-shot changes values can be written directly to the files under  /proc/sys  but persistent settings must be written to  /etc/sysctl.d :  echo net.netfilter.nf_conntrack_max=131072 > /etc/sysctl.d/nf.conf\nsysctl --system  Some parameters, such as the conntrack one above, are only available after the module they control has been loaded. To ensure any modules are loaded in advance use  modules-load.d  as described above. A complete Container Linux Config using both would look like:  storage:\n  files:\n    - path: /etc/modules-load.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          nf_conntrack\n    - path: /etc/sysctl.d/nf.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          net.netfilter.nf_conntrack_max=131072  Further details can be found in the systemd man pages: sysctl.d(5)  systemd-sysctl.service(8)",
            "title": "Tuning sysctl parameters"
        },
        {
            "location": "/os/other-settings/#adding-custom-kernel-boot-options",
            "text": "The Flatcar Container Linux bootloader parses the configuration file  /usr/share/oem/grub.cfg , where custom kernel boot options may be set.  The  /usr/share/oem/grub.cfg  file can be configured with Ignition. Note that Ignition runs after GRUB. Therefore, the GRUB configuration won't take effect until the next reboot of the node.   Here's an example configuration:  storage:\n  filesystems:\n    - name: \"OEM\"\n      mount:\n        device: \"/dev/disk/by-label/OEM\"\n        format: \"ext4\"\n  files:\n    - filesystem: \"OEM\"\n      path: \"/grub.cfg\"\n      mode: 0644\n      append: true\n      contents:\n        inline: |\n          set linux_append=\"$linux_append flatcar.autologin=tty1\"",
            "title": "Adding custom kernel boot options"
        },
        {
            "location": "/os/other-settings/#enable-flatcar-container-linux-autologin",
            "text": "To login without a password on every boot, edit  /usr/share/oem/grub.cfg  to add the line:  set linux_append=\"$linux_append flatcar.autologin=tty1\"",
            "title": "Enable Flatcar Container Linux autologin"
        },
        {
            "location": "/os/other-settings/#enable-systemd-debug-logging",
            "text": "Edit  /usr/share/oem/grub.cfg  to add the following line, enabling systemd's most verbose  debug -level logging:  set linux_append=\"$linux_append systemd.log_level=debug\"",
            "title": "Enable systemd debug logging"
        },
        {
            "location": "/os/other-settings/#mask-a-systemd-unit",
            "text": "Completely disable the  systemd-networkd.service  unit by adding this line to  /usr/share/oem/grub.cfg :  set linux_append=\"$linux_append systemd.mask=systemd-networkd.service\"",
            "title": "Mask a systemd unit"
        },
        {
            "location": "/os/other-settings/#adding-custom-messages-to-motd",
            "text": "When logging in interactively, a brief message (the \"Message of the Day (MOTD)\") reports the Flatcar Container Linux release channel, version, and a list of any services or systemd units that have failed. Additional text can be added by dropping text files into  /etc/motd.d . The directory may need to be created first, and the drop-in file name must end in  .conf . Flatcar Container Linux versions 555.0.0 and greater support customization of the MOTD.  mkdir -p /etc/motd.d\necho \"This machine is dedicated to computing Pi\" > /etc/motd.d/pi.conf  Or via a Container Linux Config:  storage:\n  files:\n    - path: /etc/motd.d/pi.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: This machine is dedicated to computing Pi",
            "title": "Adding custom messages to MOTD"
        },
        {
            "location": "/os/other-settings/#prevent-login-prompts-from-clearing-the-console",
            "text": "The system boot messages that are printed to the console will be cleared when systemd starts a login prompt. In order to preserve these messages, the  getty  services will need to have their  TTYVTDisallocate  setting disabled. This can be achieved with a drop-in for the template unit,  getty@.service . Note that the console will still scroll so the login prompt is at the top of the screen, but the boot messages will be available by scrolling.  mkdir -p '/etc/systemd/system/getty@.service.d'\necho -e '[Service]\\nTTYVTDisallocate=no' > '/etc/systemd/system/getty@.service.d/no-disallocate.conf'  Or via a Container Linux Config:  systemd:\n  units:\n    - name: getty@.service\n      dropins:\n        - name: no-disallocate.conf\n          contents: |\n            [Service]\n            TTYVTDisallocate=no  When the  TTYVTDisallocate  setting is disabled, the console scrollback is not cleared on logout, not even by the  clear  command in the default  .bash_logout  file. Scrollback must be cleared explicitly, e.g. by running  echo -en '\\033[3J' > /dev/console  as the root user.",
            "title": "Prevent login prompts from clearing the console"
        },
        {
            "location": "/os/overview-of-systemctl/",
            "text": "Overview of systemctl\n\u00b6\n\n\nsystemctl\n is your interface to systemd, the init system used in Flatcar Container Linux. All processes on a single machine are started and managed by systemd, including your Docker containers. You can learn more in our \nGetting Started with systemd\n guide. Let's explore a few helpful \nsystemctl\n commands. You must run all of these commands locally on the Flatcar Container Linux machine:\n\n\nFind the status of a container\n\u00b6\n\n\nThe first step to troubleshooting with \nsystemctl\n is to find the status of the item in question. If you have multiple \nExec\n commands in your service file, you can see which one of them is failing and view the exit code. Here's a failing service that starts a private Docker registry in a container:\n\n\n$ sudo systemctl status custom-registry.service\n\ncustom-registry.service - Custom Registry Service\n   Loaded: loaded (/media/state/units/custom-registry.service; enabled-runtime)\n   Active: failed (Result: exit-code) since Sun 2013-12-22 12:40:11 UTC; 35s ago\n  Process: 10191 ExecStopPost=/usr/bin/etcdctl delete /registry (code=exited, status=0/SUCCESS)\n  Process: 10172 ExecStartPost=/usr/bin/etcdctl set /registry index.domain.com:5000 (code=exited, status=0/SUCCESS)\n  Process: 10171 ExecStart=/usr/bin/docker run -rm -p 5555:5000 54.202.26.87:5000/registry /bin/sh /root/boot.sh (code=exited, status=1/FAILURE)\n Main PID: 10171 (code=exited, status=1/FAILURE)\n   CGroup: /system.slice/custom-registry.service\n\nDec 22 12:40:01 localhost etcdctl[10172]: index.domain.com:5000\nDec 22 12:40:01 localhost systemd[1]: Started Custom Registry Service.\nDec 22 12:40:01 localhost docker[10171]: Unable to find image '54.202.26.87:5000/registry' (tag: latest) locally\nDec 22 12:40:11 localhost docker[10171]: 2013/12/22 12:40:11 Invalid Registry endpoint: Get http://index2.domain.com:5000/v1/_ping: dial tcp 54.204.26.2...o timeout\nDec 22 12:40:11 localhost systemd[1]: custom-registry.service: main process exited, code=exited, status=1/FAILURE\nDec 22 12:40:11 localhost etcdctl[10191]: index.domain.com:5000\nDec 22 12:40:11 localhost systemd[1]: Unit custom-registry.service entered failed state.\nHint: Some lines were ellipsized, use -l to show in full.\n\n\n\nYou can see that \nProcess: 10171 ExecStart=/usr/bin/docker\n exited with \nstatus=1/FAILURE\n and the log states that the index that we attempted to launch the container from, \n54.202.26.87\n wasn't valid, so the container image couldn't be downloaded.\n\n\nList status of all units\n\u00b6\n\n\nListing all of the processes running on the box is too much information, but you can pipe the output into grep to find the services you're looking for. Here's all service files and their status:\n\n\nsudo systemctl list-units | grep .service\n\n\n\nStart or stop a service\n\u00b6\n\n\nsudo systemctl start apache.service\n\n\n\nsudo systemctl stop apache.service\n\n\n\nKill a service\n\u00b6\n\n\nThis will stop the process immediately:\n\n\nsudo systemctl kill apache.service\n\n\n\nRestart a service\n\u00b6\n\n\nRestarting a service is as easy as:\n\n\nsudo systemctl restart apache.service\n\n\n\nIf you're restarting a service after you changed its service file, you will need to reload all of the service files before your changes take effect:\n\n\nsudo systemctl daemon-reload\n\n\n\nMore information\n\u00b6\n\n\nGetting Started with systemd\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs",
            "title": "Overview of systemctl"
        },
        {
            "location": "/os/overview-of-systemctl/#overview-of-systemctl",
            "text": "systemctl  is your interface to systemd, the init system used in Flatcar Container Linux. All processes on a single machine are started and managed by systemd, including your Docker containers. You can learn more in our  Getting Started with systemd  guide. Let's explore a few helpful  systemctl  commands. You must run all of these commands locally on the Flatcar Container Linux machine:",
            "title": "Overview of systemctl"
        },
        {
            "location": "/os/overview-of-systemctl/#find-the-status-of-a-container",
            "text": "The first step to troubleshooting with  systemctl  is to find the status of the item in question. If you have multiple  Exec  commands in your service file, you can see which one of them is failing and view the exit code. Here's a failing service that starts a private Docker registry in a container:  $ sudo systemctl status custom-registry.service\n\ncustom-registry.service - Custom Registry Service\n   Loaded: loaded (/media/state/units/custom-registry.service; enabled-runtime)\n   Active: failed (Result: exit-code) since Sun 2013-12-22 12:40:11 UTC; 35s ago\n  Process: 10191 ExecStopPost=/usr/bin/etcdctl delete /registry (code=exited, status=0/SUCCESS)\n  Process: 10172 ExecStartPost=/usr/bin/etcdctl set /registry index.domain.com:5000 (code=exited, status=0/SUCCESS)\n  Process: 10171 ExecStart=/usr/bin/docker run -rm -p 5555:5000 54.202.26.87:5000/registry /bin/sh /root/boot.sh (code=exited, status=1/FAILURE)\n Main PID: 10171 (code=exited, status=1/FAILURE)\n   CGroup: /system.slice/custom-registry.service\n\nDec 22 12:40:01 localhost etcdctl[10172]: index.domain.com:5000\nDec 22 12:40:01 localhost systemd[1]: Started Custom Registry Service.\nDec 22 12:40:01 localhost docker[10171]: Unable to find image '54.202.26.87:5000/registry' (tag: latest) locally\nDec 22 12:40:11 localhost docker[10171]: 2013/12/22 12:40:11 Invalid Registry endpoint: Get http://index2.domain.com:5000/v1/_ping: dial tcp 54.204.26.2...o timeout\nDec 22 12:40:11 localhost systemd[1]: custom-registry.service: main process exited, code=exited, status=1/FAILURE\nDec 22 12:40:11 localhost etcdctl[10191]: index.domain.com:5000\nDec 22 12:40:11 localhost systemd[1]: Unit custom-registry.service entered failed state.\nHint: Some lines were ellipsized, use -l to show in full.  You can see that  Process: 10171 ExecStart=/usr/bin/docker  exited with  status=1/FAILURE  and the log states that the index that we attempted to launch the container from,  54.202.26.87  wasn't valid, so the container image couldn't be downloaded.",
            "title": "Find the status of a container"
        },
        {
            "location": "/os/overview-of-systemctl/#list-status-of-all-units",
            "text": "Listing all of the processes running on the box is too much information, but you can pipe the output into grep to find the services you're looking for. Here's all service files and their status:  sudo systemctl list-units | grep .service",
            "title": "List status of all units"
        },
        {
            "location": "/os/overview-of-systemctl/#start-or-stop-a-service",
            "text": "sudo systemctl start apache.service  sudo systemctl stop apache.service",
            "title": "Start or stop a service"
        },
        {
            "location": "/os/overview-of-systemctl/#kill-a-service",
            "text": "This will stop the process immediately:  sudo systemctl kill apache.service",
            "title": "Kill a service"
        },
        {
            "location": "/os/overview-of-systemctl/#restart-a-service",
            "text": "Restarting a service is as easy as:  sudo systemctl restart apache.service  If you're restarting a service after you changed its service file, you will need to reload all of the service files before your changes take effect:  sudo systemctl daemon-reload",
            "title": "Restart a service"
        },
        {
            "location": "/os/overview-of-systemctl/#more-information",
            "text": "Getting Started with systemd  systemd.service Docs  systemd.unit Docs",
            "title": "More information"
        },
        {
            "location": "/os/power-management/",
            "text": "Tuning Flatcar Container Linux power management\n\u00b6\n\n\nCPU governor\n\u00b6\n\n\nBy default, Flatcar Container Linux uses the \"performance\" CPU governor meaning that the CPU operates at the maximum frequency regardless of load. This is reasonable for a system that is under constant load or cannot tolerate increased latency. On the other hand, if the system is idle much of the time and latency is not a concern, power savings may be desired.\n\n\nSeveral governors are available:\n\n\n\n\n\n\n\n\nGovernor\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nperformance\n\n\nDefault. Operate at the maximum frequency\n\n\n\n\n\n\nondemand\n\n\nDynamically scale frequency at 75% cpu load\n\n\n\n\n\n\nconservative\n\n\nDynamically scale frequency at 95% cpu load\n\n\n\n\n\n\npowersave\n\n\nOperate at the minimum frequency\n\n\n\n\n\n\nuserspace\n\n\nControlled by a userspace application via the \nscaling_setspeed\n file\n\n\n\n\n\n\n\n\nThe \"conservative\" governor can be used instead using the following shell commands:\n\n\nmodprobe cpufreq_conservative\necho \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor > /dev/null\n\n\n\nThis can be configured with a \nContainer Linux Config\n as well:\n\n\nsystemd:\n  units:\n    - name: cpu-governor.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable CPU power saving\n\n        [Service]\n        Type=oneshot\n        RemainAfterExit=yes\n        ExecStart=/usr/sbin/modprobe cpufreq_conservative\n        ExecStart=/usr/bin/sh -c '/usr/bin/echo \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor'\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nMore information on further tuning each governor is available in the \nKernel Documentation",
            "title": "Tuning Flatcar Container Linux power management"
        },
        {
            "location": "/os/power-management/#tuning-flatcar-container-linux-power-management",
            "text": "",
            "title": "Tuning Flatcar Container Linux power management"
        },
        {
            "location": "/os/power-management/#cpu-governor",
            "text": "By default, Flatcar Container Linux uses the \"performance\" CPU governor meaning that the CPU operates at the maximum frequency regardless of load. This is reasonable for a system that is under constant load or cannot tolerate increased latency. On the other hand, if the system is idle much of the time and latency is not a concern, power savings may be desired.  Several governors are available:     Governor  Description      performance  Default. Operate at the maximum frequency    ondemand  Dynamically scale frequency at 75% cpu load    conservative  Dynamically scale frequency at 95% cpu load    powersave  Operate at the minimum frequency    userspace  Controlled by a userspace application via the  scaling_setspeed  file     The \"conservative\" governor can be used instead using the following shell commands:  modprobe cpufreq_conservative\necho \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor > /dev/null  This can be configured with a  Container Linux Config  as well:  systemd:\n  units:\n    - name: cpu-governor.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Enable CPU power saving\n\n        [Service]\n        Type=oneshot\n        RemainAfterExit=yes\n        ExecStart=/usr/sbin/modprobe cpufreq_conservative\n        ExecStart=/usr/bin/sh -c '/usr/bin/echo \"conservative\" | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor'\n\n        [Install]\n        WantedBy=multi-user.target  More information on further tuning each governor is available in the  Kernel Documentation",
            "title": "CPU governor"
        },
        {
            "location": "/os/provisioning/",
            "text": "Provisioning\n\u00b6\n\n\nFlatcar Container Linux automates machine provisioning with a specialized system for applying initial configuration. This system implements a process of (trans)compilation and validation for machine configs, and an atomic service to apply validated configurations to machines.\n\n\nContainer Linux Config\n\u00b6\n\n\nFlatcar Container Linux admins define these configurations in a format called the \nContainer Linux Config\n, which was originally designed for CoreOS Container Linux, but works perfectly well with Flatcar Container Linux. Container Linux Configs are structured as YAML, and intended to be human-readable. The Container Linux Config has features devoted to configuring Flatcar Container Linux services such as \netcd\n, \nrkt\n, Docker, \nflannel\n, and \nlocksmith\n. \nThe defining feature of the config is that it cannot be sent directly to a Flatcar Container Linux provisioning target\n. Instead, it is first validated and transformed into a machine-readable and wire-efficient form.\n\n\nThe following examples demonstrate the simplicity of the Container Linux Config format.\n\n\nThis extremely simple Container Linux Config will fetch and run the current release of etcd:\n\n\netcd:\n\n\n\nExtend the definition to specify the version of etcd to run. The following example will provision a new Flatcar Container Linux machine to fetch and run the etcd service, version 3.1.6:\n\n\netcd:\n  version: 3.1.6\n\n\n\nUse variable replacement to configure the etcd service with the provisioning target's public and private IPv4 addresses, making it repeatable across a group of machines.\n\n\netcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>\n\n\n\nPUBLIC_IPV4\n and \nPRIVATE_IPV4\n are automatically populated from the environment in which Flatcar Container Linux runs, if this metadata exists. Given the many different environments in which Flatcar Container Linux can run, it's difficult if not impossible to accurately determine these variables in every instance. Be certain to check this value as a troubleshooting measure.\n\n\nFor example, the default metadata for an EC2 environment would be used: \npublic_ipv4\n and \nlocal_ipv4\n. On Azure, \neither\n the virtual IP or public IP could be used for the \nPUBLIC_IPV4\n (\nct\n makes a best guess and uses the virtual IP, but this could change in the future), and the dynamic IP would be used for the \nPRIVATE_IPV4\n. On bare metal, this information cannot be reliably derived in a general manner, so these variables cannot be used.\n\n\nBecause variable expansion is unpredictable and complex, and because it is also common for users to inadvertently write invalid configs, the use of a transformation tool is strongly encouraged. The default tool recommended for this task is the \nConfig Transpiler\n (ct for short). The Config Transpiler will validate and transform a Container Linux Config into the format that Flatcar Container Linux can consume: the Ignition Config.\n\n\nIgnition Config\n\u00b6\n\n\nIgnition, the utility in Flatcar Container Linux responsible for provisioning the machine, fetches and executes the Ignition Config. Flatcar Container Linux directly consumes the Ignition Config configuration format.\n\n\nIgnition Configs are mostly static, distro-agnostic, and meant to be generated by a machine rather than a human. While they can be written directly by users, it is highly discouraged due to the ease with which errors may be introduced. Rather than writing Ignition Configs directly, users are encouraged to use provisioning tools like \nMatchbox\n, which transparently translate Container Linux Configs to Ignition Configs, or to use the Config Transpiler itself.\n\n\n\n\nAs shown in this diagram, \nct\n is manually invoked only when users are manually provisioning machines. If a provisioning tool like Matchbox is used, \nct\n will transparently be incorporated into the deployment pipeline. In which case, the user only needs to prepare a Container Linux Config - Ignition and the Ignition Config are merely an implementation detail.\n\n\nConfig Transpiler\n\u00b6\n\n\nThe Container Linux Config Transpiler abstracts the details of configuring Flatcar Container Linux. It's responsible for transforming a Container Linux Config written by a user into an Ignition Config to be consumed by instances of Flatcar Container Linux.\n\n\nThe Container Linux Config Transpiler command line interface, \nct\n for short, can be downloaded from its \nGitHub Releases page\n.\n\n\nThe following config will configure an etcd cluster using the machine's public and private IP addresses:\n\n\netcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>\n\n\n\nAs suggested earlier, \nct\n requires information about the target environment before it can transform configs which use templating. If this config is passed to \nct\n without any other arguments, \nct\n fails with the following error message:\n\n\n$ ct < example.yml\nerror: platform must be specified to use templating\n\n\n\nThis message states that because the config takes advantage of templating (in this case,  \nPUBLIC_IPV4\n), \nct\n must be invoked with the \n--platform\n argument. This extra information is used by \nct\n to make the platform-specific customizations necessary. Keeping the Container Linux Config and the invocation arguments separate allows the Container Linux Config to remain largely platform independent.\n\n\nCT can be invoked again and given Amazon EC2 as an example:\n\n\n$ ct --platform=ec2 < example.yml\n{\"ignition\":{\"version\":\"2.0.0\",\"config\"...\n\n\n\nThis time, \nct\n successfully runs and produces the following Ignition Config:\n\n\n{\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"etcd-member.service\",\n      \"enable\": true,\n      \"dropins\": [{\n        \"name\": \"20-clct-etcd-member.conf\",\n        \"contents\": \"[Unit]\\nRequires=coreos-metadata.service\\nAfter=coreos-metadata.service\\n\\n[Service]\\nEnvironmentFile=/run/metadata/coreos\\nExecStart=\\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\\\\n  --listen-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --listen-client-urls=\\\"http://0.0.0.0:2379\\\" \\\\\\n  --initial-advertise-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --advertise-client-urls=\\\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\\\" \\\\\\n  --discovery=\\\"https://discovery.etcd.io/\\u003ctoken\\u003e\\\"\"\n      }]\n    }]\n  }\n}\n\n\n\nThis Ignition Config enables and configures etcd as specified in the above Container Linux Config. This can be more easily seen if the contents of the etcd drop-in are formatted nicely:\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\" \\\n  --discovery=\"https://discovery.etcd.io/<token>\"\n\n\n\nThe details of these changes are covered in depth in Ignition's \nmetadata documentation\n, but the gist is that \ncoreos-metadata\n is used to fetch the IP addresses from the Amazon APIs and then \nsystemd\n is leveraged to substitute the IP addresses into the invocation of etcd. The result is that even though Ignition only runs once, \ncoreos-metadata\n fetches the IP addresses whenever etcd is run, allowing etcd to use IP addresses that have the potential to change.\n\n\nMigrating from cloud configs\n\u00b6\n\n\nPreviously, the recommended way to provision a Flatcar Container Linux machine was with a cloud-config. These configs would be given to a Flatcar Container Linux machine and a utility called \ncoreos-cloudinit\n would read this file and apply the configuration on every boot.\n\n\nFor a \nnumber of reasons\n, coreos-cloudinit has been deprecated in favor of Container Linux Configs and Ignition. For help migrating from these legacy cloud-configs to Container Linux Configs, refer to the \nmigration guide\n.\n\n\nUsing Container Linux Configs\n\u00b6\n\n\nNow that the basics of Container Linux Configs have been covered, a good next step is to read through the \nexamples\n and start experimenting. The \ntroubleshooting guide\n is a good reference for debugging issues.",
            "title": "Provisioning"
        },
        {
            "location": "/os/provisioning/#provisioning",
            "text": "Flatcar Container Linux automates machine provisioning with a specialized system for applying initial configuration. This system implements a process of (trans)compilation and validation for machine configs, and an atomic service to apply validated configurations to machines.",
            "title": "Provisioning"
        },
        {
            "location": "/os/provisioning/#container-linux-config",
            "text": "Flatcar Container Linux admins define these configurations in a format called the  Container Linux Config , which was originally designed for CoreOS Container Linux, but works perfectly well with Flatcar Container Linux. Container Linux Configs are structured as YAML, and intended to be human-readable. The Container Linux Config has features devoted to configuring Flatcar Container Linux services such as  etcd ,  rkt , Docker,  flannel , and  locksmith .  The defining feature of the config is that it cannot be sent directly to a Flatcar Container Linux provisioning target . Instead, it is first validated and transformed into a machine-readable and wire-efficient form.  The following examples demonstrate the simplicity of the Container Linux Config format.  This extremely simple Container Linux Config will fetch and run the current release of etcd:  etcd:  Extend the definition to specify the version of etcd to run. The following example will provision a new Flatcar Container Linux machine to fetch and run the etcd service, version 3.1.6:  etcd:\n  version: 3.1.6  Use variable replacement to configure the etcd service with the provisioning target's public and private IPv4 addresses, making it repeatable across a group of machines.  etcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>  PUBLIC_IPV4  and  PRIVATE_IPV4  are automatically populated from the environment in which Flatcar Container Linux runs, if this metadata exists. Given the many different environments in which Flatcar Container Linux can run, it's difficult if not impossible to accurately determine these variables in every instance. Be certain to check this value as a troubleshooting measure.  For example, the default metadata for an EC2 environment would be used:  public_ipv4  and  local_ipv4 . On Azure,  either  the virtual IP or public IP could be used for the  PUBLIC_IPV4  ( ct  makes a best guess and uses the virtual IP, but this could change in the future), and the dynamic IP would be used for the  PRIVATE_IPV4 . On bare metal, this information cannot be reliably derived in a general manner, so these variables cannot be used.  Because variable expansion is unpredictable and complex, and because it is also common for users to inadvertently write invalid configs, the use of a transformation tool is strongly encouraged. The default tool recommended for this task is the  Config Transpiler  (ct for short). The Config Transpiler will validate and transform a Container Linux Config into the format that Flatcar Container Linux can consume: the Ignition Config.",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/provisioning/#ignition-config",
            "text": "Ignition, the utility in Flatcar Container Linux responsible for provisioning the machine, fetches and executes the Ignition Config. Flatcar Container Linux directly consumes the Ignition Config configuration format.  Ignition Configs are mostly static, distro-agnostic, and meant to be generated by a machine rather than a human. While they can be written directly by users, it is highly discouraged due to the ease with which errors may be introduced. Rather than writing Ignition Configs directly, users are encouraged to use provisioning tools like  Matchbox , which transparently translate Container Linux Configs to Ignition Configs, or to use the Config Transpiler itself.   As shown in this diagram,  ct  is manually invoked only when users are manually provisioning machines. If a provisioning tool like Matchbox is used,  ct  will transparently be incorporated into the deployment pipeline. In which case, the user only needs to prepare a Container Linux Config - Ignition and the Ignition Config are merely an implementation detail.",
            "title": "Ignition Config"
        },
        {
            "location": "/os/provisioning/#config-transpiler",
            "text": "The Container Linux Config Transpiler abstracts the details of configuring Flatcar Container Linux. It's responsible for transforming a Container Linux Config written by a user into an Ignition Config to be consumed by instances of Flatcar Container Linux.  The Container Linux Config Transpiler command line interface,  ct  for short, can be downloaded from its  GitHub Releases page .  The following config will configure an etcd cluster using the machine's public and private IP addresses:  etcd:\n  advertise_client_urls:       http://{PUBLIC_IPV4}:2379\n  initial_advertise_peer_urls: http://{PRIVATE_IPV4}:2380\n  listen_client_urls:          http://0.0.0.0:2379\n  listen_peer_urls:            http://{PRIVATE_IPV4}:2380\n  discovery:                   https://discovery.etcd.io/<token>  As suggested earlier,  ct  requires information about the target environment before it can transform configs which use templating. If this config is passed to  ct  without any other arguments,  ct  fails with the following error message:  $ ct < example.yml\nerror: platform must be specified to use templating  This message states that because the config takes advantage of templating (in this case,   PUBLIC_IPV4 ),  ct  must be invoked with the  --platform  argument. This extra information is used by  ct  to make the platform-specific customizations necessary. Keeping the Container Linux Config and the invocation arguments separate allows the Container Linux Config to remain largely platform independent.  CT can be invoked again and given Amazon EC2 as an example:  $ ct --platform=ec2 < example.yml\n{\"ignition\":{\"version\":\"2.0.0\",\"config\"...  This time,  ct  successfully runs and produces the following Ignition Config:  {\n  \"ignition\": { \"version\": \"2.0.0\" },\n  \"systemd\": {\n    \"units\": [{\n      \"name\": \"etcd-member.service\",\n      \"enable\": true,\n      \"dropins\": [{\n        \"name\": \"20-clct-etcd-member.conf\",\n        \"contents\": \"[Unit]\\nRequires=coreos-metadata.service\\nAfter=coreos-metadata.service\\n\\n[Service]\\nEnvironmentFile=/run/metadata/coreos\\nExecStart=\\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\\\\n  --listen-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --listen-client-urls=\\\"http://0.0.0.0:2379\\\" \\\\\\n  --initial-advertise-peer-urls=\\\"http://${COREOS_EC2_IPV4_LOCAL}:2380\\\" \\\\\\n  --advertise-client-urls=\\\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\\\" \\\\\\n  --discovery=\\\"https://discovery.etcd.io/\\u003ctoken\\u003e\\\"\"\n      }]\n    }]\n  }\n}  This Ignition Config enables and configures etcd as specified in the above Container Linux Config. This can be more easily seen if the contents of the etcd drop-in are formatted nicely:  [Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/lib/coreos/etcd-wrapper $ETCD_OPTS \\\n  --listen-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --listen-client-urls=\"http://0.0.0.0:2379\" \\\n  --initial-advertise-peer-urls=\"http://${COREOS_EC2_IPV4_LOCAL}:2380\" \\\n  --advertise-client-urls=\"http://${COREOS_EC2_IPV4_PUBLIC}:2379\" \\\n  --discovery=\"https://discovery.etcd.io/<token>\"  The details of these changes are covered in depth in Ignition's  metadata documentation , but the gist is that  coreos-metadata  is used to fetch the IP addresses from the Amazon APIs and then  systemd  is leveraged to substitute the IP addresses into the invocation of etcd. The result is that even though Ignition only runs once,  coreos-metadata  fetches the IP addresses whenever etcd is run, allowing etcd to use IP addresses that have the potential to change.",
            "title": "Config Transpiler"
        },
        {
            "location": "/os/provisioning/#migrating-from-cloud-configs",
            "text": "Previously, the recommended way to provision a Flatcar Container Linux machine was with a cloud-config. These configs would be given to a Flatcar Container Linux machine and a utility called  coreos-cloudinit  would read this file and apply the configuration on every boot.  For a  number of reasons , coreos-cloudinit has been deprecated in favor of Container Linux Configs and Ignition. For help migrating from these legacy cloud-configs to Container Linux Configs, refer to the  migration guide .",
            "title": "Migrating from cloud configs"
        },
        {
            "location": "/os/provisioning/#using-container-linux-configs",
            "text": "Now that the basics of Container Linux Configs have been covered, a good next step is to read through the  examples  and start experimenting. The  troubleshooting guide  is a good reference for debugging issues.",
            "title": "Using Container Linux Configs"
        },
        {
            "location": "/os/quickstart/",
            "text": "Flatcar Container Linux quick start\n\u00b6\n\n\nIf you don't have a Flatcar Container Linux machine running, check out the guides on \nrunning Flatcar Container Linux\n on most cloud providers (\nEC2\n, \nAzure\n, \nGCE\n), virtualization platforms (\nVagrant\n, \nVMware\n, \nQEMU/KVM\n) and bare metal servers (\nPXE\n, \niPXE\n, \nISO\n, \nInstaller\n). With any of these guides you will have machines up and running in a few minutes.\n\n\nIt's highly recommended that you set up a cluster of at least 3 machines \u2014 it's not as much fun on a single machine. If you don't want to break the bank, \nVagrant\n allows you to run an entire cluster on your laptop. For a cluster to be properly bootstrapped, you have to provide ideally an \nIgnition config\n (generated from a \nContainer Linux Config\n), or possibly a cloud-config, via user-data, which is covered in each platform's guide.\n\n\nFlatcar Container Linux gives you three essential tools: service discovery, container management and process management. Let's try each of them out.\n\n\nFirst, on the client start your user agent by typing:\n\n\neval $(ssh-agent)\n\n\n\nThen, add your private key to the agent by typing:\n\n\nssh-add\n\n\n\nConnect to a Flatcar Container Linux machine via SSH as the user \ncore\n. For example, on Amazon, use:\n\n\n$ ssh core@an.ip.compute-1.amazonaws.com\nFlatcar Container Linux (beta)\n\n\n\nIf you're using Vagrant, you'll need to connect a bit differently:\n\n\n$ ssh-add ~/.vagrant.d/insecure_private_key\nIdentity added: /Users/core/.vagrant.d/insecure_private_key (/Users/core/.vagrant.d/insecure_private_key)\n$ vagrant ssh core-01\nFlatcar Container Linux (beta)\n\n\n\nService discovery with etcd\n\u00b6\n\n\nThe first building block of Flatcar Container Linux is service discovery with \netcd\n (\ndocs\n). Data stored in etcd is distributed across all of your machines running Flatcar Container Linux. For example, each of your app containers can announce itself to a proxy container, which would automatically know which machines should receive traffic. Building service discovery into your application allows you to add more machines and scale your services seamlessly.\n\n\nIf you used an example \nContainer Linux Config\n or \ncloud-config\n from a guide linked in the first paragraph, etcd is automatically started on boot.\n\n\nA good starting point for a Container Linux Config would be something like:\n\n\netcd:\n  discovery: https://discovery.etcd.io/<token>\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAA...\n\n\n\nIn order to get the discovery token, visit \nhttps://discovery.etcd.io/new\n and you will receive a URL including your token. Paste the whole thing into your Container Linux Config file.\n\n\netcdctl\n is a command line interface to etcd that is preinstalled on Flatcar Container Linux. To set and retrieve a key from etcd you can use the following examples:\n\n\nSet a key \nmessage\n with value \nHello world\n:\n\n\netcdctl set /message \"Hello world\"\n\n\n\nRead the value of \nmessage\n back:\n\n\netcdctl get /message\n\n\n\nYou can also use simple \ncurl\n. These examples correspond to previous ones:\n\n\nSet the value:\n\n\ncurl -L http://127.0.0.1:2379/v2/keys/message -XPUT -d value=\"Hello world\"\n\n\n\nRead the value:\n\n\ncurl -L http://127.0.0.1:2379/v2/keys/message\n\n\n\nIf you followed a guide to set up more than one Flatcar Container Linux machine, you can SSH into another machine and can retrieve this same value.\n\n\nMore detailed information\n\u00b6\n\n\nView Complete Guide\n\n\nRead etcd API Docs\n\n\nContainer management with Docker\n\u00b6\n\n\nThe second building block, \nDocker\n (\ndocs\n), is where your applications and code run. It is installed on each Flatcar Container Linux machine. You should make each of your services (web server, caching, database) into a container and connect them together by reading and writing to etcd. You can quickly try out a minimal busybox container in two different ways:\n\n\nRun a command in the container and then stop it:\n\n\ndocker run busybox /bin/echo hello world\n\n\n\nOpen a shell prompt inside the container:\n\n\ndocker run -i -t busybox /bin/sh\n\n\n\nMore detailed information\n\u00b6\n\n\nView Complete Guide\n\n\nRead Docker Docs",
            "title": "Flatcar Container Linux quick start"
        },
        {
            "location": "/os/quickstart/#flatcar-container-linux-quick-start",
            "text": "If you don't have a Flatcar Container Linux machine running, check out the guides on  running Flatcar Container Linux  on most cloud providers ( EC2 ,  Azure ,  GCE ), virtualization platforms ( Vagrant ,  VMware ,  QEMU/KVM ) and bare metal servers ( PXE ,  iPXE ,  ISO ,  Installer ). With any of these guides you will have machines up and running in a few minutes.  It's highly recommended that you set up a cluster of at least 3 machines \u2014 it's not as much fun on a single machine. If you don't want to break the bank,  Vagrant  allows you to run an entire cluster on your laptop. For a cluster to be properly bootstrapped, you have to provide ideally an  Ignition config  (generated from a  Container Linux Config ), or possibly a cloud-config, via user-data, which is covered in each platform's guide.  Flatcar Container Linux gives you three essential tools: service discovery, container management and process management. Let's try each of them out.  First, on the client start your user agent by typing:  eval $(ssh-agent)  Then, add your private key to the agent by typing:  ssh-add  Connect to a Flatcar Container Linux machine via SSH as the user  core . For example, on Amazon, use:  $ ssh core@an.ip.compute-1.amazonaws.com\nFlatcar Container Linux (beta)  If you're using Vagrant, you'll need to connect a bit differently:  $ ssh-add ~/.vagrant.d/insecure_private_key\nIdentity added: /Users/core/.vagrant.d/insecure_private_key (/Users/core/.vagrant.d/insecure_private_key)\n$ vagrant ssh core-01\nFlatcar Container Linux (beta)",
            "title": "Flatcar Container Linux quick start"
        },
        {
            "location": "/os/quickstart/#service-discovery-with-etcd",
            "text": "The first building block of Flatcar Container Linux is service discovery with  etcd  ( docs ). Data stored in etcd is distributed across all of your machines running Flatcar Container Linux. For example, each of your app containers can announce itself to a proxy container, which would automatically know which machines should receive traffic. Building service discovery into your application allows you to add more machines and scale your services seamlessly.  If you used an example  Container Linux Config  or  cloud-config  from a guide linked in the first paragraph, etcd is automatically started on boot.  A good starting point for a Container Linux Config would be something like:  etcd:\n  discovery: https://discovery.etcd.io/<token>\npasswd:\n  users:\n    - name: core\n      ssh_authorized_keys:\n        - ssh-rsa AAAA...  In order to get the discovery token, visit  https://discovery.etcd.io/new  and you will receive a URL including your token. Paste the whole thing into your Container Linux Config file.  etcdctl  is a command line interface to etcd that is preinstalled on Flatcar Container Linux. To set and retrieve a key from etcd you can use the following examples:  Set a key  message  with value  Hello world :  etcdctl set /message \"Hello world\"  Read the value of  message  back:  etcdctl get /message  You can also use simple  curl . These examples correspond to previous ones:  Set the value:  curl -L http://127.0.0.1:2379/v2/keys/message -XPUT -d value=\"Hello world\"  Read the value:  curl -L http://127.0.0.1:2379/v2/keys/message  If you followed a guide to set up more than one Flatcar Container Linux machine, you can SSH into another machine and can retrieve this same value.",
            "title": "Service discovery with etcd"
        },
        {
            "location": "/os/quickstart/#more-detailed-information",
            "text": "View Complete Guide  Read etcd API Docs",
            "title": "More detailed information"
        },
        {
            "location": "/os/quickstart/#container-management-with-docker",
            "text": "The second building block,  Docker  ( docs ), is where your applications and code run. It is installed on each Flatcar Container Linux machine. You should make each of your services (web server, caching, database) into a container and connect them together by reading and writing to etcd. You can quickly try out a minimal busybox container in two different ways:  Run a command in the container and then stop it:  docker run busybox /bin/echo hello world  Open a shell prompt inside the container:  docker run -i -t busybox /bin/sh",
            "title": "Container management with Docker"
        },
        {
            "location": "/os/quickstart/#more-detailed-information_1",
            "text": "View Complete Guide  Read Docker Docs",
            "title": "More detailed information"
        },
        {
            "location": "/os/reading-the-system-log/",
            "text": "Reading the system log\n\u00b6\n\n\njournalctl\n is your interface into a single machine's journal/logging. All service files insert data into the systemd journal. There are a few helpful commands to read the journal:\n\n\nRead the entire journal\n\u00b6\n\n\n$ journalctl\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:28:45 UTC. --\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 184.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 188.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuset\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpu\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuacct\nDec 22 00:10:21 localhost kernel: Linux version 3.11.7+ (buildbot@10.10.10.10) (gcc version 4.6.3 (Gentoo Hardened 4.6.3 p1.13, pie-0.5.2)\n...\n1000s more lines\n\n\n\nRead entries for a specific service\n\u00b6\n\n\nRead entries generated by a specific unit:\n\n\n$ journalctl -u apache.service\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:32:52 UTC. --\nDec 22 12:32:39 localhost systemd[1]: Starting Apache Service...\nDec 22 12:32:39 localhost systemd[1]: Started Apache Service.\nDec 22 12:32:39 localhost docker[9772]: /usr/sbin/apache2ctl: 87: ulimit: error setting limit (Operation not permitted)\nDec 22 12:32:39 localhost docker[9772]: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.6 for ServerName\n\n\n\nRead entries since boot\n\u00b6\n\n\nReading just the entries since the last boot is an easy way to troubleshoot services that are failing to start properly:\n\n\njournalctl --boot\n\n\n\nTail the journal\n\u00b6\n\n\nYou can tail the entire journal or just a specific service:\n\n\njournalctl -f\n\n\n\njournalctl -u apache.service -f\n\n\n\nRead entries with line wrapping\n\u00b6\n\n\nBy default \njournalctl\n passes \nFRSXMK\n command line options to \nless\n. You can override these options by setting a custom \nSYSTEMD_LESS\n environment variable with omitted \nS\n option:\n\n\nSYSTEMD_LESS=FRXMK journalctl\n\n\n\nRead logs without pager:\n\n\njournalctl --no-pager\n\n\n\nDebugging journald\n\u00b6\n\n\nIf you've faced some problems with journald you can enable debug mode following the instructions below.\n\n\nEnable debugging manually\n\u00b6\n\n\nmkdir -p /etc/systemd/system/systemd-journald.service.d/\n\n\n\nCreate \nDrop-In\n \n/etc/systemd/system/systemd-journald.service.d/10-debug.conf\n with following content:\n\n\n[Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nAnd restart \nsystemd-journald\n service:\n\n\nsystemctl daemon-reload\nsystemctl restart systemd-journald\ndmesg | grep systemd-journald\n\n\n\nEnable debugging via a Container Linux Config\n\u00b6\n\n\nDefine a \nDrop-In\n in a \nContainer Linux Config\n:\n\n\nsystemd:\n  units:\n    - name: systemd-journald.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug\n\n\n\nMore information\n\u00b6\n\n\nGetting Started with systemd\n\n\nNetwork Configuration with networkd",
            "title": "Reading the system log"
        },
        {
            "location": "/os/reading-the-system-log/#reading-the-system-log",
            "text": "journalctl  is your interface into a single machine's journal/logging. All service files insert data into the systemd journal. There are a few helpful commands to read the journal:",
            "title": "Reading the system log"
        },
        {
            "location": "/os/reading-the-system-log/#read-the-entire-journal",
            "text": "$ journalctl\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:28:45 UTC. --\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 184.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost systemd-journal[33]: Runtime journal is using 188.0K (max 49.9M, leaving 74.8M of free 499.0M, current limit 49.9M).\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuset\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpu\nDec 22 00:10:21 localhost kernel: Initializing cgroup subsys cpuacct\nDec 22 00:10:21 localhost kernel: Linux version 3.11.7+ (buildbot@10.10.10.10) (gcc version 4.6.3 (Gentoo Hardened 4.6.3 p1.13, pie-0.5.2)\n...\n1000s more lines",
            "title": "Read the entire journal"
        },
        {
            "location": "/os/reading-the-system-log/#read-entries-for-a-specific-service",
            "text": "Read entries generated by a specific unit:  $ journalctl -u apache.service\n\n-- Logs begin at Fri 2013-12-13 23:43:32 UTC, end at Sun 2013-12-22 12:32:52 UTC. --\nDec 22 12:32:39 localhost systemd[1]: Starting Apache Service...\nDec 22 12:32:39 localhost systemd[1]: Started Apache Service.\nDec 22 12:32:39 localhost docker[9772]: /usr/sbin/apache2ctl: 87: ulimit: error setting limit (Operation not permitted)\nDec 22 12:32:39 localhost docker[9772]: apache2: Could not reliably determine the server's fully qualified domain name, using 172.17.0.6 for ServerName",
            "title": "Read entries for a specific service"
        },
        {
            "location": "/os/reading-the-system-log/#read-entries-since-boot",
            "text": "Reading just the entries since the last boot is an easy way to troubleshoot services that are failing to start properly:  journalctl --boot",
            "title": "Read entries since boot"
        },
        {
            "location": "/os/reading-the-system-log/#tail-the-journal",
            "text": "You can tail the entire journal or just a specific service:  journalctl -f  journalctl -u apache.service -f",
            "title": "Tail the journal"
        },
        {
            "location": "/os/reading-the-system-log/#read-entries-with-line-wrapping",
            "text": "By default  journalctl  passes  FRSXMK  command line options to  less . You can override these options by setting a custom  SYSTEMD_LESS  environment variable with omitted  S  option:  SYSTEMD_LESS=FRXMK journalctl  Read logs without pager:  journalctl --no-pager",
            "title": "Read entries with line wrapping"
        },
        {
            "location": "/os/reading-the-system-log/#debugging-journald",
            "text": "If you've faced some problems with journald you can enable debug mode following the instructions below.",
            "title": "Debugging journald"
        },
        {
            "location": "/os/reading-the-system-log/#enable-debugging-manually",
            "text": "mkdir -p /etc/systemd/system/systemd-journald.service.d/  Create  Drop-In   /etc/systemd/system/systemd-journald.service.d/10-debug.conf  with following content:  [Service]\nEnvironment=SYSTEMD_LOG_LEVEL=debug  And restart  systemd-journald  service:  systemctl daemon-reload\nsystemctl restart systemd-journald\ndmesg | grep systemd-journald",
            "title": "Enable debugging manually"
        },
        {
            "location": "/os/reading-the-system-log/#enable-debugging-via-a-container-linux-config",
            "text": "Define a  Drop-In  in a  Container Linux Config :  systemd:\n  units:\n    - name: systemd-journald.service\n      dropins:\n        - name: 10-debug.conf\n          contents: |\n            [Service]\n            Environment=SYSTEMD_LOG_LEVEL=debug",
            "title": "Enable debugging via a Container Linux Config"
        },
        {
            "location": "/os/reading-the-system-log/#more-information",
            "text": "Getting Started with systemd  Network Configuration with networkd",
            "title": "More information"
        },
        {
            "location": "/os/registry-authentication/",
            "text": "Using authentication for a registry\n\u00b6\n\n\nMany container image registries require authentication. This document explains how to configure container management software like Docker, Kubernetes, rkt, and Mesos to authenticate with and pull containers from registries like \nQuay\n and \nDocker Hub\n.\n\n\nUsing a Quay robot for registry auth\n\u00b6\n\n\nThe recommended way to authenticate container manager software with \nquay.io\n is via a \nQuay Robot\n. The robot account acts as an authentication token with some nice features, including:\n\n\n\n\nReadymade repository authentication configuration files\n\n\nCredentials are limited to specific repositories\n\n\nChoose from read, write, or admin privileges\n\n\nToken regeneration\n\n\n\n\n\n\nQuay robots provide config files for Kubernetes, Docker, Mesos, and rkt, along with instructions for using each. Find this information in the \nRobot Accounts\n tab under your Quay user settings. For more information, see the \nQuay robot documentation\n.\n\n\nManual registry auth setup\n\u00b6\n\n\nIf you are using a registry other than Quay (e.g., Docker Hub, Docker Store, etc) you will need to manually configure your credentials with your container-runtime or orchestration tool.\n\n\nDocker\n\u00b6\n\n\nThe Docker client uses an interactive command to authenticate with a centralized service.\n\n\n$ docker login -u <username> -p <password> https://registry.example.io\n\n\n\nThis command creates the file \n$HOME/.docker/config.json\n, formatted like the following example:\n\n\n/home/core/.docker/config.json:\n\n\n{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        },\n        \"quay.io\": {\n            \"xxxx\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n        },\n        \"https://registry.example.io/v0/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        }\n    }\n}\n\n\n\nOn Flatcar Container Linux, this process can be automated by writing out the config file during system provisioning \nwith a Container Linux Config\n. Since the config is written to the \ncore\n user's home directory, ensure that your systemd units run as that user, by adding, e.g., \nUser=core\n.\n\n\nDocker also offers the ability to configure a credentials store, such as your operating system's keychain. This is outlined  in the \nDocker login documentation\n.\n\n\nKubernetes\n\u00b6\n\n\nKubernetes uses \nSecrets\n to store registry credentials.\n\n\nWhen manually configuring authentication with \nany\n registry in Kubernetes (including Quay and Docker Hub) the following command is used to generate the Kubernetes registry-auth secret:\n\n\n$ kubectl create secret docker-registry my-favorite-registry-secret --docker-username=giffee_lover_93 --docker-password='passphrases are great!' --docker-email='giffee.lover.93@example.com' --docker-server=registry.example.io\nsecret \"my-favorite-registry-secret\" created\n\n\n\nIf you prefer you can store this in a YAML file by adding the \n--dry-run\n and \n-o yaml\n flag to the end of your command and copying or redirecting the output to a file:\n\n\n$ kubectl create secret docker-registry my-favorite-registry [...] --dry-run -o yaml | tee credentials.yaml\n\n\n\napiVersion: v1\ndata:\n  .dockercfg: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx==\nkind: Secret\nmetadata:\n  creationTimestamp: null \n  name: my-favorite-registry-secret\ntype: kubernetes.io/dockercfg\n\n\n\n$ kubectl create -f credentials.yaml\nsecret \"my-favorite-registry-secret\" created\n\n\n\nYou can check that this secret is loaded with with the \nkubectl get\n command:\n\n\n$ kubectl get my-favorite-registry-secret\nNAME                            TYPE                      DATA      AGE\nmy-favorite-registry-secret     kubernetes.io/dockercfg   1         30m\n\n\n\nThe secret can be used in a Pod spec with the \nimagePullSecrets\n variable:\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: somepod\n  namespace: all\nspec:\n  containers:\n    - name: web\n      image: registry.example.io/v0/giffee_lover_93/somerepo\n\n  imagePullSecrets:\n    - name: my-favorite-registry-secret\n\n\n\nFor more information, check the \ndocker-registry Kubernetes secret\n and \nKubernetes imagePullSecrets\n documentation.\n\n\nrkt\n\u00b6\n\n\nrkt stores registry-authentication in a JSON file stored in the directory \n/etc/rkt/auth.d/\n. \n\n\n/etc/rkt/auth.d/registry.example.io.json\n\n\n{\n  \"rktKind\": \"auth\",\n  \"rktVersion\": \"v1\",\n  \"domains\": [\n    \"https://registry.example.io/v0/\"\n  ],\n  \"type\": \"basic\",\n  \"credentials\": {\n    \"user\": \"giffeeLover93\",\n    \"password\": \"passphrases are great!\"\n  }\n}\n\n\n\nWhile you \ncan\n embed your password in plaintext in this file, you should try using a disposable token instead. Check your registry documentation to see if it offers token-based authentication.\n\n\nNow rkt will authenticate with \nhttps://registry.example.io/v0/\n using the provided credentials to fetch images.\n\n\nFor more information about rkt credentials, see the \nrkt configuration docs\n.\n\n\nJust like with the Docker config, this file can be copied to \n/etc/rkt/auth.d/registry.example.io.json\n on a Flatcar Container Linux node during system provisioning with \na Container Linux Config\n.\n\n\nMesos\n\u00b6\n\n\nMesos uses a gzip-compressed archive of a \n.docker/config.json\n (directory and file) to access private repositories.\n\n\nOnce you have followed the above steps to \ncreate the docker registry auth config file\n create your Mesos configuration using \ntar\n:\n\n\n$ tar cxf ~/.docker/config.json\n\n\n\nThe archive secret is referenced via the \nuris\n field in a container specification file:\n\n\n{\n  \"id\": \"/some/name/or/id\",\n  \"cpus\": 1,\n  \"mem\": 1024,\n  \"instances\": 1,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"https://registry.example.io/v0/giffee_lover_93/some-image\",\n      \"network\": \"HOST\"\n    }\n  },\n\n  \"uris\":  [\n      \"file:///path/to/registry.example.io.tar.gz\"\n  ]\n}\n\n\n\nMore thorough information about configuring Mesos registry authentication can be found on the \n'Using a Private Docker Registry'\n documentation.\n\n\nCopying the config file with a Container Linux Config\n\u00b6\n\n\nContainer Linux Configs\n can be used to provision a Flatcar Container Linux node on first boot. Here we will use it to copy registry authentication config files to their appropriate destination on disk. This provides immediate access to your private Docker Hub and Quay image repositories without the need for manual intervention. The same Container Linux Config file can be used to copy registry auth configs onto an entire cluster of Flatcar Container Linux nodes.\n\n\nHere is an example of using a Container Linux Config to write the .docker/config.json registry auth configuration file mentioned above to the appropriate path on the Flatcar Container Linux node:\n\n\nstorage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          {\n            \"auths\": {\n              \"quay.io\": {\n                \"auth\": \"AbCdEfGhIj\",\n                \"email\": \"your.email@example.com\"\n              }\n            }\n          }\n\n\n\nContainer Linux Configs can also download a file from a remote location and verify its integrity with a SHA512 hash:\n\n\nstorage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        remote:\n          url: http://internal.infra.example.com/cluster-docker-config.json\n          verification:\n            hash:\n              function: sha512\n              sum: 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef\n\n\n\nFor details, check out the \nContainer Linux Config examples\n.",
            "title": "Using authentication for a registry"
        },
        {
            "location": "/os/registry-authentication/#using-authentication-for-a-registry",
            "text": "Many container image registries require authentication. This document explains how to configure container management software like Docker, Kubernetes, rkt, and Mesos to authenticate with and pull containers from registries like  Quay  and  Docker Hub .",
            "title": "Using authentication for a registry"
        },
        {
            "location": "/os/registry-authentication/#using-a-quay-robot-for-registry-auth",
            "text": "The recommended way to authenticate container manager software with  quay.io  is via a  Quay Robot . The robot account acts as an authentication token with some nice features, including:   Readymade repository authentication configuration files  Credentials are limited to specific repositories  Choose from read, write, or admin privileges  Token regeneration    Quay robots provide config files for Kubernetes, Docker, Mesos, and rkt, along with instructions for using each. Find this information in the  Robot Accounts  tab under your Quay user settings. For more information, see the  Quay robot documentation .",
            "title": "Using a Quay robot for registry auth"
        },
        {
            "location": "/os/registry-authentication/#manual-registry-auth-setup",
            "text": "If you are using a registry other than Quay (e.g., Docker Hub, Docker Store, etc) you will need to manually configure your credentials with your container-runtime or orchestration tool.",
            "title": "Manual registry auth setup"
        },
        {
            "location": "/os/registry-authentication/#docker",
            "text": "The Docker client uses an interactive command to authenticate with a centralized service.  $ docker login -u <username> -p <password> https://registry.example.io  This command creates the file  $HOME/.docker/config.json , formatted like the following example:  /home/core/.docker/config.json:  {\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        },\n        \"quay.io\": {\n            \"xxxx\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n        },\n        \"https://registry.example.io/v0/\": {\n            \"auth\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=\"\n        }\n    }\n}  On Flatcar Container Linux, this process can be automated by writing out the config file during system provisioning  with a Container Linux Config . Since the config is written to the  core  user's home directory, ensure that your systemd units run as that user, by adding, e.g.,  User=core .  Docker also offers the ability to configure a credentials store, such as your operating system's keychain. This is outlined  in the  Docker login documentation .",
            "title": "Docker"
        },
        {
            "location": "/os/registry-authentication/#kubernetes",
            "text": "Kubernetes uses  Secrets  to store registry credentials.  When manually configuring authentication with  any  registry in Kubernetes (including Quay and Docker Hub) the following command is used to generate the Kubernetes registry-auth secret:  $ kubectl create secret docker-registry my-favorite-registry-secret --docker-username=giffee_lover_93 --docker-password='passphrases are great!' --docker-email='giffee.lover.93@example.com' --docker-server=registry.example.io\nsecret \"my-favorite-registry-secret\" created  If you prefer you can store this in a YAML file by adding the  --dry-run  and  -o yaml  flag to the end of your command and copying or redirecting the output to a file:  $ kubectl create secret docker-registry my-favorite-registry [...] --dry-run -o yaml | tee credentials.yaml  apiVersion: v1\ndata:\n  .dockercfg: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx==\nkind: Secret\nmetadata:\n  creationTimestamp: null \n  name: my-favorite-registry-secret\ntype: kubernetes.io/dockercfg  $ kubectl create -f credentials.yaml\nsecret \"my-favorite-registry-secret\" created  You can check that this secret is loaded with with the  kubectl get  command:  $ kubectl get my-favorite-registry-secret\nNAME                            TYPE                      DATA      AGE\nmy-favorite-registry-secret     kubernetes.io/dockercfg   1         30m  The secret can be used in a Pod spec with the  imagePullSecrets  variable:  apiVersion: v1\nkind: Pod\nmetadata:\n  name: somepod\n  namespace: all\nspec:\n  containers:\n    - name: web\n      image: registry.example.io/v0/giffee_lover_93/somerepo\n\n  imagePullSecrets:\n    - name: my-favorite-registry-secret  For more information, check the  docker-registry Kubernetes secret  and  Kubernetes imagePullSecrets  documentation.",
            "title": "Kubernetes"
        },
        {
            "location": "/os/registry-authentication/#rkt",
            "text": "rkt stores registry-authentication in a JSON file stored in the directory  /etc/rkt/auth.d/ .   /etc/rkt/auth.d/registry.example.io.json  {\n  \"rktKind\": \"auth\",\n  \"rktVersion\": \"v1\",\n  \"domains\": [\n    \"https://registry.example.io/v0/\"\n  ],\n  \"type\": \"basic\",\n  \"credentials\": {\n    \"user\": \"giffeeLover93\",\n    \"password\": \"passphrases are great!\"\n  }\n}  While you  can  embed your password in plaintext in this file, you should try using a disposable token instead. Check your registry documentation to see if it offers token-based authentication.  Now rkt will authenticate with  https://registry.example.io/v0/  using the provided credentials to fetch images.  For more information about rkt credentials, see the  rkt configuration docs .  Just like with the Docker config, this file can be copied to  /etc/rkt/auth.d/registry.example.io.json  on a Flatcar Container Linux node during system provisioning with  a Container Linux Config .",
            "title": "rkt"
        },
        {
            "location": "/os/registry-authentication/#mesos",
            "text": "Mesos uses a gzip-compressed archive of a  .docker/config.json  (directory and file) to access private repositories.  Once you have followed the above steps to  create the docker registry auth config file  create your Mesos configuration using  tar :  $ tar cxf ~/.docker/config.json  The archive secret is referenced via the  uris  field in a container specification file:  {\n  \"id\": \"/some/name/or/id\",\n  \"cpus\": 1,\n  \"mem\": 1024,\n  \"instances\": 1,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"https://registry.example.io/v0/giffee_lover_93/some-image\",\n      \"network\": \"HOST\"\n    }\n  },\n\n  \"uris\":  [\n      \"file:///path/to/registry.example.io.tar.gz\"\n  ]\n}  More thorough information about configuring Mesos registry authentication can be found on the  'Using a Private Docker Registry'  documentation.",
            "title": "Mesos"
        },
        {
            "location": "/os/registry-authentication/#copying-the-config-file-with-a-container-linux-config",
            "text": "Container Linux Configs  can be used to provision a Flatcar Container Linux node on first boot. Here we will use it to copy registry authentication config files to their appropriate destination on disk. This provides immediate access to your private Docker Hub and Quay image repositories without the need for manual intervention. The same Container Linux Config file can be used to copy registry auth configs onto an entire cluster of Flatcar Container Linux nodes.  Here is an example of using a Container Linux Config to write the .docker/config.json registry auth configuration file mentioned above to the appropriate path on the Flatcar Container Linux node:  storage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          {\n            \"auths\": {\n              \"quay.io\": {\n                \"auth\": \"AbCdEfGhIj\",\n                \"email\": \"your.email@example.com\"\n              }\n            }\n          }  Container Linux Configs can also download a file from a remote location and verify its integrity with a SHA512 hash:  storage:\n  files:\n    - path: /home/core/.docker/config.json\n      filesystem: root\n      mode: 0644\n      contents:\n        remote:\n          url: http://internal.infra.example.com/cluster-docker-config.json\n          verification:\n            hash:\n              function: sha512\n              sum: 0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef0123456789abcdef  For details, check out the  Container Linux Config examples .",
            "title": "Copying the config file with a Container Linux Config"
        },
        {
            "location": "/os/root-filesystem-placement/",
            "text": "Configuring Root Filesystem Placement\n\u00b6\n\n\nFlatcar Container Linux supports composite disk devices such as RAID arrays. If the root filesystem is placed on a composite device, special care must be taken to ensure Flatcar Container Linux can find and mount the filesystem early in the boot process. GPT partition entries have a \npartition type GUID\n that specifies what type of partition it is (e.g. Linux filesystem); Flatcar Container Linux uses special type GUIDs to indicate that a partition is a component of a composite device containing the root filesystem.\n\n\nRoot on RAID\n\u00b6\n\n\nRAID enables multiple disks to be combined into a single logical disk to increase reliability and performance. To create a software RAID array when provisioning a Flatcar Container Linux system, use the \nstorage.raid\n section of a \nContainer Linux Config\n. RAID components containing the root filesystem must have the type GUID \nbe9067b9-ea49-4f15-b4f6-f36f8c9e1818\n. All other RAID arrays must not have that GUID; the Linux RAID partition GUID \na19d880f-05fc-4d3b-a006-743f0f84911e\n is recommended instead. See the \nIgnition documentation\n for more information on setting up RAID for data volumes.\n\n\nOverview\n\u00b6\n\n\nTo place the root filesystem on a RAID array:\n\n\n\n\nCreate the component partitions used in the RAID array with the type GUID \nbe9067b9-ea49-4f15-b4f6-f36f8c9e1818\n.\n\n\nCreate a RAID array from the component partitions.\n\n\nCreate a filesystem labeled \nROOT\n on the RAID array.\n\n\nRemove the \nROOT\n label from the original root filesystem.\n\n\n\n\nExample Container Linux Config\n\u00b6\n\n\nThis Container Linux Config creates partitions on \n/dev/vdb\n and \n/dev/vdc\n that fill each disk, creates a RAID array named \nroot_array\n from those partitions, and finally creates the root filesystem on the array. To prevent inadvertent booting from the \noriginal root filesystem\n, \n/dev/vda9\n is reformatted with a blank ext4 filesystem labeled \nunused\n.\n\n\nWarning: This will erase both \n/dev/vdb\n and \n/dev/vdc\n.\n\n\nstorage:\n  disks:\n    - device: /dev/vdb\n      wipe_table: true\n      partitions:\n       - label: root1\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n    - device: /dev/vdc\n      wipe_table: true\n      partitions:\n       - label: root2\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n  raid:\n    - name: \"root_array\"\n      level: \"raid1\"\n      devices:\n        - \"/dev/vdb1\"\n        - \"/dev/vdc1\"\n  filesystems:\n    - name: \"ROOT\"\n      mount:\n        device: \"/dev/md/root_array\"\n        format: \"ext4\"\n        label: \"ROOT\"\n    - name: \"unused\"\n      mount:\n        device: \"/dev/vda9\"\n        format: \"ext4\"\n        wipe_filesystem: true\n        label: \"unused\"\n\n\nLimitations\n\u00b6\n\n\n\n\nOther system partitions, such as \nUSR-A\n, \nUSR-B\n, \nOEM\n, and \nEFI-SYSTEM\n, cannot be placed on a software RAID array.\n\n\nRAID components containing the root filesystem must be partitions on a GPT-partitioned device, not whole-disk devices or partitions on an MBR-partitioned disk.\n\n\n/etc/mdadm.conf\n cannot be used to configure a RAID array containing the root filesystem.\n\n\nSince Ignition cannot modify the type GUID of existing partitions, the default \nROOT\n partition cannot be reused as a component of a RAID array. A future version of Ignition will support resizing the \nROOT\n partition and changing its type GUID, allowing it to be used as part of a RAID array.",
            "title": "Configuring Root Filesystem Placement"
        },
        {
            "location": "/os/root-filesystem-placement/#configuring-root-filesystem-placement",
            "text": "Flatcar Container Linux supports composite disk devices such as RAID arrays. If the root filesystem is placed on a composite device, special care must be taken to ensure Flatcar Container Linux can find and mount the filesystem early in the boot process. GPT partition entries have a  partition type GUID  that specifies what type of partition it is (e.g. Linux filesystem); Flatcar Container Linux uses special type GUIDs to indicate that a partition is a component of a composite device containing the root filesystem.",
            "title": "Configuring Root Filesystem Placement"
        },
        {
            "location": "/os/root-filesystem-placement/#root-on-raid",
            "text": "RAID enables multiple disks to be combined into a single logical disk to increase reliability and performance. To create a software RAID array when provisioning a Flatcar Container Linux system, use the  storage.raid  section of a  Container Linux Config . RAID components containing the root filesystem must have the type GUID  be9067b9-ea49-4f15-b4f6-f36f8c9e1818 . All other RAID arrays must not have that GUID; the Linux RAID partition GUID  a19d880f-05fc-4d3b-a006-743f0f84911e  is recommended instead. See the  Ignition documentation  for more information on setting up RAID for data volumes.",
            "title": "Root on RAID"
        },
        {
            "location": "/os/root-filesystem-placement/#overview",
            "text": "To place the root filesystem on a RAID array:   Create the component partitions used in the RAID array with the type GUID  be9067b9-ea49-4f15-b4f6-f36f8c9e1818 .  Create a RAID array from the component partitions.  Create a filesystem labeled  ROOT  on the RAID array.  Remove the  ROOT  label from the original root filesystem.",
            "title": "Overview"
        },
        {
            "location": "/os/root-filesystem-placement/#example-container-linux-config",
            "text": "This Container Linux Config creates partitions on  /dev/vdb  and  /dev/vdc  that fill each disk, creates a RAID array named  root_array  from those partitions, and finally creates the root filesystem on the array. To prevent inadvertent booting from the  original root filesystem ,  /dev/vda9  is reformatted with a blank ext4 filesystem labeled  unused .  Warning: This will erase both  /dev/vdb  and  /dev/vdc .  storage:\n  disks:\n    - device: /dev/vdb\n      wipe_table: true\n      partitions:\n       - label: root1\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n    - device: /dev/vdc\n      wipe_table: true\n      partitions:\n       - label: root2\n         type_guid: be9067b9-ea49-4f15-b4f6-f36f8c9e1818\n  raid:\n    - name: \"root_array\"\n      level: \"raid1\"\n      devices:\n        - \"/dev/vdb1\"\n        - \"/dev/vdc1\"\n  filesystems:\n    - name: \"ROOT\"\n      mount:\n        device: \"/dev/md/root_array\"\n        format: \"ext4\"\n        label: \"ROOT\"\n    - name: \"unused\"\n      mount:\n        device: \"/dev/vda9\"\n        format: \"ext4\"\n        wipe_filesystem: true\n        label: \"unused\"",
            "title": "Example Container Linux Config"
        },
        {
            "location": "/os/root-filesystem-placement/#limitations",
            "text": "Other system partitions, such as  USR-A ,  USR-B ,  OEM , and  EFI-SYSTEM , cannot be placed on a software RAID array.  RAID components containing the root filesystem must be partitions on a GPT-partitioned device, not whole-disk devices or partitions on an MBR-partitioned disk.  /etc/mdadm.conf  cannot be used to configure a RAID array containing the root filesystem.  Since Ignition cannot modify the type GUID of existing partitions, the default  ROOT  partition cannot be reused as a component of a RAID array. A future version of Ignition will support resizing the  ROOT  partition and changing its type GUID, allowing it to be used as part of a RAID array.",
            "title": "Limitations"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/",
            "text": "Scheduling tasks with systemd timers\n\u00b6\n\n\nFlatcar Container Linux uses systemd timers (\ncron\n replacement) to schedule tasks. Here we will show you how you can schedule a periodic job.\n\n\nLet's create an alternative for this \ncrontab\n job:\n\n\n*/10 * * * * /usr/bin/date >> /tmp/date\n\n\n\nTimers work directly with services' units. So we have to create \n/etc/systemd/system/date.service\n first:\n\n\n[Unit]\nDescription=Prints date into /tmp/date file\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'\n\n\n\nThen we have to create timer unit with the same name but with \n*.timer\n suffix \n/etc/systemd/system/date.timer\n:\n\n\n[Unit]\nDescription=Run date.service every 10 minutes\n\n[Timer]\nOnCalendar=*:0/10\n\n\n\nThis config will run \ndate.service\n every 10 minutes. You can also list all timers enabled in your system using \nsystemctl list-timers\n command or \nsystemctl list-timers --all\n to list all timers. Run \nsystemctl start date.timer\n to enable timer.\n\n\nYou can also create timer with different name, i.e. \ntask.timer\n. In this case you have specify service unit name:\n\n\nUnit=date.service\n\n\n\nContainer Linux Config\n\u00b6\n\n\nHere you'll find an example Container Linux Config demonstrating how to install systemd timers:\n\n\nsystemd:\n  units:\n    - name: date.service\n      contents: |\n        [Unit]\n        Description=Prints date into /tmp/date file\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'\n    - name: date.timer\n      enable: true\n      contents: |\n        [Unit]\n        Description=Run date.service every 10 minutes\n\n        [Timer]\n        OnCalendar=*:0/10\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nFurther reading\n\u00b6\n\n\nIf you're interested in more general systemd timers feature, check out the \nfull documentation\n.",
            "title": "Scheduling tasks with systemd timers"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/#scheduling-tasks-with-systemd-timers",
            "text": "Flatcar Container Linux uses systemd timers ( cron  replacement) to schedule tasks. Here we will show you how you can schedule a periodic job.  Let's create an alternative for this  crontab  job:  */10 * * * * /usr/bin/date >> /tmp/date  Timers work directly with services' units. So we have to create  /etc/systemd/system/date.service  first:  [Unit]\nDescription=Prints date into /tmp/date file\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'  Then we have to create timer unit with the same name but with  *.timer  suffix  /etc/systemd/system/date.timer :  [Unit]\nDescription=Run date.service every 10 minutes\n\n[Timer]\nOnCalendar=*:0/10  This config will run  date.service  every 10 minutes. You can also list all timers enabled in your system using  systemctl list-timers  command or  systemctl list-timers --all  to list all timers. Run  systemctl start date.timer  to enable timer.  You can also create timer with different name, i.e.  task.timer . In this case you have specify service unit name:  Unit=date.service",
            "title": "Scheduling tasks with systemd timers"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/#container-linux-config",
            "text": "Here you'll find an example Container Linux Config demonstrating how to install systemd timers:  systemd:\n  units:\n    - name: date.service\n      contents: |\n        [Unit]\n        Description=Prints date into /tmp/date file\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/sh -c '/usr/bin/date >> /tmp/date'\n    - name: date.timer\n      enable: true\n      contents: |\n        [Unit]\n        Description=Run date.service every 10 minutes\n\n        [Timer]\n        OnCalendar=*:0/10\n\n        [Install]\n        WantedBy=multi-user.target",
            "title": "Container Linux Config"
        },
        {
            "location": "/os/scheduling-tasks-with-systemd-timers/#further-reading",
            "text": "If you're interested in more general systemd timers feature, check out the  full documentation .",
            "title": "Further reading"
        },
        {
            "location": "/os/sdk-building-production-images/",
            "text": "Building production images\n\u00b6\n\n\nThis document is not maintained\n\u00b6\n\n\nThis document still contains useful pointers, but the details are not necessarily up to date.\n\n\nIntroduction\n\u00b6\n\n\nIn general the automated process should always be used but in a pinch putting together a release manually may be necessary. All release information is tracked in the \nmanifest\n git repository which is usually organized like so:\n\n\n\n\nbuild-109.xml (previous release manifest)\n\n\nbuild-115.xml (current release manifest)\n\n\nmaster.xml    (master branch manifest)\n\n\nversion.txt   (current version information)\n\n\ndefault.xml -> master.xml\n\n\nrelease.xml -> build-115.xml\n\n\n\n\nTagging releases\n\u00b6\n\n\nThe first step of building a release is updating and tagging the release in the manifest git repository. A typical release off of master involves the following steps:\n\n\n\n\nMake sure you are on the master branch: \nrepo init -b master\n\n\nSync/checkout source, excluding local changes: \nrepo sync --detach\n\n\nIn the scripts directory: \n./tag_release --push\n\n\n\n\nThat was far too easy, if you need to do it the hard way try this:\n\n\n\n\nMake sure you are on the master branch: \nrepo init -b master\n\n\nSync/checkout source, excluding local changes: \nrepo sync --detach\n\n\nSwitch to the somewhat hidden manifests checkout: \ncd .repo/manifests\n\n\nUpdate \nversion.txt\n with the desired version number.\n\n\nCOREOS_BUILD is the major version number, and should be the number of days since July 1\nst\n, 2013. COREOS_BRANCH should start at 0 and is incremented for every normal release based on a particular COREOS_BUILD version. COREOS_PATCH is reserved for exceptional situations such as emergency manual releases and should normally be 0.\n\n\nThe complete version string is COREOS_BUILD.COREOS_BRANCH.COREOS_PATCH\n\n\nCOREOS_SDK_VERSION should be the complete version string of an existing build. \ncork\n uses this to pick what SDK tarball to use when creating a fresh chroot and provides a fallback set of binary packages to use when the current release's packages are unavailable. Usually it will be one release behind COREOS_BUILD.\n\n\n\n\n\n\nGenerate a release manifest: \nrepo manifest -r -o build-$BUILD.xml\n where \n$BUILD\n is the current value of COREOS_BUILD in \nversion.txt\n.\n\n\nUpdate \nrelease.xml\n: \nln -sf build-$BUILD.xml release.xml\n\n\nCommit! \ngit add build-$BUILD.xml; git commit -a\n\n\nTag! \ngit tag v$BUILD.$BRANCH.$PATCH\n\n\nPush! \ngit push origin HEAD:master HEAD:dev-channel HEAD:build-$BUILD v$BUILD.$BRANCH.$PATCH\n\n\n\n\nIf a release branch needs to be updated after master has moved on the procedure is similar. Unfortunately since tagging branched releases (not on master) is a bit trickier to get right the \ntag_release\n script cannot be used. The automated build will kick off after updating the \ndev-channel\n branch.\n\n\n\n\nCheck out the release instead of master: \nrepo init -b build-$BUILD -m release.xml\n\n\nSync, cherry-pick, push, and whatever else is required to publish the desired changes in the repo-managed projects. If the desired changes are already published (such as if you are just updating to a later commit from a project's master branch) then this can be skipped.\n\n\ncd .repo/manifests\n\n\nUpdate \nversion.txt\n as desired. Usually just increment COREOS_PATCH.\n\n\nUpdate \nbuild-$BUILD.xml\n as desired. The output of \nrepo manifest -r\n shouldn't be used verbatim this time because it won't generate meaningful values for the \nupstream\n project attribute when starting from a release manifest instead of \nmaster.xml\n but it can be useful for looking up the git commit to update the \nrevision\n attribute to. If the new git commit is on a branch other than master be sure to update the \nupstream\n attribute with the appropriate ref spec for that branch.\n\n\nIf this is the first time this branch has been updated on its own update the \ndefault.xml\n link so checking out this manifest branch with repo init but without the \n-m\n argument works: \nln -sf build-$BUILD.xml default.xml\n\n\nCommit! \ngit commit -a\n\n\nTag! \ngit tag v$BUILD.$BRANCH.$PATCH\n\n\nPush! \ngit push origin HEAD:dev-channel HEAD:build-$BUILD v$BUILD.$BRANCH.$PATCH\n\n\n\n\nNow you can start building images! This will build an image that can be ran under KVM and uses near production values.\n\n\nNote: Add \nCOREOS_OFFICIAL=1\n here if you are making a real release. That will change the version to leave off the build id suffix.\n\n\n./build_image prod --group alpha\n\n\n\nThe generated production image is bootable as-is by qemu but for a larger ROOT partition or VMware images use \nimage_to_vm.sh\n as described in the final output of \nbuild_image\n.\n\n\nTips and Tricks\n\u00b6\n\n\nWe've compiled a \nlist of tips and tricks\n that can make working with the SDK a bit easier.",
            "title": "Building production images"
        },
        {
            "location": "/os/sdk-building-production-images/#building-production-images",
            "text": "",
            "title": "Building production images"
        },
        {
            "location": "/os/sdk-building-production-images/#this-document-is-not-maintained",
            "text": "This document still contains useful pointers, but the details are not necessarily up to date.",
            "title": "This document is not maintained"
        },
        {
            "location": "/os/sdk-building-production-images/#introduction",
            "text": "In general the automated process should always be used but in a pinch putting together a release manually may be necessary. All release information is tracked in the  manifest  git repository which is usually organized like so:   build-109.xml (previous release manifest)  build-115.xml (current release manifest)  master.xml    (master branch manifest)  version.txt   (current version information)  default.xml -> master.xml  release.xml -> build-115.xml",
            "title": "Introduction"
        },
        {
            "location": "/os/sdk-building-production-images/#tagging-releases",
            "text": "The first step of building a release is updating and tagging the release in the manifest git repository. A typical release off of master involves the following steps:   Make sure you are on the master branch:  repo init -b master  Sync/checkout source, excluding local changes:  repo sync --detach  In the scripts directory:  ./tag_release --push   That was far too easy, if you need to do it the hard way try this:   Make sure you are on the master branch:  repo init -b master  Sync/checkout source, excluding local changes:  repo sync --detach  Switch to the somewhat hidden manifests checkout:  cd .repo/manifests  Update  version.txt  with the desired version number.  COREOS_BUILD is the major version number, and should be the number of days since July 1 st , 2013. COREOS_BRANCH should start at 0 and is incremented for every normal release based on a particular COREOS_BUILD version. COREOS_PATCH is reserved for exceptional situations such as emergency manual releases and should normally be 0.  The complete version string is COREOS_BUILD.COREOS_BRANCH.COREOS_PATCH  COREOS_SDK_VERSION should be the complete version string of an existing build.  cork  uses this to pick what SDK tarball to use when creating a fresh chroot and provides a fallback set of binary packages to use when the current release's packages are unavailable. Usually it will be one release behind COREOS_BUILD.    Generate a release manifest:  repo manifest -r -o build-$BUILD.xml  where  $BUILD  is the current value of COREOS_BUILD in  version.txt .  Update  release.xml :  ln -sf build-$BUILD.xml release.xml  Commit!  git add build-$BUILD.xml; git commit -a  Tag!  git tag v$BUILD.$BRANCH.$PATCH  Push!  git push origin HEAD:master HEAD:dev-channel HEAD:build-$BUILD v$BUILD.$BRANCH.$PATCH   If a release branch needs to be updated after master has moved on the procedure is similar. Unfortunately since tagging branched releases (not on master) is a bit trickier to get right the  tag_release  script cannot be used. The automated build will kick off after updating the  dev-channel  branch.   Check out the release instead of master:  repo init -b build-$BUILD -m release.xml  Sync, cherry-pick, push, and whatever else is required to publish the desired changes in the repo-managed projects. If the desired changes are already published (such as if you are just updating to a later commit from a project's master branch) then this can be skipped.  cd .repo/manifests  Update  version.txt  as desired. Usually just increment COREOS_PATCH.  Update  build-$BUILD.xml  as desired. The output of  repo manifest -r  shouldn't be used verbatim this time because it won't generate meaningful values for the  upstream  project attribute when starting from a release manifest instead of  master.xml  but it can be useful for looking up the git commit to update the  revision  attribute to. If the new git commit is on a branch other than master be sure to update the  upstream  attribute with the appropriate ref spec for that branch.  If this is the first time this branch has been updated on its own update the  default.xml  link so checking out this manifest branch with repo init but without the  -m  argument works:  ln -sf build-$BUILD.xml default.xml  Commit!  git commit -a  Tag!  git tag v$BUILD.$BRANCH.$PATCH  Push!  git push origin HEAD:dev-channel HEAD:build-$BUILD v$BUILD.$BRANCH.$PATCH   Now you can start building images! This will build an image that can be ran under KVM and uses near production values.  Note: Add  COREOS_OFFICIAL=1  here if you are making a real release. That will change the version to leave off the build id suffix.  ./build_image prod --group alpha  The generated production image is bootable as-is by qemu but for a larger ROOT partition or VMware images use  image_to_vm.sh  as described in the final output of  build_image .",
            "title": "Tagging releases"
        },
        {
            "location": "/os/sdk-building-production-images/#tips-and-tricks",
            "text": "We've compiled a  list of tips and tricks  that can make working with the SDK a bit easier.",
            "title": "Tips and Tricks"
        },
        {
            "location": "/os/sdk-disk-partitions/",
            "text": "Flatcar Container Linux disk layout\n\u00b6\n\n\nFlatcar Container Linux is designed to be reliably updated via a continuous stream of updates. The operating system has 9 different disk partitions, utilizing a subset of those to make each update safe and enable a roll-back to a previous version if anything goes wrong.\n\n\nPartition table\n\u00b6\n\n\n\n\n\n\n\n\nNumber\n\n\nLabel\n\n\nDescription\n\n\nPartition Type\n\n\n\n\n\n\n\n\n\n\n1\n\n\nEFI-SYSTEM\n\n\nContains the bootloader\n\n\nFAT32\n\n\n\n\n\n\n2\n\n\nBIOS-BOOT\n\n\nContains the second stages of GRUB for use when booting from BIOS\n\n\ngrub core.img\n\n\n\n\n\n\n3\n\n\nUSR-A\n\n\nOne of two active/passive partitions holding Flatcar Container Linux\n\n\nEXT4\n\n\n\n\n\n\n4\n\n\nUSR-B\n\n\nOne of two active/passive partitions holding Flatcar Container Linux\n\n\n(empty on first boot)\n\n\n\n\n\n\n5\n\n\nROOT-C\n\n\nThis partition is reserved for future use\n\n\n(none)\n\n\n\n\n\n\n6\n\n\nOEM\n\n\nStores configuration data specific to an \nOEM platform\n\n\nEXT4\n\n\n\n\n\n\n7\n\n\nOEM-CONFIG\n\n\nOptional storage for an OEM\n\n\n(defined by OEM)\n\n\n\n\n\n\n8\n\n\n(unused)\n\n\nThis partition is reserved for future use\n\n\n(none)\n\n\n\n\n\n\n9\n\n\nROOT\n\n\nStateful partition for storing persistent data\n\n\nEXT4, BTRFS, or XFS\n\n\n\n\n\n\n\n\nFor more information, \nread more about the disk layout\n used by Chromium and ChromeOS, which inspired the layout used by Flatcar Container Linux.\n\n\nMounted filesystems\n\u00b6\n\n\nFlatcar Container Linux is divided into two main filesystems, a read-only \n/usr\n and a stateful read/write \n/\n.\n\n\nRead-only /usr\n\u00b6\n\n\nThe \nUSR-A\n or \nUSR-B\n partitions are interchangeable and one of the two is mounted as a read-only filesystem at \n/usr\n. After an update, Flatcar Container Linux will re-configure the GPT priority attribute, instructing the bootloader to boot from the passive (newly updated) partition. Here's an example of the priority flags set on an Amazon EC2 machine:\n\n\n$ sudo cgpt show /dev/xvda\n       start        size    part  contents\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for coreos-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1\n\n\n\nFlatcar Container Linux images ship with the \nUSR-B\n partition empty to reduce the image filesize. The first Flatcar Container Linux update will populate it and start the normal active/passive scheme.\n\n\nThe OEM partition is also mounted as read-only at \n/usr/share/oem\n.\n\n\nStateful root\n\u00b6\n\n\nAll stateful data, including container images, is stored within the read/write filesystem mounted at \n/\n. On first boot, the ROOT partition and filesystem will expand to fill any remaining free space at the end of the drive.\n\n\nThe data stored on the root partition isn't manipulated by the update process. In return, we do our best to prevent you from modifying the data in /usr.\n\n\nDue to the unique disk layout of Flatcar Container Linux, an \nrm -rf --one-file-system --no-preserve-root /\n is an unsupported but valid operation to purge any OS data. On the next boot, the machine should just start from a clean state.\n\n\nTo \nre-provision\n the node after such cleanup, use \ntouch /boot/flatcar/first_boot\n to trigger Ignition \nto run once\n again on the next boot.",
            "title": "Flatcar Container Linux disk layout"
        },
        {
            "location": "/os/sdk-disk-partitions/#flatcar-container-linux-disk-layout",
            "text": "Flatcar Container Linux is designed to be reliably updated via a continuous stream of updates. The operating system has 9 different disk partitions, utilizing a subset of those to make each update safe and enable a roll-back to a previous version if anything goes wrong.",
            "title": "Flatcar Container Linux disk layout"
        },
        {
            "location": "/os/sdk-disk-partitions/#partition-table",
            "text": "Number  Label  Description  Partition Type      1  EFI-SYSTEM  Contains the bootloader  FAT32    2  BIOS-BOOT  Contains the second stages of GRUB for use when booting from BIOS  grub core.img    3  USR-A  One of two active/passive partitions holding Flatcar Container Linux  EXT4    4  USR-B  One of two active/passive partitions holding Flatcar Container Linux  (empty on first boot)    5  ROOT-C  This partition is reserved for future use  (none)    6  OEM  Stores configuration data specific to an  OEM platform  EXT4    7  OEM-CONFIG  Optional storage for an OEM  (defined by OEM)    8  (unused)  This partition is reserved for future use  (none)    9  ROOT  Stateful partition for storing persistent data  EXT4, BTRFS, or XFS     For more information,  read more about the disk layout  used by Chromium and ChromeOS, which inspired the layout used by Flatcar Container Linux.",
            "title": "Partition table"
        },
        {
            "location": "/os/sdk-disk-partitions/#mounted-filesystems",
            "text": "Flatcar Container Linux is divided into two main filesystems, a read-only  /usr  and a stateful read/write  / .",
            "title": "Mounted filesystems"
        },
        {
            "location": "/os/sdk-disk-partitions/#read-only-usr",
            "text": "The  USR-A  or  USR-B  partitions are interchangeable and one of the two is mounted as a read-only filesystem at  /usr . After an update, Flatcar Container Linux will re-configure the GPT priority attribute, instructing the bootloader to boot from the passive (newly updated) partition. Here's an example of the priority flags set on an Amazon EC2 machine:  $ sudo cgpt show /dev/xvda\n       start        size    part  contents\n      270336     2097152       3  Label: \"USR-A\"\n                                  Type: Alias for coreos-rootfs\n                                  UUID: 7130C94A-213A-4E5A-8E26-6CCE9662F132\n                                  Attr: priority=1 tries=0 successful=1  Flatcar Container Linux images ship with the  USR-B  partition empty to reduce the image filesize. The first Flatcar Container Linux update will populate it and start the normal active/passive scheme.  The OEM partition is also mounted as read-only at  /usr/share/oem .",
            "title": "Read-only /usr"
        },
        {
            "location": "/os/sdk-disk-partitions/#stateful-root",
            "text": "All stateful data, including container images, is stored within the read/write filesystem mounted at  / . On first boot, the ROOT partition and filesystem will expand to fill any remaining free space at the end of the drive.  The data stored on the root partition isn't manipulated by the update process. In return, we do our best to prevent you from modifying the data in /usr.  Due to the unique disk layout of Flatcar Container Linux, an  rm -rf --one-file-system --no-preserve-root /  is an unsupported but valid operation to purge any OS data. On the next boot, the machine should just start from a clean state.  To  re-provision  the node after such cleanup, use  touch /boot/flatcar/first_boot  to trigger Ignition  to run once  again on the next boot.",
            "title": "Stateful root"
        },
        {
            "location": "/os/sdk-modifying-coreos/",
            "text": "Flatcar Container Linux developer SDK guide\n\u00b6\n\n\nThese are the instructions for building Flatcar Container Linux itself. By the end of the guide you will build a developer image that you can run under KVM and have tools for making changes to the code.\n\n\nFlatcar Container Linux is an open source project. All of the source for Flatcar Container Linux is available on \ngithub\n. If you find issues with these docs or the code please send a pull request.\n\n\nDirect questions and suggestions to the \nIRC channel\n or \nmailing list\n.\n\n\nGetting started\n\u00b6\n\n\nLet's get set up with an SDK chroot and build a bootable image of Flatcar Container Linux. The SDK chroot has a full toolchain and isolates the build process from quirks and differences between host OSes. The SDK must be run on an x86-64 Linux machine, the distro should not matter (Ubuntu, Fedora, etc).\n\n\nPrerequisites\n\u00b6\n\n\nSystem requirements to get started:\n\n\n\n\ncurl\n\n\ngit\n\n\nbzip2\n\n\ngpg\n\n\nsudo\n\n\n\n\nYou also need a proper git setup:\n\n\ngit config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"\n\n\n\nNOTE\n: Do the git configuration as a normal user and not with sudo.\n\n\nUsing Cork\n\u00b6\n\n\nThe \ncork\n utility, included in the Flatcar Container Linux \nmantle\n project, is used to create and work with an SDK chroot.\n\n\nFirst, download the cork utility and verify it with the signature:\n\n\ncurl -L -o cork https://github.com/flatcar-linux/mantle/releases/download/v0.13.2/cork-0.13.2-amd64\ncurl -L -o cork.sig https://github.com/flatcar-linux/mantle/releases/download/v0.13.2/cork-0.13.2-amd64.sig\ngpg --receive-keys 84C8E771C0DF83DFBFCAAAF03ADA89DEC2507883\ngpg --verify cork.sig cork\n\n\n\nThe \ngpg --verify\n command should output something like this:\n\n\ngpg: Signature made Mon 07 Jan 2019 14:51:50 CET\ngpg:                using RSA key 84C8E771C0DF83DFBFCAAAF03ADA89DEC2507883\ngpg: Good signature from \"Flatcar Application Signing Key <buildbot@flatcar-linux.org>\" [unknown]\nPrimary key fingerprint: C1C0 B82A 2F75 90B2 E369  822B E52F 0DB3 9145 3C45\n     Subkey fingerprint: 84C8 E771 C0DF 83DF BFCA  AAF0 3ADA 89DE C250 7883\n\n\n\nThen proceed with the installation of the cork binary to a location on your path:\n\n\nchmod +x cork\nmkdir -p ~/.local/bin\nmv cork ~/.local/bin\nexport PATH=$PATH:$HOME/.local/bin\n\n\n\nYou may want to add the \nPATH\n export to your shell profile (e.g. \n.bashrc\n).\n\n\nNext, use the cork utility to create a project directory. This will hold all of your git repos and the SDK chroot. A few gigabytes of space will be necessary.\n\n\nmkdir flatcar-sdk\ncd flatcar-sdk\ncork create # This will request root permisions via sudo\ncork enter # This will request root permisions via sudo\n\n\n\nVerify you are in the SDK chroot:\n\n\n$ grep NAME /etc/os-release\nNAME=\"Flatcar Container Linux by Kinvolk\"\n\nTo leave the SDK chroot, simply run \nexit\n.\n\n\nTo use the SDK chroot in the future, run \ncork enter\n from the above directory.\n\n\nBuilding an image\n\u00b6\n\n\nSet up the chroot\n\u00b6\n\n\nAfter entering the chroot via \ncork\n for the first time, you should set user \ncore\n's password:\n\n\n./set_shared_user_password.sh\n\n\n\nThis is the password you will use to log into the console of images built and launched with the SDK.\n\n\nSelecting the architecture to build\n\u00b6\n\n\namd64-usr\n and \narm64-usr\n are the only targets supported by Flatcar.\n\n\n64 bit AMD: The amd64-usr target\n\u00b6\n\n\nThe \n--board\n option can be set to one of a few known target architectures, or system \"boards\", to build for a given CPU.\n\n\nTo create a root filesystem for the \namd64-usr\n target beneath the directory \n/build/amd64-usr/\n:\n\n\n./setup_board --default --board=amd64-usr\n\n\n\nCompile and link system binaries\n\u00b6\n\n\nBuild all of the target binary packages:\n\n\n./build_packages\n\n\n\nRender the Flatcar Container Linux image\n\u00b6\n\n\nBuild a production image based on the binary packages built above:\n\n\n./build_image\n\n\n\nAfter \nbuild_image\n completes, it prints commands for converting the raw bin into a bootable virtual machine. Run the \nimage_to_vm.sh\n command.\n\n\nBooting\n\u00b6\n\n\nOnce you build an image you can launch it with KVM (instructions will print out after \nimage_to_vm.sh\n runs).\n\n\nIf you encounter errors with KVM, verify that virtualization is supported by your CPU by running \negrep '(vmx|svm)' /proc/cpuinfo\n. The \n/dev/kvm\n directory will be in your host OS when virtualization is enabled in the BIOS.\n\n\nThe \n./flatcar_production_qemu.sh\n file can be found in the \n~/trunk/src/build/images/amd64-usr/latest\n directory inside the SDK chroot.\n\n\nBoot Options\n\u00b6\n\n\nAfter \nimage_to_vm.sh\n completes, run \n./flatcar_production_qemu.sh -curses\n to launch a graphical interface to log in to the Flatcar Container Linux VM.\n\n\nYou could instead use the \n-nographic\n option, \n./flatcar_production_qemu.sh -nographic\n, which gives you the ability to switch from the VM to the QEMU monitor console by pressing \nCTRL\n+\na\n and then \nc\n. To close the Flatcar Container Linux Guest OS VM, run \nsudo systemctl poweroff\n inside the VM. \n\n\nYou could also log in via SSH by running \n./flatcar_production_qemu.sh\n and then running \nssh core@127.0.0.1 -p 2222\n to enter the guest OS. Running without the \n-p 2222\n option will arise a \nssh: connect to host 127.0.0.1 port 22: Connection refused\n or \nPermission denied (publickey,gssapi-keyex,gssapi-with-mic)\n warning. Additionally, you can log in via SSH keys or with a different ssh port by running this example \n./flatcar_production_qemu.sh -a ~/.ssh/authorized_keys -p 2223 -- -curses\n. Refer to the \nBooting with QEMU\n guide for more information on this usage.\n\n\nThe default login username is \ncore\n and the \npassword is the one set in the \n./set_shared_user_password\n step of this guide. If you forget your password, you will need to rerun \n./set_shared_user_password\n and then \n./build_image\n again.\n\n\nMaking changes\n\u00b6\n\n\ngit and repo\n\u00b6\n\n\nFlatcar Container Linux is managed by \nrepo\n, a tool built for the Android project that makes managing a large number of git repositories easier. From the repo announcement blog:\n\n\n\n\nThe repo tool uses an XML-based manifest file describing where the upstream\nrepositories are, and how to merge them into a single working checkout. repo\nwill recurse across all the git subtrees and handle uploads, pulls, and other\nneeded items. repo has built-in knowledge of topic branches and makes working\nwith them an essential part of the workflow.\n\n\n\n\n(from the \nGoogle Open Source Blog\n)\n\n\nYou can find the full manual for repo by visiting \nandroid.com - Developing\n.\n\n\nUpdating repo manifests\n\u00b6\n\n\nThe repo manifest for Flatcar Container Linux lives in a git repository in\n\n.repo/manifests\n. If you need to update the manifest edit \ndefault.xml\n\nin this directory.\n\n\nrepo\n uses a branch called 'default' to track the upstream branch you\nspecify in \nrepo init\n, this defaults to 'origin/master'. Keep this in\nmind when making changes, the origin git repository should not have a\n'default' branch.\n\n\nBuilding release images\n\u00b6\n\n\nThe \nproduction images\n document is unmaintained and out of date, but contains useful pointers as to how official release images are built.\n\n\nTips and tricks\n\u00b6\n\n\nWe've compiled a \nlist of tips and tricks\n that can make working with the SDK a bit easier.\n\n\nTesting images\n\u00b6\n\n\nMantle\n is a collection of utilities used in testing and launching SDK images.",
            "title": "Flatcar Container Linux developer SDK guide"
        },
        {
            "location": "/os/sdk-modifying-coreos/#flatcar-container-linux-developer-sdk-guide",
            "text": "These are the instructions for building Flatcar Container Linux itself. By the end of the guide you will build a developer image that you can run under KVM and have tools for making changes to the code.  Flatcar Container Linux is an open source project. All of the source for Flatcar Container Linux is available on  github . If you find issues with these docs or the code please send a pull request.  Direct questions and suggestions to the  IRC channel  or  mailing list .",
            "title": "Flatcar Container Linux developer SDK guide"
        },
        {
            "location": "/os/sdk-modifying-coreos/#getting-started",
            "text": "Let's get set up with an SDK chroot and build a bootable image of Flatcar Container Linux. The SDK chroot has a full toolchain and isolates the build process from quirks and differences between host OSes. The SDK must be run on an x86-64 Linux machine, the distro should not matter (Ubuntu, Fedora, etc).",
            "title": "Getting started"
        },
        {
            "location": "/os/sdk-modifying-coreos/#prerequisites",
            "text": "System requirements to get started:   curl  git  bzip2  gpg  sudo   You also need a proper git setup:  git config --global user.email \"you@example.com\"\ngit config --global user.name \"Your Name\"  NOTE : Do the git configuration as a normal user and not with sudo.",
            "title": "Prerequisites"
        },
        {
            "location": "/os/sdk-modifying-coreos/#using-cork",
            "text": "The  cork  utility, included in the Flatcar Container Linux  mantle  project, is used to create and work with an SDK chroot.  First, download the cork utility and verify it with the signature:  curl -L -o cork https://github.com/flatcar-linux/mantle/releases/download/v0.13.2/cork-0.13.2-amd64\ncurl -L -o cork.sig https://github.com/flatcar-linux/mantle/releases/download/v0.13.2/cork-0.13.2-amd64.sig\ngpg --receive-keys 84C8E771C0DF83DFBFCAAAF03ADA89DEC2507883\ngpg --verify cork.sig cork  The  gpg --verify  command should output something like this:  gpg: Signature made Mon 07 Jan 2019 14:51:50 CET\ngpg:                using RSA key 84C8E771C0DF83DFBFCAAAF03ADA89DEC2507883\ngpg: Good signature from \"Flatcar Application Signing Key <buildbot@flatcar-linux.org>\" [unknown]\nPrimary key fingerprint: C1C0 B82A 2F75 90B2 E369  822B E52F 0DB3 9145 3C45\n     Subkey fingerprint: 84C8 E771 C0DF 83DF BFCA  AAF0 3ADA 89DE C250 7883  Then proceed with the installation of the cork binary to a location on your path:  chmod +x cork\nmkdir -p ~/.local/bin\nmv cork ~/.local/bin\nexport PATH=$PATH:$HOME/.local/bin  You may want to add the  PATH  export to your shell profile (e.g.  .bashrc ).  Next, use the cork utility to create a project directory. This will hold all of your git repos and the SDK chroot. A few gigabytes of space will be necessary.  mkdir flatcar-sdk\ncd flatcar-sdk\ncork create # This will request root permisions via sudo\ncork enter # This will request root permisions via sudo  Verify you are in the SDK chroot:  $ grep NAME /etc/os-release\nNAME=\"Flatcar Container Linux by Kinvolk\" \nTo leave the SDK chroot, simply run  exit .  To use the SDK chroot in the future, run  cork enter  from the above directory.",
            "title": "Using Cork"
        },
        {
            "location": "/os/sdk-modifying-coreos/#building-an-image",
            "text": "",
            "title": "Building an image"
        },
        {
            "location": "/os/sdk-modifying-coreos/#set-up-the-chroot",
            "text": "After entering the chroot via  cork  for the first time, you should set user  core 's password:  ./set_shared_user_password.sh  This is the password you will use to log into the console of images built and launched with the SDK.",
            "title": "Set up the chroot"
        },
        {
            "location": "/os/sdk-modifying-coreos/#selecting-the-architecture-to-build",
            "text": "amd64-usr  and  arm64-usr  are the only targets supported by Flatcar.",
            "title": "Selecting the architecture to build"
        },
        {
            "location": "/os/sdk-modifying-coreos/#64-bit-amd-the-amd64-usr-target",
            "text": "The  --board  option can be set to one of a few known target architectures, or system \"boards\", to build for a given CPU.  To create a root filesystem for the  amd64-usr  target beneath the directory  /build/amd64-usr/ :  ./setup_board --default --board=amd64-usr",
            "title": "64 bit AMD: The amd64-usr target"
        },
        {
            "location": "/os/sdk-modifying-coreos/#compile-and-link-system-binaries",
            "text": "Build all of the target binary packages:  ./build_packages",
            "title": "Compile and link system binaries"
        },
        {
            "location": "/os/sdk-modifying-coreos/#render-the-flatcar-container-linux-image",
            "text": "Build a production image based on the binary packages built above:  ./build_image  After  build_image  completes, it prints commands for converting the raw bin into a bootable virtual machine. Run the  image_to_vm.sh  command.",
            "title": "Render the Flatcar Container Linux image"
        },
        {
            "location": "/os/sdk-modifying-coreos/#booting",
            "text": "Once you build an image you can launch it with KVM (instructions will print out after  image_to_vm.sh  runs).  If you encounter errors with KVM, verify that virtualization is supported by your CPU by running  egrep '(vmx|svm)' /proc/cpuinfo . The  /dev/kvm  directory will be in your host OS when virtualization is enabled in the BIOS.  The  ./flatcar_production_qemu.sh  file can be found in the  ~/trunk/src/build/images/amd64-usr/latest  directory inside the SDK chroot.",
            "title": "Booting"
        },
        {
            "location": "/os/sdk-modifying-coreos/#boot-options",
            "text": "After  image_to_vm.sh  completes, run  ./flatcar_production_qemu.sh -curses  to launch a graphical interface to log in to the Flatcar Container Linux VM.  You could instead use the  -nographic  option,  ./flatcar_production_qemu.sh -nographic , which gives you the ability to switch from the VM to the QEMU monitor console by pressing  CTRL + a  and then  c . To close the Flatcar Container Linux Guest OS VM, run  sudo systemctl poweroff  inside the VM.   You could also log in via SSH by running  ./flatcar_production_qemu.sh  and then running  ssh core@127.0.0.1 -p 2222  to enter the guest OS. Running without the  -p 2222  option will arise a  ssh: connect to host 127.0.0.1 port 22: Connection refused  or  Permission denied (publickey,gssapi-keyex,gssapi-with-mic)  warning. Additionally, you can log in via SSH keys or with a different ssh port by running this example  ./flatcar_production_qemu.sh -a ~/.ssh/authorized_keys -p 2223 -- -curses . Refer to the  Booting with QEMU  guide for more information on this usage.  The default login username is  core  and the  password is the one set in the  ./set_shared_user_password  step of this guide. If you forget your password, you will need to rerun  ./set_shared_user_password  and then  ./build_image  again.",
            "title": "Boot Options"
        },
        {
            "location": "/os/sdk-modifying-coreos/#making-changes",
            "text": "",
            "title": "Making changes"
        },
        {
            "location": "/os/sdk-modifying-coreos/#git-and-repo",
            "text": "Flatcar Container Linux is managed by  repo , a tool built for the Android project that makes managing a large number of git repositories easier. From the repo announcement blog:   The repo tool uses an XML-based manifest file describing where the upstream\nrepositories are, and how to merge them into a single working checkout. repo\nwill recurse across all the git subtrees and handle uploads, pulls, and other\nneeded items. repo has built-in knowledge of topic branches and makes working\nwith them an essential part of the workflow.   (from the  Google Open Source Blog )  You can find the full manual for repo by visiting  android.com - Developing .",
            "title": "git and repo"
        },
        {
            "location": "/os/sdk-modifying-coreos/#updating-repo-manifests",
            "text": "The repo manifest for Flatcar Container Linux lives in a git repository in .repo/manifests . If you need to update the manifest edit  default.xml \nin this directory.  repo  uses a branch called 'default' to track the upstream branch you\nspecify in  repo init , this defaults to 'origin/master'. Keep this in\nmind when making changes, the origin git repository should not have a\n'default' branch.",
            "title": "Updating repo manifests"
        },
        {
            "location": "/os/sdk-modifying-coreos/#building-release-images",
            "text": "The  production images  document is unmaintained and out of date, but contains useful pointers as to how official release images are built.",
            "title": "Building release images"
        },
        {
            "location": "/os/sdk-modifying-coreos/#tips-and-tricks",
            "text": "We've compiled a  list of tips and tricks  that can make working with the SDK a bit easier.",
            "title": "Tips and tricks"
        },
        {
            "location": "/os/sdk-modifying-coreos/#testing-images",
            "text": "Mantle  is a collection of utilities used in testing and launching SDK images.",
            "title": "Testing images"
        },
        {
            "location": "/os/sdk-tips-and-tricks/",
            "text": "Tips and tricks\n\u00b6\n\n\nFinding all open pull requests and issues\n\u00b6\n\n\n\n\nFlatcar Container Linux Issues\n\n\nFlatcar Container Linux Pull Requests\n\n\n\n\nSearching all repo code\n\u00b6\n\n\nUsing \nrepo grep\n you can search across all of the Git repos at once:\n\n\nrepo grep CONFIG_EXTRA_FIRMWARE\n\n\n\nNote: this could take some time.\n\n\nBase system dependency graph\n\u00b6\n\n\nGet a view into what the base system will contain and why it will contain those things with the emerge tree view:\n\n\nemerge-amd64-usr --emptytree -p -v --tree coreos-base/coreos-dev\n\n\n\nGet a tree view of the SDK dependencies:\n\n\nemerge-amd64-usr --emptytree -p -v --tree coreos-base/hard-host-depends\n\n\n\nAdd new upstream package\n\u00b6\n\n\nAn overview on contributing new packages to Flatcar Container Linux:\n\n\n\n\ncreate a git branch for the work\n\n\nfetch the the target package(s) from upstream (Gentoo)\n\n\nmake any necessary changes for Flatcar Container Linux\n\n\nadd the package(s) as a dependency of \ncoreos-base/coreos\n\n\nbuild the package(s) and test\n\n\ncommit changes to git\n\n\npush the branch to your GitHub account and create a pull request\n\n\n\n\nSee \nCONTRIBUTING\n for guidelines before you push.\n\n\nThe following Flatcar Container Linux repositories are used:\n\n\n\n\nPackages that will work unmodified are versioned in \nsrc/third_party/portage-stable\n\n\nPackages with Container-Linux-specific changes are versioned in \nsrc/third_party/coreos-overlay\n\n\n\n\nUse \nrepo start\n to create a work branch before making any changes.\n\n\n~/trunk/src/scripts $ repo start my_package_update --all \n\n\n\nYou can use \nscripts/update_ebuilds\n to fetch unmodified packages into \nsrc/third_party/portage-stable\n and add the files to git. The package argument should be in the format of \ncategory/package-name\n, e.g.:\n\n\n~/trunk/src/scripts $ ./update_ebuilds sys-block/open-iscsi\n\n\n\nModified packages must be moved out of \nsrc/third_party/portage-stable\n to \nsrc/third_party/coreos-overlay\n.\n\n\nIf you know in advance that any files in the upstream package will need to be changed, the package can be fetched from upstream Gentoo directly into \nsrc/third_party/coreos-overlay\n. e.g.:\n\n\n~/trunk/src/third_party/coreos-overlay $ mkdir -p sys-block/open-iscsi\n~/trunk/src/third_party/coreos-overlay $ rsync -av rsync://rsync.gentoo.org/gentoo-portage/sys-block/open-iscsi/ sys-block/open-iscsi/\n\n\n\nThe tailing / prevents rsync from creating the directory for the package so you don't end up with \nsys-block/open-iscsi/open-iscsi\n. Remember to add any new files to git.\n\n\nTo quickly test your new package(s), use the following commands:\n\n\n~/trunk/src/scripts $ # Manually merge a package in the chroot\n~/trunk/src/scripts $ emerge-amd64-usr packagename\n~/trunk/src/scripts $ # Manually unmerge a package in the chroot\n~/trunk/src/scripts $ emerge-amd64-usr --unmerge packagename\n~/trunk/src/scripts $ # Remove a binary from the cache\n~/trunk/src/scripts $ sudo rm /build/amd64-usr/packages/category/packagename-version.tbz2\n\n\n\nTo recreate the chroot prior to a clean rebuild, exit the chroot and run:\n\n\n~/flatcar-sdk $ cork create --replace\n\n\n\nTo include the new package as a dependency of Flatcar Container Linux, add the package to the end of the \nRDEPEND\n environment variable in \ncoreos-base/coreos/coreos-0.0.1.ebuild\n then increment the revision of Flatcar Container Linux by renaming the softlink (e.g.):\n\n\n~/trunk/src/third_party/coreos-overly $ git mv coreos-base/coreos/coreos-0.0.1-r237.ebuild coreos-base/coreos/coreos-0.0.1-r238.ebuild\n\n\n\nThe new package will now be built and installed as part of the normal build flow when you run \nbuild_packages\n again.\n\n\nIf tests are successful, commit the changes, push to your GitHub fork and create a pull request.\n\n\nPackaging references\n\u00b6\n\n\nReferences:\n\n\n\n\nChromium OS \nPortage Build FAQ\n\n\nGentoo Development Guide\n\n\nPackage Manager Specification\n\n\n\n\nCreating SDK with different options\n\u00b6\n\n\nTo create SDK from a non-default manifest branch, for example, \nnew-sdk\n:\n\n\n~/flatcar-sdk $ cork create --manifest-branch=new-sdk\n\n\n\nTo create SDK with a non-default SDK version, for example, \n2229.0.0\n:\n\n\n~/flatcar-sdk $ cork create --sdk-version=2229.0.0\n\n\n\nCaching git https passwords\n\u00b6\n\n\nTurn on the credential helper and git will save your password in memory for some time:\n\n\ngit config --global credential.helper cache\n\n\n\nNote: You need git 1.7.10 or newer to use the credential helper\n\n\nWhy doesn't Flatcar Container Linux use SSH in the git remotes?  Because we can't do anonymous clones from GitHub with an SSH URL.  This will be fixed eventually.\n\n\nSSH config\n\u00b6\n\n\nYou will be booting lots of VMs with on the fly ssh key generation. Add this in your \n$HOME/.ssh/config\n to stop the annoying fingerprint warnings.\n\n\nHost 127.0.0.1\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n  User core\n  LogLevel QUIET\n\n\n\nHide loop devices from desktop environments\n\u00b6\n\n\nBy default desktop environments will diligently display any mounted devices including loop devices used to construct Flatcar Container Linux disk images. If the daemon responsible for this happens to be \nudisks\n then you can disable this behavior with the following udev rule:\n\n\necho 'SUBSYSTEM==\"block\", KERNEL==\"ram*|loop*\", ENV{UDISKS_PRESENTATION_HIDE}=\"1\", ENV{UDISKS_PRESENTATION_NOPOLICY}=\"1\"' > /etc/udev/rules.d/85-hide-loop.rules\nudevadm control --reload\n\n\n\nLeaving developer mode\n\u00b6\n\n\nSome daemons act differently in \"dev mode\". For example update_engine refuses to auto-update or connect to HTTPS URLs. If you need to test something out of dev_mode on a vm you can do the following:\n\n\nmv /root/.dev_mode{,.old}\n\n\n\nIf you want to permanently leave you can run the following:\n\n\ncrossystem disable_dev_request=1; reboot\n\n\n\nBuild everything from scratch\n\u00b6\n\n\nIf you want to build everything from scratch, but at the same time want to exclude several packages that take much time.\n\n\nemerge-amd64-usr --emptytree -1 -v --tree --exclude=\"dev-lang/rust sys-devel/gcc\" coreos-base/coreos-dev\n\n\n\nOr if you want to do the rebuild by running \nbuild_packages\n, you should remove the binary package of \ncoreos\n before rebuilding it:\n\n\nemerge-amd64-usr --unmerge coreos-base/coreos\nrm -f /build/amd64-usr/var/lib/portage/pkgs/coreos-base/coreos-0.0.1*.tbz2\n./build_packages\n\n\n\nModify or update invididual packages\n\u00b6\n\n\nBefore or after setting up the SDK with \n./setup_board\n you can modify the package definitions in \nthird_party/coreos-overlay/\n.\nChanges for toolchain packages like the compiler need to be done before running \n./setup_board\n but any changes for the final image\ncan be done before running \n./build_packages && ./build_image\n.\nAll build commands can be run multiple times but whether your last changes are picked up depends on whether the package revision\nwas increased (by renaming the ebuild file) or the package uninstalled and the binary package removed (See the last commands in\n\nBuild everything from scratch\n where it was done for the parent package \ncoreos-base/coreos\n).\nTherefore, we recommend to run every build command only once in a fresh SDK to be sure that your most recent modification is used.\n\n\nFor some packages, like the Linux kernel in \ncoreos-source\n, \ncoreos-kernel\n, and \ncoreos-modules\n, it is enough to rename\nthe ebuild file and it will download a new kernel version.\nEbuilds for other packages under \ncoreos-overlay/\n reference a specific commit in \nCROS_WORKON_COMMIT\n which needs to be changed.\nIf files of a package changed their hash sums, use \nebuild packagename.ebuild manifest\n to recalculate the hashes for\nthe \nManifest\n file.\n\n\nHere is an example of updating an individual package to a newer version:\n\n\ngit mv aaa-bbb/package/package-0.0.1-r1.ebuild aaa-bbb/package/package-0.0.1-r2.ebuild\nebuild aaa-bbb/package/package-0.0.1-r2.ebuild manifest\nemerge-amd64-usr -1 -v aaa-bbb/package\n\n\n\nDo not forget about updating its version and revision in \npackage.accept_keywords\n files in the \nprofiles\n directory.\nIn some cases such a file can pin an exact version of a specific package, which needs to be updated as well.\n\n\nUse binary packages from a shared build store\n\u00b6\n\n\nSome packages like \ncoreos-modules\n take a long time to build. Use \n\n\n./build_packages --getbinpkgver=$(gsutil cat gs://\u2026/boards/amd64-usr/current-master/version.txt |& sed -n 's/^FLATCAR_VERSION=//p')\n\n\n\nto use packages from the another build store.\n\n\nKnown issues\n\u00b6\n\n\nbuild_packages fails on coreos-base\n\u00b6\n\n\nSometimes coreos-dev or coreos builds will fail in \nbuild_packages\n with a backtrace pointing to \nepoll\n. This hasn't been tracked down but running \nbuild_packages\n again should fix it. The error looks something like this:\n\n\nPackages failed:\ncoreos-base/coreos-dev-0.1.0-r63\ncoreos-base/coreos-0.0.1-r187\n\n\n\nNewly added package fails checking for kernel sources\n\u00b6\n\n\nIt may be necessary to comment out kernel source checks from the ebuild if the build fails, as Flatcar Container Linux does not yet provide visibility of the configured kernel source at build time.  Usually this is not a problem, but may lead to warning messages.\n\n\ncoreos-kernel\n fails to link after previously aborting a build\n\u00b6\n\n\nEmerging \ncoreos-kernel\n (either manually or through \nbuild_packages\n) may fail with the error:\n\n\n/usr/lib/gcc/x86_64-pc-linux-gnu/4.9.4/../../../../x86_64-pc-linux-gnu/bin/ld: scripts/kconfig/conf.o: relocation R_X86_64_32 against `.rodata.str1.8' can not be used when making a shared object; recompile with -fPIC scripts/kconfig/conf.o: error adding symbols: Bad value\n\n\n\nThis indicates the ccache is corrupt. To clear the ccache, run:\n\n\nCCACHE_DIR=/var/tmp/ccache ccache -C\n\n\n\nTo avoid corrupting the ccache, do not abort builds.\n\n\nbuild_image\n hangs while emerging packages after previously aborting a build\n\u00b6\n\n\nDelete all \n*.portage_lockfile\ns in \n/build/<arch>/\n. To avoid stale lockfiles, do not abort builds.\n\n\nConstants and IDs\n\u00b6\n\n\nFlatcar Container Linux app ID\n\u00b6\n\n\nThis UUID is used to identify Flatcar Container Linux to the update service and elsewhere.\n\n\ne96281a6-d1af-4bde-9a0a-97b76e56dc57\n\n\n\nGPT UUID types\n\u00b6\n\n\n\n\nFlatcar Container Linux Root: 5dfbf5f4-2848-4bac-aa5e-0d9a20b745a6\n\n\nFlatcar Container Linux Reserved: c95dc21a-df0e-4340-8d7b-26cbfa9a03e0\n\n\nFlatcar Container Linux Raid Containing Root: be9067b9-ea49-4f15-b4f6-f36f8c9e1818",
            "title": "Tips and tricks"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#tips-and-tricks",
            "text": "",
            "title": "Tips and tricks"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#finding-all-open-pull-requests-and-issues",
            "text": "Flatcar Container Linux Issues  Flatcar Container Linux Pull Requests",
            "title": "Finding all open pull requests and issues"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#searching-all-repo-code",
            "text": "Using  repo grep  you can search across all of the Git repos at once:  repo grep CONFIG_EXTRA_FIRMWARE  Note: this could take some time.",
            "title": "Searching all repo code"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#base-system-dependency-graph",
            "text": "Get a view into what the base system will contain and why it will contain those things with the emerge tree view:  emerge-amd64-usr --emptytree -p -v --tree coreos-base/coreos-dev  Get a tree view of the SDK dependencies:  emerge-amd64-usr --emptytree -p -v --tree coreos-base/hard-host-depends",
            "title": "Base system dependency graph"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#add-new-upstream-package",
            "text": "An overview on contributing new packages to Flatcar Container Linux:   create a git branch for the work  fetch the the target package(s) from upstream (Gentoo)  make any necessary changes for Flatcar Container Linux  add the package(s) as a dependency of  coreos-base/coreos  build the package(s) and test  commit changes to git  push the branch to your GitHub account and create a pull request   See  CONTRIBUTING  for guidelines before you push.  The following Flatcar Container Linux repositories are used:   Packages that will work unmodified are versioned in  src/third_party/portage-stable  Packages with Container-Linux-specific changes are versioned in  src/third_party/coreos-overlay   Use  repo start  to create a work branch before making any changes.  ~/trunk/src/scripts $ repo start my_package_update --all   You can use  scripts/update_ebuilds  to fetch unmodified packages into  src/third_party/portage-stable  and add the files to git. The package argument should be in the format of  category/package-name , e.g.:  ~/trunk/src/scripts $ ./update_ebuilds sys-block/open-iscsi  Modified packages must be moved out of  src/third_party/portage-stable  to  src/third_party/coreos-overlay .  If you know in advance that any files in the upstream package will need to be changed, the package can be fetched from upstream Gentoo directly into  src/third_party/coreos-overlay . e.g.:  ~/trunk/src/third_party/coreos-overlay $ mkdir -p sys-block/open-iscsi\n~/trunk/src/third_party/coreos-overlay $ rsync -av rsync://rsync.gentoo.org/gentoo-portage/sys-block/open-iscsi/ sys-block/open-iscsi/  The tailing / prevents rsync from creating the directory for the package so you don't end up with  sys-block/open-iscsi/open-iscsi . Remember to add any new files to git.  To quickly test your new package(s), use the following commands:  ~/trunk/src/scripts $ # Manually merge a package in the chroot\n~/trunk/src/scripts $ emerge-amd64-usr packagename\n~/trunk/src/scripts $ # Manually unmerge a package in the chroot\n~/trunk/src/scripts $ emerge-amd64-usr --unmerge packagename\n~/trunk/src/scripts $ # Remove a binary from the cache\n~/trunk/src/scripts $ sudo rm /build/amd64-usr/packages/category/packagename-version.tbz2  To recreate the chroot prior to a clean rebuild, exit the chroot and run:  ~/flatcar-sdk $ cork create --replace  To include the new package as a dependency of Flatcar Container Linux, add the package to the end of the  RDEPEND  environment variable in  coreos-base/coreos/coreos-0.0.1.ebuild  then increment the revision of Flatcar Container Linux by renaming the softlink (e.g.):  ~/trunk/src/third_party/coreos-overly $ git mv coreos-base/coreos/coreos-0.0.1-r237.ebuild coreos-base/coreos/coreos-0.0.1-r238.ebuild  The new package will now be built and installed as part of the normal build flow when you run  build_packages  again.  If tests are successful, commit the changes, push to your GitHub fork and create a pull request.",
            "title": "Add new upstream package"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#packaging-references",
            "text": "References:   Chromium OS  Portage Build FAQ  Gentoo Development Guide  Package Manager Specification",
            "title": "Packaging references"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#creating-sdk-with-different-options",
            "text": "To create SDK from a non-default manifest branch, for example,  new-sdk :  ~/flatcar-sdk $ cork create --manifest-branch=new-sdk  To create SDK with a non-default SDK version, for example,  2229.0.0 :  ~/flatcar-sdk $ cork create --sdk-version=2229.0.0",
            "title": "Creating SDK with different options"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#caching-git-https-passwords",
            "text": "Turn on the credential helper and git will save your password in memory for some time:  git config --global credential.helper cache  Note: You need git 1.7.10 or newer to use the credential helper  Why doesn't Flatcar Container Linux use SSH in the git remotes?  Because we can't do anonymous clones from GitHub with an SSH URL.  This will be fixed eventually.",
            "title": "Caching git https passwords"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#ssh-config",
            "text": "You will be booting lots of VMs with on the fly ssh key generation. Add this in your  $HOME/.ssh/config  to stop the annoying fingerprint warnings.  Host 127.0.0.1\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n  User core\n  LogLevel QUIET",
            "title": "SSH config"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#hide-loop-devices-from-desktop-environments",
            "text": "By default desktop environments will diligently display any mounted devices including loop devices used to construct Flatcar Container Linux disk images. If the daemon responsible for this happens to be  udisks  then you can disable this behavior with the following udev rule:  echo 'SUBSYSTEM==\"block\", KERNEL==\"ram*|loop*\", ENV{UDISKS_PRESENTATION_HIDE}=\"1\", ENV{UDISKS_PRESENTATION_NOPOLICY}=\"1\"' > /etc/udev/rules.d/85-hide-loop.rules\nudevadm control --reload",
            "title": "Hide loop devices from desktop environments"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#leaving-developer-mode",
            "text": "Some daemons act differently in \"dev mode\". For example update_engine refuses to auto-update or connect to HTTPS URLs. If you need to test something out of dev_mode on a vm you can do the following:  mv /root/.dev_mode{,.old}  If you want to permanently leave you can run the following:  crossystem disable_dev_request=1; reboot",
            "title": "Leaving developer mode"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#build-everything-from-scratch",
            "text": "If you want to build everything from scratch, but at the same time want to exclude several packages that take much time.  emerge-amd64-usr --emptytree -1 -v --tree --exclude=\"dev-lang/rust sys-devel/gcc\" coreos-base/coreos-dev  Or if you want to do the rebuild by running  build_packages , you should remove the binary package of  coreos  before rebuilding it:  emerge-amd64-usr --unmerge coreos-base/coreos\nrm -f /build/amd64-usr/var/lib/portage/pkgs/coreos-base/coreos-0.0.1*.tbz2\n./build_packages",
            "title": "Build everything from scratch"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#modify-or-update-invididual-packages",
            "text": "Before or after setting up the SDK with  ./setup_board  you can modify the package definitions in  third_party/coreos-overlay/ .\nChanges for toolchain packages like the compiler need to be done before running  ./setup_board  but any changes for the final image\ncan be done before running  ./build_packages && ./build_image .\nAll build commands can be run multiple times but whether your last changes are picked up depends on whether the package revision\nwas increased (by renaming the ebuild file) or the package uninstalled and the binary package removed (See the last commands in Build everything from scratch  where it was done for the parent package  coreos-base/coreos ).\nTherefore, we recommend to run every build command only once in a fresh SDK to be sure that your most recent modification is used.  For some packages, like the Linux kernel in  coreos-source ,  coreos-kernel , and  coreos-modules , it is enough to rename\nthe ebuild file and it will download a new kernel version.\nEbuilds for other packages under  coreos-overlay/  reference a specific commit in  CROS_WORKON_COMMIT  which needs to be changed.\nIf files of a package changed their hash sums, use  ebuild packagename.ebuild manifest  to recalculate the hashes for\nthe  Manifest  file.  Here is an example of updating an individual package to a newer version:  git mv aaa-bbb/package/package-0.0.1-r1.ebuild aaa-bbb/package/package-0.0.1-r2.ebuild\nebuild aaa-bbb/package/package-0.0.1-r2.ebuild manifest\nemerge-amd64-usr -1 -v aaa-bbb/package  Do not forget about updating its version and revision in  package.accept_keywords  files in the  profiles  directory.\nIn some cases such a file can pin an exact version of a specific package, which needs to be updated as well.",
            "title": "Modify or update invididual packages"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#use-binary-packages-from-a-shared-build-store",
            "text": "Some packages like  coreos-modules  take a long time to build. Use   ./build_packages --getbinpkgver=$(gsutil cat gs://\u2026/boards/amd64-usr/current-master/version.txt |& sed -n 's/^FLATCAR_VERSION=//p')  to use packages from the another build store.",
            "title": "Use binary packages from a shared build store"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#known-issues",
            "text": "",
            "title": "Known issues"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#build95packages-fails-on-coreos-base",
            "text": "Sometimes coreos-dev or coreos builds will fail in  build_packages  with a backtrace pointing to  epoll . This hasn't been tracked down but running  build_packages  again should fix it. The error looks something like this:  Packages failed:\ncoreos-base/coreos-dev-0.1.0-r63\ncoreos-base/coreos-0.0.1-r187",
            "title": "build_packages fails on coreos-base"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#newly-added-package-fails-checking-for-kernel-sources",
            "text": "It may be necessary to comment out kernel source checks from the ebuild if the build fails, as Flatcar Container Linux does not yet provide visibility of the configured kernel source at build time.  Usually this is not a problem, but may lead to warning messages.",
            "title": "Newly added package fails checking for kernel sources"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#coreos-kernel-fails-to-link-after-previously-aborting-a-build",
            "text": "Emerging  coreos-kernel  (either manually or through  build_packages ) may fail with the error:  /usr/lib/gcc/x86_64-pc-linux-gnu/4.9.4/../../../../x86_64-pc-linux-gnu/bin/ld: scripts/kconfig/conf.o: relocation R_X86_64_32 against `.rodata.str1.8' can not be used when making a shared object; recompile with -fPIC scripts/kconfig/conf.o: error adding symbols: Bad value  This indicates the ccache is corrupt. To clear the ccache, run:  CCACHE_DIR=/var/tmp/ccache ccache -C  To avoid corrupting the ccache, do not abort builds.",
            "title": "coreos-kernel fails to link after previously aborting a build"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#build_image-hangs-while-emerging-packages-after-previously-aborting-a-build",
            "text": "Delete all  *.portage_lockfile s in  /build/<arch>/ . To avoid stale lockfiles, do not abort builds.",
            "title": "build_image hangs while emerging packages after previously aborting a build"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#constants-and-ids",
            "text": "",
            "title": "Constants and IDs"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#flatcar-container-linux-app-id",
            "text": "This UUID is used to identify Flatcar Container Linux to the update service and elsewhere.  e96281a6-d1af-4bde-9a0a-97b76e56dc57",
            "title": "Flatcar Container Linux app ID"
        },
        {
            "location": "/os/sdk-tips-and-tricks/#gpt-uuid-types",
            "text": "Flatcar Container Linux Root: 5dfbf5f4-2848-4bac-aa5e-0d9a20b745a6  Flatcar Container Linux Reserved: c95dc21a-df0e-4340-8d7b-26cbfa9a03e0  Flatcar Container Linux Raid Containing Root: be9067b9-ea49-4f15-b4f6-f36f8c9e1818",
            "title": "GPT UUID types"
        },
        {
            "location": "/os/selinux/",
            "text": "SELinux on Flatcar Container Linux\n\u00b6\n\n\nSELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux and rkt. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.\n\n\nFlatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. This allows deployers to verify container operation before enabling SELinux enforcement. This document covers the process of checking containers for SELinux policy compatibility, and switching SELinux into \nenforcing\n mode.\n\n\nCheck a container's compatibility with SELinux policy\n\u00b6\n\n\nTo verify whether the current SELinux policy would inhibit your containers, enable SELinux logging. In the following set of commands, we delete the rules that suppress this logging by default, and copy the policy store from Flatcar Container Linux's read-only \n/usr\n to a writable file system location.\n\n\n$ rm /etc/audit/rules.d/80-selinux.rules\n$ rm /etc/audit/rules.d/99-default.rules\n$ rm /etc/selinux/mcs\n$ cp -a /usr/lib/selinux/mcs /etc/selinux\n$ rm /var/lib/selinux\n$ cp -a /usr/lib/selinux/policy /var/lib/selinux\n$ semodule -DB\n$ systemctl restart audit-rules\n\n\n\nNow run your container. Check the system logs for any messages containing \navc: denied\n. Such messages indicate that an \nenforcing\n SELinux would prevent the container from performing the logged operation. Please open an issue at \nflatcar-linux/Flatcar\n, including the full avc log message.\n\n\nEnable SELinux enforcement\n\u00b6\n\n\nOnce satisfied that your container workload is compatible with the SELinux policy, you can temporarily enable enforcement by running the following command as root:\n\n\n$ setenforce 1\n\n\nA reboot will reset SELinux to \npermissive\n mode.\n\n\nMake SELinux enforcement permanent\n\u00b6\n\n\nTo enable SELinux enforcement across reboots, replace the symbolic link \n/etc/selinux/config\n with the file it targets, so that the file can be written. You can use the \nreadlink\n command to dereference the link, as shown in the following one-liner:\n\n\n$ cp --remove-destination $(readlink -f /etc/selinux/config) /etc/selinux/config\n\n\nNow, edit \n/etc/selinux/config\n to replace \nSELINUX=permissive\n with \nSELINUX=enforcing\n.\n\n\nLimitations\n\u00b6\n\n\nSELinux enforcement is currently incompatible with Btrfs volumes and volumes that are shared between multiple containers.",
            "title": "SELinux on Flatcar Container Linux"
        },
        {
            "location": "/os/selinux/#selinux-on-flatcar-container-linux",
            "text": "SELinux is a fine-grained access control mechanism integrated into Flatcar Container Linux and rkt. Each container runs in its own independent SELinux context, increasing isolation between containers and providing another layer of protection should a container be compromised.  Flatcar Container Linux implements SELinux, but currently does not enforce SELinux protections by default. This allows deployers to verify container operation before enabling SELinux enforcement. This document covers the process of checking containers for SELinux policy compatibility, and switching SELinux into  enforcing  mode.",
            "title": "SELinux on Flatcar Container Linux"
        },
        {
            "location": "/os/selinux/#check-a-containers-compatibility-with-selinux-policy",
            "text": "To verify whether the current SELinux policy would inhibit your containers, enable SELinux logging. In the following set of commands, we delete the rules that suppress this logging by default, and copy the policy store from Flatcar Container Linux's read-only  /usr  to a writable file system location.  $ rm /etc/audit/rules.d/80-selinux.rules\n$ rm /etc/audit/rules.d/99-default.rules\n$ rm /etc/selinux/mcs\n$ cp -a /usr/lib/selinux/mcs /etc/selinux\n$ rm /var/lib/selinux\n$ cp -a /usr/lib/selinux/policy /var/lib/selinux\n$ semodule -DB\n$ systemctl restart audit-rules  Now run your container. Check the system logs for any messages containing  avc: denied . Such messages indicate that an  enforcing  SELinux would prevent the container from performing the logged operation. Please open an issue at  flatcar-linux/Flatcar , including the full avc log message.",
            "title": "Check a container's compatibility with SELinux policy"
        },
        {
            "location": "/os/selinux/#enable-selinux-enforcement",
            "text": "Once satisfied that your container workload is compatible with the SELinux policy, you can temporarily enable enforcement by running the following command as root:  $ setenforce 1  A reboot will reset SELinux to  permissive  mode.",
            "title": "Enable SELinux enforcement"
        },
        {
            "location": "/os/selinux/#make-selinux-enforcement-permanent",
            "text": "To enable SELinux enforcement across reboots, replace the symbolic link  /etc/selinux/config  with the file it targets, so that the file can be written. You can use the  readlink  command to dereference the link, as shown in the following one-liner:  $ cp --remove-destination $(readlink -f /etc/selinux/config) /etc/selinux/config  Now, edit  /etc/selinux/config  to replace  SELINUX=permissive  with  SELINUX=enforcing .",
            "title": "Make SELinux enforcement permanent"
        },
        {
            "location": "/os/selinux/#limitations",
            "text": "SELinux enforcement is currently incompatible with Btrfs volumes and volumes that are shared between multiple containers.",
            "title": "Limitations"
        },
        {
            "location": "/os/disabling-smt/",
            "text": "Disabling SMT on Flatcar Container Linux\n\u00b6\n\n\nRecent Intel CPU vulnerabilities (\nL1TF\n and \nMDS\n) cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.\n\n\nIn addition, the Intel \nTAA\n vulnerability cannot be fully mitigated without disabling either of SMT or the Transactional Synchronization Extensions (TSX). Disabling TSX generally has less performance impact, so is the preferred approach on systems that don't otherwise need to disable SMT. For compatibility reasons, TSX is enabled by default.\n\n\nSMT and TSX should be disabled on affected Intel processors under the following circumstances:\n1. A bare-metal host runs untrusted virtual machines, and \nother arrangements\n have not been made for mitigation.\n2. A bare-metal host runs untrusted code outside a virtual machine.\n\n\nSMT can be conditionally disabled by passing \nmitigations=auto,nosmt\n on the kernel command line. This will disable SMT only if required for mitigating a vulnerability. This approach has two caveats:\n1. It does not protect against unknown vulnerabilities in SMT.\n2. It allows future Flatcar Container Linux updates to disable SMT if needed to mitigate new vulnerabilities.\n\n\nAlternatively, SMT can be unconditionally disabled by passing \nnosmt\n on the kernel command line. This provides the most protection and avoids possible behavior changes on upgrades, at the cost of a potentially unnecessary reduction in performance.\n\n\nTSX can be conditionally disabled on vulnerable CPUs by passing \ntsx=auto\n on the kernel command line, or unconditionally disabled by passing \ntsx=off\n. However, neither setting takes effect on systems affected by MDS, since MDS mitigation automatically protects against TAA as well.\n\n\nFor typical use cases, we recommend enabling the \nmitigations=auto,nosmt\n and \ntsx=auto\n command-line options.\n\n\nConfiguring new machines\n\u00b6\n\n\nThe following Container Linux Config performs two tasks:\n\n\n\n\nAdds \nmitigations=auto,nosmt tsx=auto\n to the kernel command line. This affects the second and subsequent boots of the machine, but not the first boot.\n\n\nOn the first boot, disables SMT at runtime if the system has an Intel processor. This is sufficient to protect against currently-known SMT vulnerabilities until the system is rebooted. After reboot, SMT will be re-enabled if the processor is not actually vulnerable.\n\n\n\n\n# Add kernel command-line arguments to automatically disable SMT or TSX\n# on CPUs where they are vulnerable.  This will affect the second and\n# subsequent boots of the machine, but not the first boot.\nstorage:\n  filesystems:\n    - name: OEM\n      mount:\n        device: /dev/disk/by-label/OEM\n        format: ext4\n  files:\n    - filesystem: OEM\n      path: /grub.cfg\n      append: true\n      mode: 0644\n      contents:\n        inline: |\n          # Disable SMT on CPUs affected by MDS or similar vulnerabilities.\n          # Disable TSX on CPUs affected by TAA but not by MDS.\n          set linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"\n\n# On the first boot only, disable SMT at runtime if it is enabled and\n# the system has an Intel CPU.  L1TF, MDS, and TAA vulnerabilities are\n# limited to Intel CPUs.\nsystemd:\n  units:\n    - name: disable-smt-firstboot.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Disable SMT on first boot on Intel CPUs to mitigate MDS\n        DefaultDependencies=no\n        Before=sysinit.target shutdown.target\n        Conflicts=shutdown.target\n        ConditionFirstBoot=true\n\n        [Service]\n        Type=oneshot\n        ExecStart=/bin/bash -c 'active=\"$(cat /sys/devices/system/cpu/smt/active)\" && if [[ \"$active\" != 0 ]] && grep -q \"vendor_id.*GenuineIntel\" /proc/cpuinfo; then echo \"Disabling SMT.\" && echo off > /sys/devices/system/cpu/smt/control; fi'\n\n        [Install]\n        WantedBy=sysinit.target\n\n\n\nConfiguring existing machines\n\u00b6\n\n\nTo add \nmitigations=auto,nosmt tsx=auto\n to the kernel command line on an existing system, add the following line to \n/usr/share/oem/grub.cfg\n:\n\n\nset linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"\n\n\n\nFor example, using SSH:\n\n\nssh core@node01 'sudo sh -c \"echo \\\"set linux_append=\\\\\\\"\\\\\\$linux_append mitigations=auto,nosmt tsx=auto\\\\\\\"\\\" >> /usr/share/oem/grub.cfg && systemctl reboot\"'\n\n\n\nIf you use locksmith for reboot coordination, replace \nsystemctl reboot\n with \nlocksmithctl send-need-reboot\n.",
            "title": "Disabling SMT on Flatcar Container Linux"
        },
        {
            "location": "/os/disabling-smt/#disabling-smt-on-flatcar-container-linux",
            "text": "Recent Intel CPU vulnerabilities ( L1TF  and  MDS ) cannot be fully mitigated in software without disabling Simultaneous Multi-Threading. This can have a substantial performance impact and is only necessary for certain workloads, so for compatibility reasons, SMT is enabled by default.  In addition, the Intel  TAA  vulnerability cannot be fully mitigated without disabling either of SMT or the Transactional Synchronization Extensions (TSX). Disabling TSX generally has less performance impact, so is the preferred approach on systems that don't otherwise need to disable SMT. For compatibility reasons, TSX is enabled by default.  SMT and TSX should be disabled on affected Intel processors under the following circumstances:\n1. A bare-metal host runs untrusted virtual machines, and  other arrangements  have not been made for mitigation.\n2. A bare-metal host runs untrusted code outside a virtual machine.  SMT can be conditionally disabled by passing  mitigations=auto,nosmt  on the kernel command line. This will disable SMT only if required for mitigating a vulnerability. This approach has two caveats:\n1. It does not protect against unknown vulnerabilities in SMT.\n2. It allows future Flatcar Container Linux updates to disable SMT if needed to mitigate new vulnerabilities.  Alternatively, SMT can be unconditionally disabled by passing  nosmt  on the kernel command line. This provides the most protection and avoids possible behavior changes on upgrades, at the cost of a potentially unnecessary reduction in performance.  TSX can be conditionally disabled on vulnerable CPUs by passing  tsx=auto  on the kernel command line, or unconditionally disabled by passing  tsx=off . However, neither setting takes effect on systems affected by MDS, since MDS mitigation automatically protects against TAA as well.  For typical use cases, we recommend enabling the  mitigations=auto,nosmt  and  tsx=auto  command-line options.",
            "title": "Disabling SMT on Flatcar Container Linux"
        },
        {
            "location": "/os/disabling-smt/#configuring-new-machines",
            "text": "The following Container Linux Config performs two tasks:   Adds  mitigations=auto,nosmt tsx=auto  to the kernel command line. This affects the second and subsequent boots of the machine, but not the first boot.  On the first boot, disables SMT at runtime if the system has an Intel processor. This is sufficient to protect against currently-known SMT vulnerabilities until the system is rebooted. After reboot, SMT will be re-enabled if the processor is not actually vulnerable.   # Add kernel command-line arguments to automatically disable SMT or TSX\n# on CPUs where they are vulnerable.  This will affect the second and\n# subsequent boots of the machine, but not the first boot.\nstorage:\n  filesystems:\n    - name: OEM\n      mount:\n        device: /dev/disk/by-label/OEM\n        format: ext4\n  files:\n    - filesystem: OEM\n      path: /grub.cfg\n      append: true\n      mode: 0644\n      contents:\n        inline: |\n          # Disable SMT on CPUs affected by MDS or similar vulnerabilities.\n          # Disable TSX on CPUs affected by TAA but not by MDS.\n          set linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"\n\n# On the first boot only, disable SMT at runtime if it is enabled and\n# the system has an Intel CPU.  L1TF, MDS, and TAA vulnerabilities are\n# limited to Intel CPUs.\nsystemd:\n  units:\n    - name: disable-smt-firstboot.service\n      enabled: true\n      contents: |\n        [Unit]\n        Description=Disable SMT on first boot on Intel CPUs to mitigate MDS\n        DefaultDependencies=no\n        Before=sysinit.target shutdown.target\n        Conflicts=shutdown.target\n        ConditionFirstBoot=true\n\n        [Service]\n        Type=oneshot\n        ExecStart=/bin/bash -c 'active=\"$(cat /sys/devices/system/cpu/smt/active)\" && if [[ \"$active\" != 0 ]] && grep -q \"vendor_id.*GenuineIntel\" /proc/cpuinfo; then echo \"Disabling SMT.\" && echo off > /sys/devices/system/cpu/smt/control; fi'\n\n        [Install]\n        WantedBy=sysinit.target",
            "title": "Configuring new machines"
        },
        {
            "location": "/os/disabling-smt/#configuring-existing-machines",
            "text": "To add  mitigations=auto,nosmt tsx=auto  to the kernel command line on an existing system, add the following line to  /usr/share/oem/grub.cfg :  set linux_append=\"$linux_append mitigations=auto,nosmt tsx=auto\"  For example, using SSH:  ssh core@node01 'sudo sh -c \"echo \\\"set linux_append=\\\\\\\"\\\\\\$linux_append mitigations=auto,nosmt tsx=auto\\\\\\\"\\\" >> /usr/share/oem/grub.cfg && systemctl reboot\"'  If you use locksmith for reboot coordination, replace  systemctl reboot  with  locksmithctl send-need-reboot .",
            "title": "Configuring existing machines"
        },
        {
            "location": "/os/sssd/",
            "text": "Configuring SSSD on Flatcar Container Linux\n\u00b6\n\n\nFlatcar Container Linux ships with the System Security Services Daemon, allowing integration between Flatcar Container Linux and enterprise authentication services.\n\n\nConfiguring SSSD\n\u00b6\n\n\nEdit /etc/sssd/sssd.conf. This configuration file is fully documented \nhere\n. For example, to configure SSSD to use an IPA server called ipa.example.com, sssd.conf should read:\n\n\n[sssd]\nconfig_file_version = 2\nservices = nss, pam\ndomains = LDAP\n[nss]\n[pam]\n[domain/LDAP]\nid_provider = ldap\nauth_provider = ldap\nldap_schema = ipa\nldap_uri = ldap://ipa.example.com\n\n\n\nStart SSSD\n\u00b6\n\n\nsudo systemctl start sssd\n\n\n\nMake SSSD available on future reboots\n\u00b6\n\n\nsudo systemctl enable sssd",
            "title": "Configuring SSSD on Flatcar Container Linux"
        },
        {
            "location": "/os/sssd/#configuring-sssd-on-flatcar-container-linux",
            "text": "Flatcar Container Linux ships with the System Security Services Daemon, allowing integration between Flatcar Container Linux and enterprise authentication services.",
            "title": "Configuring SSSD on Flatcar Container Linux"
        },
        {
            "location": "/os/sssd/#configuring-sssd",
            "text": "Edit /etc/sssd/sssd.conf. This configuration file is fully documented  here . For example, to configure SSSD to use an IPA server called ipa.example.com, sssd.conf should read:  [sssd]\nconfig_file_version = 2\nservices = nss, pam\ndomains = LDAP\n[nss]\n[pam]\n[domain/LDAP]\nid_provider = ldap\nauth_provider = ldap\nldap_schema = ipa\nldap_uri = ldap://ipa.example.com",
            "title": "Configuring SSSD"
        },
        {
            "location": "/os/sssd/#start-sssd",
            "text": "sudo systemctl start sssd",
            "title": "Start SSSD"
        },
        {
            "location": "/os/sssd/#make-sssd-available-on-future-reboots",
            "text": "sudo systemctl enable sssd",
            "title": "Make SSSD available on future reboots"
        },
        {
            "location": "/os/switching-channels/",
            "text": "Switching release channels\n\u00b6\n\n\nFlatcar Container Linux is designed to be updated automatically with different schedules per channel. You can \ndisable this feature\n, although we don't recommend it. Read the \nrelease notes\n for specific features and bug fixes.\n\n\nBy design, the Flatcar Container Linux update engine does not execute downgrades. If you're switching from a channel with a higher Flatcar Container Linux version than the new channel, your machine won't be updated again until the new channel contains a higher version number.\n\n\n\n\nCustomizing channel configuration\n\u00b6\n\n\nThe update engine sources its configuration from \n/usr/share/flatcar/update.conf\n and \n/etc/flatcar/update.conf\n.\nThe former file contains the default hardcoded configuration from the running OS version. Its values cannot be edited, but they can be overridden by the ones in the latter file.\n\n\nTo switch a machine to a different channel, specify the new channel group in \n/etc/flatcar/update.conf\n:\n\n\nGROUP=beta\n\n\n\nIn order for the configuration override to take effect, the update engine must first be restarted:\n\n\nsudo systemctl restart update-engine\n\n\n\nDebugging\n\u00b6\n\n\nAfter the update engine is restarted, the machine should check for an update within an hour.\n\n\nThe live status of updates checking can queried via:\n\n\nupdate_engine_client --status\n\n\n\nThe update engine logs all update attempts, which can inspected in the system journal:\n\n\njournalctl -f -u update-engine\n\n\n\nFor reference, the OS version and channel for a running system can be determined via:\n\n\ncat /usr/share/flatcar/os-release\n\ncat /usr/share/flatcar/update.conf\n\n\n\nNote: while a manual channel switch is in progress, \n/usr/share/flatcar/update.conf\n shows the channel for the current OS while \n/etc/flatcar/update.conf\n shows the one for the next update.",
            "title": "Switching release channels"
        },
        {
            "location": "/os/switching-channels/#switching-release-channels",
            "text": "Flatcar Container Linux is designed to be updated automatically with different schedules per channel. You can  disable this feature , although we don't recommend it. Read the  release notes  for specific features and bug fixes.  By design, the Flatcar Container Linux update engine does not execute downgrades. If you're switching from a channel with a higher Flatcar Container Linux version than the new channel, your machine won't be updated again until the new channel contains a higher version number.",
            "title": "Switching release channels"
        },
        {
            "location": "/os/switching-channels/#customizing-channel-configuration",
            "text": "The update engine sources its configuration from  /usr/share/flatcar/update.conf  and  /etc/flatcar/update.conf .\nThe former file contains the default hardcoded configuration from the running OS version. Its values cannot be edited, but they can be overridden by the ones in the latter file.  To switch a machine to a different channel, specify the new channel group in  /etc/flatcar/update.conf :  GROUP=beta  In order for the configuration override to take effect, the update engine must first be restarted:  sudo systemctl restart update-engine",
            "title": "Customizing channel configuration"
        },
        {
            "location": "/os/switching-channels/#debugging",
            "text": "After the update engine is restarted, the machine should check for an update within an hour.  The live status of updates checking can queried via:  update_engine_client --status  The update engine logs all update attempts, which can inspected in the system journal:  journalctl -f -u update-engine  For reference, the OS version and channel for a running system can be determined via:  cat /usr/share/flatcar/os-release\n\ncat /usr/share/flatcar/update.conf  Note: while a manual channel switch is in progress,  /usr/share/flatcar/update.conf  shows the channel for the current OS while  /etc/flatcar/update.conf  shows the one for the next update.",
            "title": "Debugging"
        },
        {
            "location": "/os/torcx-metadata-and-systemd-target/",
            "text": "Torcx metadata and systemd target\n\u00b6\n\n\nIn many cases, it is desirable to inspect the state of a system booted with Torcx and to verify the details of the configuration that has been applied.\nFor this purpose, Torcx comes with additional facilities to integrate with systemd-based workflows: a custom target and a metadata file containing environment flags.\n\n\nMetadata entries and environment flags\n\u00b6\n\n\nIn order to signal a successful run, Torcx writes a metadata file at most once per boot. The format of this file is suitable for consumption by the systemd \nEnvironmentFile=\n \ndirective\n and can be used to introspect the booted configuration at runtime.\n\n\nThe metadata file is written to \n/run/metadata/torcx\n and contains a list of key-value pairs:\n\n\n$ cat /run/metadata/torcx\n\nTORCX_LOWER_PROFILES=\"vendor\"\nTORCX_UPPER_PROFILE=\"custom-demo\"\nTORCX_PROFILE_PATH=\"/run/torcx/profile.json\"\nTORCX_BINDIR=\"/run/torcx/bin\"\nTORCX_UNPACKDIR=\"/run/torcx/unpack\"\n\n\n\nThese values can be used to detect where assets have been unpacked and propagated (shown above as \"unpack\" and \"bin\" entries), which profiles have been sourced (both vendor- and user-provided), and what is the resulting profile that has been applied.\n\n\nFinally, the runtime profile can be inspected to detect which addons (and versions) are currently applied:\n\n\n$ cat /run/torcx/profile.json\n\n{\n  \"kind\": \"profile-manifest-v0\",\n  \"value\": {\n    \"images\": []\n  }\n}\n\n\n\nTorcx target unit\n\u00b6\n\n\nSystem services may depend on successful execution of Torcx generator. As such, \ntorcx.target\n is provided as a target unit which is only reachable if the generator successfully ran and sealed the system.\n\n\nThis target is not enabled by default, but can be referenced as a dependency by other units who want to introspect system status:\n\n\n$ sudo systemctl cat torcx-echo.service\n\n[Unit]\nDescription=Sample unit relying on torcx run\nAfter=torcx.target\nRequires=torcx.target\n\n[Service]\nEnvironmentFile=/run/metadata/torcx\nType=oneshot\nExecStart=/usr/bin/echo \"torcx: applied ${TORCX_UPPER_PROFILE}\"\n\n[Install]\nWantedBy=multi-user.target\n\n\n\n$ sudo systemctl status torcx.target\n\n\u25cf torcx.target - Verify torcx succeeded\n   Loaded: loaded (/usr/lib/systemd/system/torcx.target; disabled; vendor preset: disabled)\n   Active: active since [...]\n\n\n\n$ sudo journalctl -u torcx-echo.service\n\nlocalhost systemd[1]: Starting Sample unit relying on torcx run...\nlocalhost echo[756]: torcx: applied custom-demo\nlocalhost systemd[1]: Started Sample unit relying on torcx run.",
            "title": "Torcx metadata and systemd target"
        },
        {
            "location": "/os/torcx-metadata-and-systemd-target/#torcx-metadata-and-systemd-target",
            "text": "In many cases, it is desirable to inspect the state of a system booted with Torcx and to verify the details of the configuration that has been applied.\nFor this purpose, Torcx comes with additional facilities to integrate with systemd-based workflows: a custom target and a metadata file containing environment flags.",
            "title": "Torcx metadata and systemd target"
        },
        {
            "location": "/os/torcx-metadata-and-systemd-target/#metadata-entries-and-environment-flags",
            "text": "In order to signal a successful run, Torcx writes a metadata file at most once per boot. The format of this file is suitable for consumption by the systemd  EnvironmentFile=   directive  and can be used to introspect the booted configuration at runtime.  The metadata file is written to  /run/metadata/torcx  and contains a list of key-value pairs:  $ cat /run/metadata/torcx\n\nTORCX_LOWER_PROFILES=\"vendor\"\nTORCX_UPPER_PROFILE=\"custom-demo\"\nTORCX_PROFILE_PATH=\"/run/torcx/profile.json\"\nTORCX_BINDIR=\"/run/torcx/bin\"\nTORCX_UNPACKDIR=\"/run/torcx/unpack\"  These values can be used to detect where assets have been unpacked and propagated (shown above as \"unpack\" and \"bin\" entries), which profiles have been sourced (both vendor- and user-provided), and what is the resulting profile that has been applied.  Finally, the runtime profile can be inspected to detect which addons (and versions) are currently applied:  $ cat /run/torcx/profile.json\n\n{\n  \"kind\": \"profile-manifest-v0\",\n  \"value\": {\n    \"images\": []\n  }\n}",
            "title": "Metadata entries and environment flags"
        },
        {
            "location": "/os/torcx-metadata-and-systemd-target/#torcx-target-unit",
            "text": "System services may depend on successful execution of Torcx generator. As such,  torcx.target  is provided as a target unit which is only reachable if the generator successfully ran and sealed the system.  This target is not enabled by default, but can be referenced as a dependency by other units who want to introspect system status:  $ sudo systemctl cat torcx-echo.service\n\n[Unit]\nDescription=Sample unit relying on torcx run\nAfter=torcx.target\nRequires=torcx.target\n\n[Service]\nEnvironmentFile=/run/metadata/torcx\nType=oneshot\nExecStart=/usr/bin/echo \"torcx: applied ${TORCX_UPPER_PROFILE}\"\n\n[Install]\nWantedBy=multi-user.target  $ sudo systemctl status torcx.target\n\n\u25cf torcx.target - Verify torcx succeeded\n   Loaded: loaded (/usr/lib/systemd/system/torcx.target; disabled; vendor preset: disabled)\n   Active: active since [...]  $ sudo journalctl -u torcx-echo.service\n\nlocalhost systemd[1]: Starting Sample unit relying on torcx run...\nlocalhost echo[756]: torcx: applied custom-demo\nlocalhost systemd[1]: Started Sample unit relying on torcx run.",
            "title": "Torcx target unit"
        },
        {
            "location": "/os/torcx-overview/",
            "text": "What is Torcx?\n\u00b6\n\n\nTorcx\n is a new boot-time addon manager designed specifically for CoreOS Container Linux. At the most basic level, it is a tool for applying ephemeral changes to an immutable system during early boot. This includes providing third-party binary addons and installing systemd units, which can vary across environments and boots. On every boot, Torcx reads its configuration from local disk and propagates specific assets provided by addon packages (which must be available in local stores).\n\n\nTorcx overview\n\u00b6\n\n\nTorcx complements both the \nIgnition\n provisioning utility and \nsystemd\n. Torcx allows customization of Flatcar Container Linux systems without requiring the compilation of custom system images. This goal is achieved by following two main principles: customizations are ephemeral, and they are applied exactly once per boot. Torcx also has a very simple design, with the aim of providing a small low-level system utility which can be driven by more advanced and higher-level tools.\n\n\nTorcx execution model and systemd generators\n\u00b6\n\n\nEarly in the boot process, execution starts in a minimal initramfs environment where systemd, Ignition, and other boot utilities run. Once up, execution continues by pivoting into the real root file system and by running all \nsystemd generators\n, including the main torcx component, \ntorcx-generator\n.\n\ntorcx-generator\n runs serially before any other service starts to guarantee it does not race with other startup processes. However, this restricts Torcx to using only local resources. Torcx cannot access configuration or addons from remote file systems or network locations.\n\n\nProfiles and addons\n\u00b6\n\n\nTorcx customizations are applied via local addon packages, which are referenced by profiles. Addons are simple tar-gzipped archives containing binary assets and a manifest. A user profile (upper profile) can be supplied by the administrator to be merged on top of hard-coded vendor and OEM profiles (lower profiles). Torcx will take care of computing and applying the resulting list of addons on the system.\n\n\nBoot-time customizations\n\u00b6\n\n\nTorcx guarantees that customizations are applied at most once per boot, before any other service has been considered for startup. This provides a mechanism to customize most aspects of a Flatcar Container Linux system in a reliable way, and avoids runtime upgrading/downgrading issues. Changes applied by Torcx are not persisted to disk, and therefore last exactly for the lifetime of a single boot of an instance.\n\n\nBy the same token, this should be read as a warning against abusing Torcx in the role of a general purpose container, service, or package manager. Torcx's boot-transient model consumes memory with each addon, and, worse, would require system reboots for even simple upgrades.\n\n\nFurther design details\n\u00b6\n\n\nFor further details on design and goals, Torcx repository contains extensive \ndeveloper documentation\n.",
            "title": "What is Torcx?"
        },
        {
            "location": "/os/torcx-overview/#what-is-torcx",
            "text": "Torcx  is a new boot-time addon manager designed specifically for CoreOS Container Linux. At the most basic level, it is a tool for applying ephemeral changes to an immutable system during early boot. This includes providing third-party binary addons and installing systemd units, which can vary across environments and boots. On every boot, Torcx reads its configuration from local disk and propagates specific assets provided by addon packages (which must be available in local stores).",
            "title": "What is Torcx?"
        },
        {
            "location": "/os/torcx-overview/#torcx-overview",
            "text": "Torcx complements both the  Ignition  provisioning utility and  systemd . Torcx allows customization of Flatcar Container Linux systems without requiring the compilation of custom system images. This goal is achieved by following two main principles: customizations are ephemeral, and they are applied exactly once per boot. Torcx also has a very simple design, with the aim of providing a small low-level system utility which can be driven by more advanced and higher-level tools.",
            "title": "Torcx overview"
        },
        {
            "location": "/os/torcx-overview/#torcx-execution-model-and-systemd-generators",
            "text": "Early in the boot process, execution starts in a minimal initramfs environment where systemd, Ignition, and other boot utilities run. Once up, execution continues by pivoting into the real root file system and by running all  systemd generators , including the main torcx component,  torcx-generator . torcx-generator  runs serially before any other service starts to guarantee it does not race with other startup processes. However, this restricts Torcx to using only local resources. Torcx cannot access configuration or addons from remote file systems or network locations.",
            "title": "Torcx execution model and systemd generators"
        },
        {
            "location": "/os/torcx-overview/#profiles-and-addons",
            "text": "Torcx customizations are applied via local addon packages, which are referenced by profiles. Addons are simple tar-gzipped archives containing binary assets and a manifest. A user profile (upper profile) can be supplied by the administrator to be merged on top of hard-coded vendor and OEM profiles (lower profiles). Torcx will take care of computing and applying the resulting list of addons on the system.",
            "title": "Profiles and addons"
        },
        {
            "location": "/os/torcx-overview/#boot-time-customizations",
            "text": "Torcx guarantees that customizations are applied at most once per boot, before any other service has been considered for startup. This provides a mechanism to customize most aspects of a Flatcar Container Linux system in a reliable way, and avoids runtime upgrading/downgrading issues. Changes applied by Torcx are not persisted to disk, and therefore last exactly for the lifetime of a single boot of an instance.  By the same token, this should be read as a warning against abusing Torcx in the role of a general purpose container, service, or package manager. Torcx's boot-transient model consumes memory with each addon, and, worse, would require system reboots for even simple upgrades.",
            "title": "Boot-time customizations"
        },
        {
            "location": "/os/torcx-overview/#further-design-details",
            "text": "For further details on design and goals, Torcx repository contains extensive  developer documentation .",
            "title": "Further design details"
        },
        {
            "location": "/os/torcx-troubleshooting/",
            "text": "Troubleshooting Torcx\n\u00b6\n\n\nTorcx generator runs early in the boot, when other system facilities are not yet set up and available for use. In case of errors, troubleshooting and debugging can be performed following the suggestions described here.\n\n\nChecking for failures\n\u00b6\n\n\nIn case of errors, Torcx stops before sealing the new system state. This means that in order to check for correct execution, it is sufficient to verify that the metadata file exists:\n\n\n$ test -f /run/metadata/torcx || echo 'torcx failed'\n\n\n\nOn failures, the metadata seal file will not exist, and \ntorcx failed\n will be printed. Verify failure at boot time using the \ntorcx.target\n unit:\n\n\n$ sudo systemctl start torcx.target ; sudo systemctl status torcx.target\n\nAssertion failed on job for torcx.target.\n\n* torcx.target - Verify torcx succeeded\n   Loaded: loaded (/usr/lib/systemd/system/torcx.target; disabled; vendor preset: disabled)\n   Active: inactive (dead) since [...]\n   Assert: start assertion failed at [...]\n           AssertPathExists=/run/metadata/torcx was not met\n\n\n\nGathering logs\n\u00b6\n\n\nThe single most useful piece of information needed when troubleshooting failure is the log from \ntorcx-generator\n. This binary does not run as a typical systemd service, thus log filtering must be done via its syslog identifier.\nWith systemd-journald, this can be accomplished with the following command:\n\n\n$ journalctl --boot 0 --identifier /usr/lib64/systemd/system-generators/torcx-generator\n\n\n\nIf this doesn't yield results, run as root. There may be instances in which the journal isn't owned by the systemd-journal group, or the current user is not part of that group.\n\n\nValidating the configuration\n\u00b6\n\n\nOne common cause for Torcx failure is a malformed configuration (such as a mis-assembled profile, or a syntax error). In other cases, the active profile might reference addon images which are no longer available on the system.",
            "title": "Torcx troubleshooting"
        },
        {
            "location": "/os/torcx-troubleshooting/#troubleshooting-torcx",
            "text": "Torcx generator runs early in the boot, when other system facilities are not yet set up and available for use. In case of errors, troubleshooting and debugging can be performed following the suggestions described here.",
            "title": "Troubleshooting Torcx"
        },
        {
            "location": "/os/torcx-troubleshooting/#checking-for-failures",
            "text": "In case of errors, Torcx stops before sealing the new system state. This means that in order to check for correct execution, it is sufficient to verify that the metadata file exists:  $ test -f /run/metadata/torcx || echo 'torcx failed'  On failures, the metadata seal file will not exist, and  torcx failed  will be printed. Verify failure at boot time using the  torcx.target  unit:  $ sudo systemctl start torcx.target ; sudo systemctl status torcx.target\n\nAssertion failed on job for torcx.target.\n\n* torcx.target - Verify torcx succeeded\n   Loaded: loaded (/usr/lib/systemd/system/torcx.target; disabled; vendor preset: disabled)\n   Active: inactive (dead) since [...]\n   Assert: start assertion failed at [...]\n           AssertPathExists=/run/metadata/torcx was not met",
            "title": "Checking for failures"
        },
        {
            "location": "/os/torcx-troubleshooting/#gathering-logs",
            "text": "The single most useful piece of information needed when troubleshooting failure is the log from  torcx-generator . This binary does not run as a typical systemd service, thus log filtering must be done via its syslog identifier.\nWith systemd-journald, this can be accomplished with the following command:  $ journalctl --boot 0 --identifier /usr/lib64/systemd/system-generators/torcx-generator  If this doesn't yield results, run as root. There may be instances in which the journal isn't owned by the systemd-journal group, or the current user is not part of that group.",
            "title": "Gathering logs"
        },
        {
            "location": "/os/torcx-troubleshooting/#validating-the-configuration",
            "text": "One common cause for Torcx failure is a malformed configuration (such as a mis-assembled profile, or a syntax error). In other cases, the active profile might reference addon images which are no longer available on the system.",
            "title": "Validating the configuration"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/",
            "text": "Checking hardware and firmware support for Flatcar Container Linux Trusted Computing\n\u00b6\n\n\nTrusted Computing requires support in both system hardware and firmware. This document specifies the required support and explains how to determine if a physical machine has the features needed to enable Trusted Computing in Flatcar Container Linux.\n\n\n1. Check for Trusted Platform Module\n\u00b6\n\n\nTrusted Computing depends on the presence of a Trusted Platform Module (TPM). The TPM is a motherboard component responsible for storing the state of the system boot process, and providing a secure communication channel over which this state can be verified. To check for the presence of a TPM, install the latest Alpha version of Flatcar Container Linux and try to list the TPM device file in the \n/sys\n system control filesystem:\n\n\n# ls /sys/class/tpm/tpm0\n\n\nIf this returns an error, the system either does not have a TPM, or it is not enabled in the system firmware. Firmware configuration varies by system. Consult vendor documentation for details.\n\n\n2. Check TPM version\n\u00b6\n\n\nVersion 1.2 TPMs are currently supported. Read the TPM device ID file to discover the TPM version:\n\n\n# cat /sys/class/tpm/tpm0/device/id\n\n\nThe contents of the \nid\n file vary for supported version 1.2 TPMs. It is simplest to check that the file does \nnot\n contain the known string for unsupported version 2.0 TPMs, \nMSFT0101\n. Almost any other non-zero, non-error output from reading the \nid\n file indicates a supported version 1.2 TPM.\n\n\nSupport for version 2.0 TPMs identified with the \nMSFT0101\n string will be added in a future Flatcar Container Linux release.\n\n\n3. Check TPM is enabled and active\n\u00b6\n\n\nThe TPM device provides control files in the \n/sys\n filesystem, as seen above. Read the \nenabled\n and \nactive\n files to check TPM status:\n\n\n# cat /sys/class/tpm/tpm0/device/enabled\n# cat /sys/class/tpm/tpm0/device/active\n\n\n\nIf either of these commands prints \"0\", reconfigure the TPM by writing a code for TPM activation at the next system boot to the PPI \nrequest\n file:\n\n\n# echo 6 > /sys/class/tpm/tpm0/device/ppi/request\n\n\nReboot the system and check TPM status again, as in Step 3.\n\n\n4. Check boot measurement\n\u00b6\n\n\nThe Flatcar Container Linux bootloader will record the state of boot components during the boot process \u2014 \nmeasuring\n each part, in TPM parlance, and storing the result in its Platform Configuration Registers (PCR). Verify that this measurement has been successful by reading the TPM device's \npcrs\n file, a textual representation of the contents of all PCRs:\n\n\n# cat /sys/class/tpm/tpm0/device/pcrs\n\n\nBoot component measurements are recorded in PCRs 9 through 13. These positions in \npcrs\n should all contain meaningful values; that is, values that are neither \n0\n:\n\n\n00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n\n\nnor \nmax\n:\n\n\nFF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF\n\n\nTrusted\n\u00b6\n\n\nA system that passes each of the above tests supports Flatcar Container Linux Trusted Computing and is actively measuring the boot process over the secure TPM channel.",
            "title": "Checking hardware and firmware support for Flatcar Container Linux Trusted Computing"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#checking-hardware-and-firmware-support-for-flatcar-container-linux-trusted-computing",
            "text": "Trusted Computing requires support in both system hardware and firmware. This document specifies the required support and explains how to determine if a physical machine has the features needed to enable Trusted Computing in Flatcar Container Linux.",
            "title": "Checking hardware and firmware support for Flatcar Container Linux Trusted Computing"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#1-check-for-trusted-platform-module",
            "text": "Trusted Computing depends on the presence of a Trusted Platform Module (TPM). The TPM is a motherboard component responsible for storing the state of the system boot process, and providing a secure communication channel over which this state can be verified. To check for the presence of a TPM, install the latest Alpha version of Flatcar Container Linux and try to list the TPM device file in the  /sys  system control filesystem:  # ls /sys/class/tpm/tpm0  If this returns an error, the system either does not have a TPM, or it is not enabled in the system firmware. Firmware configuration varies by system. Consult vendor documentation for details.",
            "title": "1. Check for Trusted Platform Module"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#2-check-tpm-version",
            "text": "Version 1.2 TPMs are currently supported. Read the TPM device ID file to discover the TPM version:  # cat /sys/class/tpm/tpm0/device/id  The contents of the  id  file vary for supported version 1.2 TPMs. It is simplest to check that the file does  not  contain the known string for unsupported version 2.0 TPMs,  MSFT0101 . Almost any other non-zero, non-error output from reading the  id  file indicates a supported version 1.2 TPM.  Support for version 2.0 TPMs identified with the  MSFT0101  string will be added in a future Flatcar Container Linux release.",
            "title": "2. Check TPM version"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#3-check-tpm-is-enabled-and-active",
            "text": "The TPM device provides control files in the  /sys  filesystem, as seen above. Read the  enabled  and  active  files to check TPM status:  # cat /sys/class/tpm/tpm0/device/enabled\n# cat /sys/class/tpm/tpm0/device/active  If either of these commands prints \"0\", reconfigure the TPM by writing a code for TPM activation at the next system boot to the PPI  request  file:  # echo 6 > /sys/class/tpm/tpm0/device/ppi/request  Reboot the system and check TPM status again, as in Step 3.",
            "title": "3. Check TPM is enabled and active"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#4-check-boot-measurement",
            "text": "The Flatcar Container Linux bootloader will record the state of boot components during the boot process \u2014  measuring  each part, in TPM parlance, and storing the result in its Platform Configuration Registers (PCR). Verify that this measurement has been successful by reading the TPM device's  pcrs  file, a textual representation of the contents of all PCRs:  # cat /sys/class/tpm/tpm0/device/pcrs  Boot component measurements are recorded in PCRs 9 through 13. These positions in  pcrs  should all contain meaningful values; that is, values that are neither  0 :  00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  nor  max :  FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF FF",
            "title": "4. Check boot measurement"
        },
        {
            "location": "/os/trusted-computing-hardware-requirements/#trusted",
            "text": "A system that passes each of the above tests supports Flatcar Container Linux Trusted Computing and is actively measuring the boot process over the secure TPM channel.",
            "title": "Trusted"
        },
        {
            "location": "/os/update-strategies/",
            "text": "Reboot strategies on updates\n\u00b6\n\n\nThe overarching goal of Flatcar Container Linux is to secure the Internet's backend infrastructure. We believe that automatically updating the operating system is one of the best tools to achieve this goal.\n\n\nWe realize that each Flatcar Container Linux cluster has a unique tolerance for risk and the operational needs of your applications are complex. In order to meet everyone's needs, there are three update strategies that we have developed based on feedback during our alpha period.\n\n\nIt's important to note that updates are always downloaded to the passive partition when they become available. A reboot is the last step of the update, where the active and passive partitions are swapped (\nrollback instructions\n). These strategies control how that reboot occurs:\n\n\n\n\n\n\n\n\nStrategy\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\netcd-lock\n\n\nReboot after first taking a distributed lock in etcd\n\n\n\n\n\n\nreboot\n\n\nReboot immediately after an update is applied\n\n\n\n\n\n\noff\n\n\nDo not reboot after updates are applied\n\n\n\n\n\n\n\n\nReboot strategy options\n\u00b6\n\n\nThe reboot strategy can be set with a Container Linux Config:\n\n\nlocksmith:\n  reboot_strategy: \"etcd-lock\"\n\n\n\netcd-lock\n\u00b6\n\n\nThe \netcd-lock\n strategy mandates that each machine acquire and hold a reboot lock before it is allowed to reboot. The main goal behind this strategy is to allow for an update to be applied to a cluster quickly, without losing the quorum membership in etcd or rapidly reducing capacity for the services running on the cluster. The reboot lock is held until the machine releases it after a successful update.\n\n\nThe number of machines allowed to reboot simultaneously is configurable via a command line utility:\n\n\n$ locksmithctl set-max 4\nOld: 1\nNew: 4\n\n\n\nThis setting is stored in etcd so it won't have to be configured for subsequent machines.\n\n\nTo view the number of available slots and find out which machines in the cluster are holding locks, run:\n\n\n$ locksmithctl status\nAvailable: 0\nMax: 1\n\nMACHINE ID\n69d27b356a94476da859461d3a3bc6fd\n\n\n\nIf needed, you can manually clear a lock by providing the machine ID:\n\n\nlocksmithctl unlock 69d27b356a94476da859461d3a3bc6fd\n\n\n\nReboot immediately\n\u00b6\n\n\nThe \nreboot\n strategy works exactly like it sounds: the machine is rebooted as soon as the update has been installed to the passive partition. If the applications running on your cluster are highly resilient, this strategy was made for you.\n\n\nOff\n\u00b6\n\n\nThe \noff\n strategy is also straightforward. The update will be installed onto the passive partition and await a reboot command to complete the update. We don't recommend this strategy unless you reboot frequently as part of your normal operations workflow.\n\n\nUpdating PXE/iPXE machines\n\u00b6\n\n\nPXE/iPXE machines download a new copy of Flatcar Container Linux every time they are started thus are dependent on the version of Flatcar Container Linux they are served. If you don't automatically load new Flatcar Container Linux images into your PXE/iPXE server, your machines will never have new features or security updates.\n\n\nAn easy solution to this problem is to use iPXE and reference images \ndirectly from the Flatcar Container Linux storage site\n. The \nalpha\n URL is automatically pointed to the new version of Flatcar Container Linux as it is released.\n\n\nDisable Automatic Updates Daemon\n\u00b6\n\n\nIn case when you don't want to install updates onto the passive partition and avoid update process on failure reboot, you can disable \nupdate-engine\n service manually with \nsudo systemctl stop update-engine\n command (it will be enabled automatically next reboot).\n\n\nIf you wish to disable automatic updates permanently, use can configure this with a Container Linux Config. This example will stop \nupdate-engine\n, which executes the updates, and \nlocksmithd\n, which coordinates reboots across the cluster:\n\n\nsystemd:\n  units:\n    - name: update-engine.service\n      mask: true\n    - name: locksmithd.service\n      mask: true\n\n\n\nUpdating behind a proxy\n\u00b6\n\n\nPublic Internet access is required to contact CoreUpdate and download new versions of Flatcar Container Linux. If direct access is not available the \nupdate-engine\n service may be configured to use a HTTP or SOCKS proxy using curl-compatible environment variables, such as \nHTTPS_PROXY\n or \nALL_PROXY\n.\nSee \ncurl's documentation\n for details.\n\n\nsystemd:\n  units:\n    - name: update-engine.service\n      dropins:\n        - name: 50-proxy.conf\n          contents: |\n            [Service]\n            Environment=ALL_PROXY=http://proxy.example.com:3128\n\n\n\nProxy environment variables can also be set \nsystem-wide\n.\n\n\nManually triggering an update\n\u00b6\n\n\nEach machine should check in about 10 minutes after boot and roughly every hour after that. If you'd like to see it sooner, you can force an update check, which will skip any rate-limiting settings that are configured in CoreUpdate.\n\n\n$ update_engine_client -check_for_update\n[0123/220706:INFO:update_engine_client.cc(245)] Initiating update check and install.\n\n\n\nAuto-updates with a maintenance window\n\u00b6\n\n\nLocksmith supports maintenance windows in addition to the reboot strategies mentioned earlier. Maintenance windows define a window of time during which a reboot can occur. These operate in addition to reboot strategies, so if the machine has a maintenance window and requires a reboot lock, the machine will only reboot when it has the lock during that window.\n\n\nWindows are defined by a start time and a length. In this example, the window is defined to be every Thursday between 04:00 and 05:00:\n\n\nlocksmith:\n  reboot_strategy: reboot\n  window_start: Thu 04:00\n  window_length: 1h\n\n\n\nThis will configure a Flatcar Container Linux machine to follow the \nreboot\n strategy, and thus when an update is ready it will simply reboot instead of attempting to grab a lock in etcd. This machine however has also been configured to only reboot between 04:00 and 05:00 on Thursdays, so if an update occurs outside of this window the machine will then wait until it is inside of this window to reboot.\n\n\nFor more information about the supported syntax, refer to the \nLocksmith documentation\n.",
            "title": "Reboot strategies on updates"
        },
        {
            "location": "/os/update-strategies/#reboot-strategies-on-updates",
            "text": "The overarching goal of Flatcar Container Linux is to secure the Internet's backend infrastructure. We believe that automatically updating the operating system is one of the best tools to achieve this goal.  We realize that each Flatcar Container Linux cluster has a unique tolerance for risk and the operational needs of your applications are complex. In order to meet everyone's needs, there are three update strategies that we have developed based on feedback during our alpha period.  It's important to note that updates are always downloaded to the passive partition when they become available. A reboot is the last step of the update, where the active and passive partitions are swapped ( rollback instructions ). These strategies control how that reboot occurs:     Strategy  Description      etcd-lock  Reboot after first taking a distributed lock in etcd    reboot  Reboot immediately after an update is applied    off  Do not reboot after updates are applied",
            "title": "Reboot strategies on updates"
        },
        {
            "location": "/os/update-strategies/#reboot-strategy-options",
            "text": "The reboot strategy can be set with a Container Linux Config:  locksmith:\n  reboot_strategy: \"etcd-lock\"",
            "title": "Reboot strategy options"
        },
        {
            "location": "/os/update-strategies/#etcd-lock",
            "text": "The  etcd-lock  strategy mandates that each machine acquire and hold a reboot lock before it is allowed to reboot. The main goal behind this strategy is to allow for an update to be applied to a cluster quickly, without losing the quorum membership in etcd or rapidly reducing capacity for the services running on the cluster. The reboot lock is held until the machine releases it after a successful update.  The number of machines allowed to reboot simultaneously is configurable via a command line utility:  $ locksmithctl set-max 4\nOld: 1\nNew: 4  This setting is stored in etcd so it won't have to be configured for subsequent machines.  To view the number of available slots and find out which machines in the cluster are holding locks, run:  $ locksmithctl status\nAvailable: 0\nMax: 1\n\nMACHINE ID\n69d27b356a94476da859461d3a3bc6fd  If needed, you can manually clear a lock by providing the machine ID:  locksmithctl unlock 69d27b356a94476da859461d3a3bc6fd",
            "title": "etcd-lock"
        },
        {
            "location": "/os/update-strategies/#reboot-immediately",
            "text": "The  reboot  strategy works exactly like it sounds: the machine is rebooted as soon as the update has been installed to the passive partition. If the applications running on your cluster are highly resilient, this strategy was made for you.",
            "title": "Reboot immediately"
        },
        {
            "location": "/os/update-strategies/#off",
            "text": "The  off  strategy is also straightforward. The update will be installed onto the passive partition and await a reboot command to complete the update. We don't recommend this strategy unless you reboot frequently as part of your normal operations workflow.",
            "title": "Off"
        },
        {
            "location": "/os/update-strategies/#updating-pxeipxe-machines",
            "text": "PXE/iPXE machines download a new copy of Flatcar Container Linux every time they are started thus are dependent on the version of Flatcar Container Linux they are served. If you don't automatically load new Flatcar Container Linux images into your PXE/iPXE server, your machines will never have new features or security updates.  An easy solution to this problem is to use iPXE and reference images  directly from the Flatcar Container Linux storage site . The  alpha  URL is automatically pointed to the new version of Flatcar Container Linux as it is released.",
            "title": "Updating PXE/iPXE machines"
        },
        {
            "location": "/os/update-strategies/#disable-automatic-updates-daemon",
            "text": "In case when you don't want to install updates onto the passive partition and avoid update process on failure reboot, you can disable  update-engine  service manually with  sudo systemctl stop update-engine  command (it will be enabled automatically next reboot).  If you wish to disable automatic updates permanently, use can configure this with a Container Linux Config. This example will stop  update-engine , which executes the updates, and  locksmithd , which coordinates reboots across the cluster:  systemd:\n  units:\n    - name: update-engine.service\n      mask: true\n    - name: locksmithd.service\n      mask: true",
            "title": "Disable Automatic Updates Daemon"
        },
        {
            "location": "/os/update-strategies/#updating-behind-a-proxy",
            "text": "Public Internet access is required to contact CoreUpdate and download new versions of Flatcar Container Linux. If direct access is not available the  update-engine  service may be configured to use a HTTP or SOCKS proxy using curl-compatible environment variables, such as  HTTPS_PROXY  or  ALL_PROXY .\nSee  curl's documentation  for details.  systemd:\n  units:\n    - name: update-engine.service\n      dropins:\n        - name: 50-proxy.conf\n          contents: |\n            [Service]\n            Environment=ALL_PROXY=http://proxy.example.com:3128  Proxy environment variables can also be set  system-wide .",
            "title": "Updating behind a proxy"
        },
        {
            "location": "/os/update-strategies/#manually-triggering-an-update",
            "text": "Each machine should check in about 10 minutes after boot and roughly every hour after that. If you'd like to see it sooner, you can force an update check, which will skip any rate-limiting settings that are configured in CoreUpdate.  $ update_engine_client -check_for_update\n[0123/220706:INFO:update_engine_client.cc(245)] Initiating update check and install.",
            "title": "Manually triggering an update"
        },
        {
            "location": "/os/update-strategies/#auto-updates-with-a-maintenance-window",
            "text": "Locksmith supports maintenance windows in addition to the reboot strategies mentioned earlier. Maintenance windows define a window of time during which a reboot can occur. These operate in addition to reboot strategies, so if the machine has a maintenance window and requires a reboot lock, the machine will only reboot when it has the lock during that window.  Windows are defined by a start time and a length. In this example, the window is defined to be every Thursday between 04:00 and 05:00:  locksmith:\n  reboot_strategy: reboot\n  window_start: Thu 04:00\n  window_length: 1h  This will configure a Flatcar Container Linux machine to follow the  reboot  strategy, and thus when an update is ready it will simply reboot instead of attempting to grab a lock in etcd. This machine however has also been configured to only reboot between 04:00 and 05:00 on Thursdays, so if an update occurs outside of this window the machine will then wait until it is inside of this window to reboot.  For more information about the supported syntax, refer to the  Locksmith documentation .",
            "title": "Auto-updates with a maintenance window"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/",
            "text": "Using environment variables in systemd units\n\u00b6\n\n\nEnvironment directive\n\u00b6\n\n\nsystemd has an Environment directive which sets environment variables for executed processes. It takes a space-separated list of variable assignments. This option may be specified more than once in which case all listed variables will be set. If the same variable is set twice, the later setting will override the earlier setting. If the empty string is assigned to this option, the list of environment variables is reset, all prior assignments have no effect. Environments directives are used in built-in Flatcar Container Linux systemd units, for example in etcd2 and flannel.\n\n\nWith the example below, you can configure your etcd2 daemon to use encryption. Just create \n/etc/systemd/system/etcd2.service.d/30-certificates.conf\n \ndrop-in\n for etcd2.service:\n\n\n[Service]\n# Client Env Vars\nEnvironment=ETCD_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_CERT_FILE=/path/to/server.crt\nEnvironment=ETCD_KEY_FILE=/path/to/server.key\n# Peer Env Vars\nEnvironment=ETCD_PEER_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_PEER_CERT_FILE=/path/to/peers.crt\nEnvironment=ETCD_PEER_KEY_FILE=/path/to/peers.key\n\n\n\nThen run \nsudo systemctl daemon-reload\n and \nsudo systemctl restart etcd2.service\n to apply new environments to etcd2 daemon. You can read more about etcd2 certificates \nhere\n.\n\n\nEnvironmentFile directive\n\u00b6\n\n\nEnvironmentFile similar to Environment directive but reads the environment variables from a text file. The text file should contain new-line-separated variable assignments.\n\n\nFor example, in Flatcar Container Linux, the \ncoreos-metadata.service\n service creates \n/run/metadata/coreos\n. This environment file can be included by other services in order to inject dynamic configuration. Here's an example of the environment file when run on DigitalOcean (the IP addresses have been removed):\n\n\nCOREOS_DIGITALOCEAN_IPV4_ANCHOR_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV4_PRIVATE_0=X.X.X.X\nCOREOS_DIGITALOCEAN_HOSTNAME=test.example.com\nCOREOS_DIGITALOCEAN_IPV4_PUBLIC_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV6_PUBLIC_0=X:X:X:X:X:X:X:X\n\n\n\nThis environment file can then be sourced and its variables used. Here is an example drop-in for \netcd-member.service\n which starts \ncoreos-metadata.service\n and then uses the generated results:\n\n\n[Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/bin/etcd2 \\\n  --advertise-client-urls=http://${COREOS_DIGITALOCEAN_IPV4_PUBLIC_0}:2379 \\\n  --initial-advertise-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --listen-client-urls=http://0.0.0.0:2379 \\\n  --listen-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --initial-cluster=%m=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380\n\n\n\nOther examples\n\u00b6\n\n\nUse host IP addresses and EnvironmentFile\n\u00b6\n\n\nYou can also write your host IP addresses into \n/etc/network-environment\n file using \nthis\n utility. Then you can run your Docker containers following way:\n\n\n[Unit]\nDescription=Nginx service\nRequires=etcd2.service\nAfter=etcd2.service\n[Service]\n# Get network environmental variables\nEnvironmentFile=/etc/network-environment\nExecStartPre=-/usr/bin/docker kill nginx\nExecStartPre=-/usr/bin/docker rm nginx\nExecStartPre=/usr/bin/docker pull nginx\nExecStartPre=/usr/bin/etcdctl set /services/nginx '{\"host\": \"%H\", \"ipv4_addr\": ${DEFAULT_IPV4}, \"port\": 80}'\nExecStart=/usr/bin/docker run --rm --name nginx -p ${DEFAULT_IPV4}:80:80 nginx\nExecStop=/usr/bin/docker stop nginx\nExecStopPost=/usr/bin/etcdctl rm /services/nginx\n\n\n\nThis unit file will run nginx Docker container and bind it to specific IP address and port.\n\n\nSystem wide environment variables\n\u00b6\n\n\nYou can define system wide environment variables using a \nContainer Linux Config\n as explained below:\n\n\nstorage:\n  files:\n    - path: /etc/systemd/system.conf.d/10-default-env.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Manager]\n          DefaultEnvironment=HTTP_PROXY=http://192.168.0.1:3128\n    - path: /etc/profile.env\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          export HTTP_PROXY=http://192.168.0.1:3128\n\n\n\nWhere:\n\n\n\n\n/etc/systemd/system.conf.d/10-default-env.conf\n config file will set default environment variables for all systemd units.\n\n\n/etc/profile.env\n will set environment variables for all users logged in Flatcar Container Linux.\n\n\n\n\netcd2.service unit advanced example\n\u00b6\n\n\nA \ncomplete example\n of combining environment variables and systemd \ndrop-ins\n to reconfigure an existing machine running etcd.\n\n\nMore systemd examples\n\u00b6\n\n\nFor more systemd examples, check out these documents:\n\n\nCustomizing Docker\n\n\nCustomizing the SSH Daemon\n\n\nUsing systemd Drop-In Units\n\n\netcd Cluster Runtime Reconfiguration on Flatcar Container Linux\n\n\nMore Information\n\u00b6\n\n\nsystemd.exec Docs\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs",
            "title": "Using environment variables in systemd units"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#using-environment-variables-in-systemd-units",
            "text": "",
            "title": "Using environment variables in systemd units"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#environment-directive",
            "text": "systemd has an Environment directive which sets environment variables for executed processes. It takes a space-separated list of variable assignments. This option may be specified more than once in which case all listed variables will be set. If the same variable is set twice, the later setting will override the earlier setting. If the empty string is assigned to this option, the list of environment variables is reset, all prior assignments have no effect. Environments directives are used in built-in Flatcar Container Linux systemd units, for example in etcd2 and flannel.  With the example below, you can configure your etcd2 daemon to use encryption. Just create  /etc/systemd/system/etcd2.service.d/30-certificates.conf   drop-in  for etcd2.service:  [Service]\n# Client Env Vars\nEnvironment=ETCD_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_CERT_FILE=/path/to/server.crt\nEnvironment=ETCD_KEY_FILE=/path/to/server.key\n# Peer Env Vars\nEnvironment=ETCD_PEER_CA_FILE=/path/to/CA.pem\nEnvironment=ETCD_PEER_CERT_FILE=/path/to/peers.crt\nEnvironment=ETCD_PEER_KEY_FILE=/path/to/peers.key  Then run  sudo systemctl daemon-reload  and  sudo systemctl restart etcd2.service  to apply new environments to etcd2 daemon. You can read more about etcd2 certificates  here .",
            "title": "Environment directive"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#environmentfile-directive",
            "text": "EnvironmentFile similar to Environment directive but reads the environment variables from a text file. The text file should contain new-line-separated variable assignments.  For example, in Flatcar Container Linux, the  coreos-metadata.service  service creates  /run/metadata/coreos . This environment file can be included by other services in order to inject dynamic configuration. Here's an example of the environment file when run on DigitalOcean (the IP addresses have been removed):  COREOS_DIGITALOCEAN_IPV4_ANCHOR_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV4_PRIVATE_0=X.X.X.X\nCOREOS_DIGITALOCEAN_HOSTNAME=test.example.com\nCOREOS_DIGITALOCEAN_IPV4_PUBLIC_0=X.X.X.X\nCOREOS_DIGITALOCEAN_IPV6_PUBLIC_0=X:X:X:X:X:X:X:X  This environment file can then be sourced and its variables used. Here is an example drop-in for  etcd-member.service  which starts  coreos-metadata.service  and then uses the generated results:  [Unit]\nRequires=coreos-metadata.service\nAfter=coreos-metadata.service\n\n[Service]\nEnvironmentFile=/run/metadata/coreos\nExecStart=\nExecStart=/usr/bin/etcd2 \\\n  --advertise-client-urls=http://${COREOS_DIGITALOCEAN_IPV4_PUBLIC_0}:2379 \\\n  --initial-advertise-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --listen-client-urls=http://0.0.0.0:2379 \\\n  --listen-peer-urls=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380 \\\n  --initial-cluster=%m=http://${COREOS_DIGITALOCEAN_IPV4_PRIVATE_0}:2380",
            "title": "EnvironmentFile directive"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#other-examples",
            "text": "",
            "title": "Other examples"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#use-host-ip-addresses-and-environmentfile",
            "text": "You can also write your host IP addresses into  /etc/network-environment  file using  this  utility. Then you can run your Docker containers following way:  [Unit]\nDescription=Nginx service\nRequires=etcd2.service\nAfter=etcd2.service\n[Service]\n# Get network environmental variables\nEnvironmentFile=/etc/network-environment\nExecStartPre=-/usr/bin/docker kill nginx\nExecStartPre=-/usr/bin/docker rm nginx\nExecStartPre=/usr/bin/docker pull nginx\nExecStartPre=/usr/bin/etcdctl set /services/nginx '{\"host\": \"%H\", \"ipv4_addr\": ${DEFAULT_IPV4}, \"port\": 80}'\nExecStart=/usr/bin/docker run --rm --name nginx -p ${DEFAULT_IPV4}:80:80 nginx\nExecStop=/usr/bin/docker stop nginx\nExecStopPost=/usr/bin/etcdctl rm /services/nginx  This unit file will run nginx Docker container and bind it to specific IP address and port.",
            "title": "Use host IP addresses and EnvironmentFile"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#system-wide-environment-variables",
            "text": "You can define system wide environment variables using a  Container Linux Config  as explained below:  storage:\n  files:\n    - path: /etc/systemd/system.conf.d/10-default-env.conf\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          [Manager]\n          DefaultEnvironment=HTTP_PROXY=http://192.168.0.1:3128\n    - path: /etc/profile.env\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          export HTTP_PROXY=http://192.168.0.1:3128  Where:   /etc/systemd/system.conf.d/10-default-env.conf  config file will set default environment variables for all systemd units.  /etc/profile.env  will set environment variables for all users logged in Flatcar Container Linux.",
            "title": "System wide environment variables"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#etcd2service-unit-advanced-example",
            "text": "A  complete example  of combining environment variables and systemd  drop-ins  to reconfigure an existing machine running etcd.",
            "title": "etcd2.service unit advanced example"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#more-systemd-examples",
            "text": "For more systemd examples, check out these documents:  Customizing Docker  Customizing the SSH Daemon  Using systemd Drop-In Units  etcd Cluster Runtime Reconfiguration on Flatcar Container Linux",
            "title": "More systemd examples"
        },
        {
            "location": "/os/using-environment-variables-in-systemd-units/#more-information",
            "text": "systemd.exec Docs  systemd.service Docs  systemd.unit Docs",
            "title": "More Information"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/",
            "text": "Using systemd and udev rules\n\u00b6\n\n\nIn our example we will use libvirt VM with Flatcar Container Linux and run systemd unit on disk attach event. First of all we have to create systemd unit file \n/etc/systemd/system/device-attach.service\n:\n\n\n[Service]\nType=oneshot\nExecStart=/usr/bin/echo 'device has been attached'\n\n\n\nThis unit file will be triggered by our udev rule.\n\n\nThen we have to start \nudevadm monitor --environment\n to monitor kernel events.\n\n\nOnce you've attached virtio libvirt device (i.e. \nvirsh attach-disk coreos /dev/VG/test vdc\n) you'll see similar \nudevadm\n output:\n\n\nUDEV  [545.954641] add      /devices/pci0000:00/0000:00:18.0/virtio4/block/vdb (block)\n.ID_FS_TYPE_NEW=\nACTION=add\nDEVNAME=/dev/vdb\nDEVPATH=/devices/pci0000:00/0000:00:18.0/virtio4/block/vdb\nDEVTYPE=disk\nID_FS_TYPE=\nMAJOR=254\nMINOR=16\nSEQNUM=1327\nSUBSYSTEM=block\nUSEC_INITIALIZED=545954447\n\n\n\nAccording to text above udev generates event which contains directives (ACTION=add and SUBSYSTEM=block) we will use in our rule. It should look this way:\n\n\nACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"\n\n\n\nThat rule means that udev will trigger \ndevice-attach.service\n systemd unit on any block device attachment. Now when we use this command \nvirsh attach-disk coreos /dev/VG/test vdc\n on host machine, we should see \ndevice has been attached\n message in Flatcar Container Linux node's journal. This example should be similar to USB/SAS/SATA device attach.\n\n\nContainer Linux Config example\n\u00b6\n\n\nTo use the unit and udev rule with a Container Linux Config, modify this example as needed:\n\n\nstorage:\n  files:\n    - path: /etc/udev/rules.d/01-block.rules\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          ACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"\nsystemd:\n  units:\n    - name: device-attach.service\n      contents: |\n        [Unit]\n        Description=Notify about attached device\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo 'device has been attached'\n\n\n\nMore systemd examples\n\u00b6\n\n\nFor more systemd examples, check out these documents:\n\n\nCustomizing Docker\n\n\nCustomizing the SSH Daemon\n\n\nUsing systemd Drop-In Units\n\n\nMore information\n\u00b6\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs\n\n\nsystemd.target Docs\n\n\nudev Docs",
            "title": "Using systemd and udev rules"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#using-systemd-and-udev-rules",
            "text": "In our example we will use libvirt VM with Flatcar Container Linux and run systemd unit on disk attach event. First of all we have to create systemd unit file  /etc/systemd/system/device-attach.service :  [Service]\nType=oneshot\nExecStart=/usr/bin/echo 'device has been attached'  This unit file will be triggered by our udev rule.  Then we have to start  udevadm monitor --environment  to monitor kernel events.  Once you've attached virtio libvirt device (i.e.  virsh attach-disk coreos /dev/VG/test vdc ) you'll see similar  udevadm  output:  UDEV  [545.954641] add      /devices/pci0000:00/0000:00:18.0/virtio4/block/vdb (block)\n.ID_FS_TYPE_NEW=\nACTION=add\nDEVNAME=/dev/vdb\nDEVPATH=/devices/pci0000:00/0000:00:18.0/virtio4/block/vdb\nDEVTYPE=disk\nID_FS_TYPE=\nMAJOR=254\nMINOR=16\nSEQNUM=1327\nSUBSYSTEM=block\nUSEC_INITIALIZED=545954447  According to text above udev generates event which contains directives (ACTION=add and SUBSYSTEM=block) we will use in our rule. It should look this way:  ACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"  That rule means that udev will trigger  device-attach.service  systemd unit on any block device attachment. Now when we use this command  virsh attach-disk coreos /dev/VG/test vdc  on host machine, we should see  device has been attached  message in Flatcar Container Linux node's journal. This example should be similar to USB/SAS/SATA device attach.",
            "title": "Using systemd and udev rules"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#container-linux-config-example",
            "text": "To use the unit and udev rule with a Container Linux Config, modify this example as needed:  storage:\n  files:\n    - path: /etc/udev/rules.d/01-block.rules\n      filesystem: root\n      mode: 0644\n      contents:\n        inline: |\n          ACTION==\"add\", SUBSYSTEM==\"block\", TAG+=\"systemd\", ENV{SYSTEMD_WANTS}=\"device-attach.service\"\nsystemd:\n  units:\n    - name: device-attach.service\n      contents: |\n        [Unit]\n        Description=Notify about attached device\n\n        [Service]\n        Type=oneshot\n        ExecStart=/usr/bin/echo 'device has been attached'",
            "title": "Container Linux Config example"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#more-systemd-examples",
            "text": "For more systemd examples, check out these documents:  Customizing Docker  Customizing the SSH Daemon  Using systemd Drop-In Units",
            "title": "More systemd examples"
        },
        {
            "location": "/os/using-systemd-and-udev-rules/#more-information",
            "text": "systemd.service Docs  systemd.unit Docs  systemd.target Docs  udev Docs",
            "title": "More information"
        },
        {
            "location": "/os/using-systemd-drop-in-units/",
            "text": "Using systemd drop-in units\n\u00b6\n\n\nThere are two methods of overriding default Flatcar Container Linux settings in unit files: copying the unit file from \n/usr/lib64/systemd/system\n to \n/etc/systemd/system\n and modifying the chosen settings. Alternatively, one can create a directory named \nunit.d\n within \n/etc/systemd/system\n and place a drop-in file \nname.conf\n there that only changes the specific settings one is interested in. Note that multiple such drop-in files are read if present.\n\n\nThe advantage of the first method is that one easily overrides the complete unit, the default Flatcar Container Linux unit is not parsed at all anymore. It has the disadvantage that improvements to the unit file supplied by Flatcar Container Linux are not automatically incorporated on updates.\n\n\nThe advantage of the second method is that one only overrides the settings one specifically wants, where updates to the original Flatcar Container Linux unit automatically apply. This has the disadvantage that some future Flatcar Container Linux updates might be incompatible with the local changes, but the risk is much lower.\n\n\nNote that for drop-in files, if one wants to remove entries from a setting that is parsed as a list (and is not a dependency), such as \nConditionPathExists=\n (or e.g. \nExecStart=\n in service units), one needs to first clear the list before re-adding all entries except the one that is to be removed. See below for an example.\n\n\nThis also applies for user instances of systemd, but with different locations for the unit files. See the section on unit load paths in \nofficial systemd doc\n for further details.\n\n\nExample: customizing locksmithd.service\n\u00b6\n\n\nLet's review \n/usr/lib64/systemd/system/locksmithd.service\n unit (you can find it using this command: \nsystemctl list-units | grep locksmithd\n) with the following contents:\n\n\n[Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nLet's walk through increasing the \nRestartSec\n parameter via both methods:\n\n\nOverride only specific option\n\u00b6\n\n\nYou can create a drop-in file \n/etc/systemd/system/locksmithd.service.d/10-restart_60s.conf\n with the following contents:\n\n\n[Service]\nRestartSec=60s\n\n\n\nThen reload systemd, scanning for new or changed units:\n\n\nsystemctl daemon-reload\n\n\n\n\nAnd restart modified service if necessary (in our example we have changed only \nRestartSec\n option, but if you want to change environment variables, \nExecStart\n or other run options you have to restart service):\n\n\nsystemctl restart locksmithd.service\n\n\n\nHere is how that could be implemented within a Container Linux Config:\n\n\nsystemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      dropins:\n        - name: 10-restart_60s.conf\n          contents: |\n            [Service]\n            RestartSec=60s\n\n\n\nThis change is small and targeted. It is the easiest way to tweak unit's parameters.\n\n\nOverride the whole unit file\n\u00b6\n\n\nAnother way is to override whole systemd unit. Copy default unit file \n/usr/lib64/systemd/system/locksmithd.service\n to \n/etc/systemd/system/locksmithd.service\n and change the chosen settings:\n\n\n[Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=60s\n\n[Install]\nWantedBy=multi-user.target\n\n\n\nContainer Linux Config example:\n\n\nsystemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Cluster reboot manager\n        After=update-engine.service\n        ConditionVirtualization=!container\n        ConditionPathExists=!/usr/.noupdate\n\n        [Service]\n        CPUShares=16\n        MemoryLimit=32M\n        PrivateDevices=true\n        Environment=GOMAXPROCS=1\n        EnvironmentFile=-/usr/share/coreos/update.conf\n        EnvironmentFile=-/etc/coreos/update.conf\n        ExecStart=/usr/lib/locksmith/locksmithd\n        Restart=on-failure\n        RestartSec=60s\n\n        [Install]\n        WantedBy=multi-user.target\n\n\n\nList drop-ins\n\u00b6\n\n\nTo see all runtime drop-in changes for system units run the command below:\n\n\nsystemd-delta --type=extended\n\n\n\nOther systemd examples\n\u00b6\n\n\nFor another real systemd examples, check out these documents:\n\n\nCustomizing Docker\n\n\nCustomizing the SSH Daemon\n\n\nUsing Environment Variables In systemd Units\n\n\nMore Information\n\u00b6\n\n\nsystemd.service Docs\n\n\nsystemd.unit Docs\n\n\nsystemd.target Docs",
            "title": "Using systemd drop-in units"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#using-systemd-drop-in-units",
            "text": "There are two methods of overriding default Flatcar Container Linux settings in unit files: copying the unit file from  /usr/lib64/systemd/system  to  /etc/systemd/system  and modifying the chosen settings. Alternatively, one can create a directory named  unit.d  within  /etc/systemd/system  and place a drop-in file  name.conf  there that only changes the specific settings one is interested in. Note that multiple such drop-in files are read if present.  The advantage of the first method is that one easily overrides the complete unit, the default Flatcar Container Linux unit is not parsed at all anymore. It has the disadvantage that improvements to the unit file supplied by Flatcar Container Linux are not automatically incorporated on updates.  The advantage of the second method is that one only overrides the settings one specifically wants, where updates to the original Flatcar Container Linux unit automatically apply. This has the disadvantage that some future Flatcar Container Linux updates might be incompatible with the local changes, but the risk is much lower.  Note that for drop-in files, if one wants to remove entries from a setting that is parsed as a list (and is not a dependency), such as  ConditionPathExists=  (or e.g.  ExecStart=  in service units), one needs to first clear the list before re-adding all entries except the one that is to be removed. See below for an example.  This also applies for user instances of systemd, but with different locations for the unit files. See the section on unit load paths in  official systemd doc  for further details.",
            "title": "Using systemd drop-in units"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#example-customizing-locksmithdservice",
            "text": "Let's review  /usr/lib64/systemd/system/locksmithd.service  unit (you can find it using this command:  systemctl list-units | grep locksmithd ) with the following contents:  [Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=10s\n\n[Install]\nWantedBy=multi-user.target  Let's walk through increasing the  RestartSec  parameter via both methods:",
            "title": "Example: customizing locksmithd.service"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#override-only-specific-option",
            "text": "You can create a drop-in file  /etc/systemd/system/locksmithd.service.d/10-restart_60s.conf  with the following contents:  [Service]\nRestartSec=60s  Then reload systemd, scanning for new or changed units:  systemctl daemon-reload  And restart modified service if necessary (in our example we have changed only  RestartSec  option, but if you want to change environment variables,  ExecStart  or other run options you have to restart service):  systemctl restart locksmithd.service  Here is how that could be implemented within a Container Linux Config:  systemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      dropins:\n        - name: 10-restart_60s.conf\n          contents: |\n            [Service]\n            RestartSec=60s  This change is small and targeted. It is the easiest way to tweak unit's parameters.",
            "title": "Override only specific option"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#override-the-whole-unit-file",
            "text": "Another way is to override whole systemd unit. Copy default unit file  /usr/lib64/systemd/system/locksmithd.service  to  /etc/systemd/system/locksmithd.service  and change the chosen settings:  [Unit]\nDescription=Cluster reboot manager\nAfter=update-engine.service\nConditionVirtualization=!container\nConditionPathExists=!/usr/.noupdate\n\n[Service]\nCPUShares=16\nMemoryLimit=32M\nPrivateDevices=true\nEnvironment=GOMAXPROCS=1\nEnvironmentFile=-/usr/share/coreos/update.conf\nEnvironmentFile=-/etc/coreos/update.conf\nExecStart=/usr/lib/locksmith/locksmithd\nRestart=on-failure\nRestartSec=60s\n\n[Install]\nWantedBy=multi-user.target  Container Linux Config example:  systemd:\n  units:\n    - name: locksmithd.service\n      enable: true\n      contents: |\n        [Unit]\n        Description=Cluster reboot manager\n        After=update-engine.service\n        ConditionVirtualization=!container\n        ConditionPathExists=!/usr/.noupdate\n\n        [Service]\n        CPUShares=16\n        MemoryLimit=32M\n        PrivateDevices=true\n        Environment=GOMAXPROCS=1\n        EnvironmentFile=-/usr/share/coreos/update.conf\n        EnvironmentFile=-/etc/coreos/update.conf\n        ExecStart=/usr/lib/locksmith/locksmithd\n        Restart=on-failure\n        RestartSec=60s\n\n        [Install]\n        WantedBy=multi-user.target",
            "title": "Override the whole unit file"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#list-drop-ins",
            "text": "To see all runtime drop-in changes for system units run the command below:  systemd-delta --type=extended",
            "title": "List drop-ins"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#other-systemd-examples",
            "text": "For another real systemd examples, check out these documents:  Customizing Docker  Customizing the SSH Daemon  Using Environment Variables In systemd Units",
            "title": "Other systemd examples"
        },
        {
            "location": "/os/using-systemd-drop-in-units/#more-information",
            "text": "systemd.service Docs  systemd.unit Docs  systemd.target Docs",
            "title": "More Information"
        },
        {
            "location": "/os/verify-images/",
            "text": "Verify Flatcar Container Linux images with GPG\n\u00b6\n\n\nKinvolk publishes new Flatcar Container Linux images for each release across a variety of platforms and hosting providers. Each channel has its own set of images (\nstable\n, \nbeta\n, \nalpha\n, \nedge\n) that are posted to our storage site. Along with each image, a signature is generated from the \nFlatcar Container Linux Image Signing Key\n and posted.\n\n\nAfter downloading your image, you should verify it with \ngpg\n tool. First, download the image signing key:\n\n\ncurl -L -O https://flatcar-linux.org/security/image-signing-key/Flatcar_Image_Signing_Key.asc\n\n\n\nNext, import the public key and verify that the ID matches the website: \nFlatcar Image Signing Key\n\n\ngpg --import --keyid-format LONG Flatcar_Image_Signing_Key.asc\ngpg: key 50E0885593D2DCB4: public key \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\ngpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model\ngpg: depth: 0  valid:   2  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 2u\n\n\n\nNow we're ready to download an image and it's signature, ending in .sig. We're using the QEMU image in this example:\n\n\ncurl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\ncurl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig\n\n\n\nVerify image with \ngpg\n tool:\n\n\ngpg --verify flatcar_production_qemu_image.img.bz2.sig\ngpg: Signature made Tue Jun 23 09:39:04 2015 CEST using RSA key ID E5676EFC\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\"\n\n\n\nThe \nGood signature\n message indicates that the file signature is valid. Go launch some machines now that we've successfully verified that this Flatcar Container Linux image isn't corrupt, that it was authored by Kinvolk, and wasn't tampered with in transit.",
            "title": "Verify Flatcar Container Linux images with GPG"
        },
        {
            "location": "/os/verify-images/#verify-flatcar-container-linux-images-with-gpg",
            "text": "Kinvolk publishes new Flatcar Container Linux images for each release across a variety of platforms and hosting providers. Each channel has its own set of images ( stable ,  beta ,  alpha ,  edge ) that are posted to our storage site. Along with each image, a signature is generated from the  Flatcar Container Linux Image Signing Key  and posted.  After downloading your image, you should verify it with  gpg  tool. First, download the image signing key:  curl -L -O https://flatcar-linux.org/security/image-signing-key/Flatcar_Image_Signing_Key.asc  Next, import the public key and verify that the ID matches the website:  Flatcar Image Signing Key  gpg --import --keyid-format LONG Flatcar_Image_Signing_Key.asc\ngpg: key 50E0885593D2DCB4: public key \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\" imported\ngpg: Total number processed: 1\ngpg:               imported: 1  (RSA: 1)\ngpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model\ngpg: depth: 0  valid:   2  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 2u  Now we're ready to download an image and it's signature, ending in .sig. We're using the QEMU image in this example:  curl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2\ncurl -L -O https://stable.release.flatcar-linux.net/amd64-usr/current/flatcar_production_qemu_image.img.bz2.sig  Verify image with  gpg  tool:  gpg --verify flatcar_production_qemu_image.img.bz2.sig\ngpg: Signature made Tue Jun 23 09:39:04 2015 CEST using RSA key ID E5676EFC\ngpg: Good signature from \"Flatcar Buildbot (Official Builds) <buildbot@flatcar-linux.org>\"  The  Good signature  message indicates that the file signature is valid. Go launch some machines now that we've successfully verified that this Flatcar Container Linux image isn't corrupt, that it was authored by Kinvolk, and wasn't tampered with in transit.",
            "title": "Verify Flatcar Container Linux images with GPG"
        }
    ]
}